\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx} % Required for inserting images
\usepackage[hidelinks]{hyperref}
\usepackage{pgfplots}
\usepackage{titling}
\usepackage{esint}
\pgfplotsset{width=3in,compat=1.9}


\DeclareMathOperator{\sgn}{sgn}

\DeclareMathOperator{\comp}{comp}
\DeclareMathOperator{\proj}{proj}

\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\divergence}{div}

\newcommand{\vect}[1]{\ensuremath{\overrightarrow{#1}}}
\newcommand{\magnitude}[1]{\ensuremath{\lVert #1 \rVert}}
\newcommand{\magvect}[1]{\magnitude{\vect{#1}}}
\newcommand{\abs}[1]{\left|#1\right|}

\renewcommand{\labelitemii}{\textopenbullet}

\title{Calculus! Calculus!}
\author{David Chen}
\date{September 2024 -- June 2025}

\begin{document}

\maketitle

\section{Multivariable Functions and Space}

\subsection{Functions}
$$f: \mathbb{R}^n \to \mathbb{R}^m$$
Functions map a \textbf{domain} (e.g. $\mathbb{R}^n$, read ``R N") to a \textbf{codomain} (e.g. $\mathbb{R}^m$ -- see appendix \ref{set-multiplication} for more information). In the scope of this course, we will usually have $n$ or $m = 1$, and $n,m \in \mathbb{N}$.\footnote{In generalized multivariable calculus, more profound matrix transformations are used.}
Common function domain/codomain pairings we consider are
\begin{itemize}
    \item $f: \mathbb{R}^2 \to \mathbb{R}$ -- can be represented as mapping points on the 2D plane to label values or as a 3D \textbf{surface}.
    \item $f: \mathbb{R} \to \mathbb{R}^2$ -- can be represented as a parametric \textbf{curve}, with 1 time dimension and 2 space dimensions.
    \item $f: \mathbb{R}^3 \to \mathbb{R}$ -- can be represented as labelling each point in 3-space with a value (e.g. temperature).
    \item $f: \mathbb{R}^2 \to \mathbb{R}^2$ -- can be considered a transformation in 2-space.
\end{itemize}

\subsection{Representing N-space on Cartesian Axes}
A figure in N-space represents N values (from both the domain and codomain) per point. For instance, a number line represents 1-space, and the xy-plane represents 2-space.

\subsubsection{Axes}
For 3-space, we conventionally use 3 mutually perpendicular axes -- $x, y, z$, meeting at an origin $O(0,0,0)$ -- to represent the three dimensions. By convention, these axes follow the ``right-hand rule", with the alternative ``left-hand rule" leading to certain negations in formulae.
This model is similar to the axes in 2-space, where the RHR does not make a difference, but is harder to visualize in 4-space.\footnote{Most of us cannot visualize 4-space with 4 orthogonal axes. Please talk to your doctor if this is not the case...}

\paragraph{Sections}
There are 4 quadrants in 2-space, and 8 octants in 3-space.

\subsubsection{Line Formulae}
These familiar formulae for 2D lines extend to 3D in intuitive ways.

\paragraph{Distance}
The 2D distance formula
$$d = \sqrt{(\Delta x)^2 + (\Delta y)^2}$$
generalizes readily into the 3D distance formula
$$d = \sqrt{(\Delta x)^2 + (\Delta y)^2 + (\Delta z)^2}$$
based on the Pythagorean Theorem.

\paragraph{Midpoint}
The formula for a midpoint on a 2D line between $(x_1, y_1)$ and $(x_2, y_2)$
$$M\left(\frac{x_1+x_2}{2},\frac{y_1+y_2}{2}\right)$$
generalizes readily into the 3D midpoint formula for points $(x_1, y_1, z_2)$ and $(x_2, y_2, z_2)$
$$M\left(\frac{x_1+x_2}{2},\frac{y_1+y_2}{2}, \frac{z_1+z_2}{2}\right)$$
Alternative coordinate systems are possible, and will be discussed in a future section.

\paragraph{Partitioning}
The point P partitioning a line segment AB determined by points $A(x_1,y_1)$ and $B(x_2,y_2)$ into segments with lengths in the ratio $a:b$ has coordinates
$$P\left(x_1+\frac{a}{a+b}\left(x_2-x_1\right), y_1+\frac{a}{a+b}\left(y_2-y_1\right)\right) = P\left(\frac{ax_2 + bx_1}{a+b}, \frac{ay_2 + by_1}{a+b}\right)$$
Similarly, the point P partitioning AB in 3-space between $A(x_1, y_1, z_1)$ and $B(x_2, y_2, z_2)$ into segments with lengths in the ratio $a:b$ has coordinates
$$P\left(\frac{ax_2 + bx_1}{a+b}, \frac{ay_2 + by_1}{a+b}, \frac{az_2 + bz_1}{a+b}\right)$$

\subsubsection{Contours}
Contours allow for the construction of a $N$-dimensional representation/sampling of a $N+1$-dimensional function. For instance, contour surfaces are 3D samples of a 4D space.
We use contour curves\footnote{Although the textbook uses the term level curve interchangably it may be helpful to depict the ``level" as the $z$-value in 3-space.} to represent a 3D surface with points $(x,y,z)$ by projecting the curves produced for a fixed $z=c$ onto the $xy$-plane.
\iffalse
Below is a (low resolution) contour plot of the top half of a sphere:
\begin{center}
\begin{tikzpicture}
\begin{axis}
[
    title={Contour plot of $x^2 + y^2 + z^2 = 1$},
    view={0}{90}
]
\addplot3[
    contour gnuplot={levels={1, 0.8, 0.6, 0.4, 0.2, 0}}
]
{sqrt(x^2+y^2)};
\end{axis}
\end{tikzpicture}
\end{center}
\fi
It is useful to have constant $\Delta z$ intervals so that the contours can visually represent the steepness of the surface.

\section{Notable 2D Figures}

\subsection{Lines}

\subsubsection{Standard Form} The set of points satisfying a linear equation of the form $$Ax + By + C = 0$$
Although there are 3 nominal coefficients, there are only 2 effective coefficients because (assuming at least one of $A,B$ is non-zero), we can divide by a coefficient, thus leaving 2 unknown ratios as coefficients. This explains why 2 points are sufficient to uniquely define a line, and distinct lines can only intersect at 1 point.

\subsubsection{Additional Equations}
\begin{itemize}
    \item Point-Slope: $y-y_1 = m(x-x_1)$
    \item Slope-Intercept: $y=mx+b$
    \item Slope Form: $m=\frac{y-y_1}{x-x_1}$
    \item Intercept Form: $\frac{x}{a} + \frac{y}{b} = 1$, axis intercepts at $(a, 0)$ and $(0, b)$
    \item Polar Form: $A\left(r\cos{\theta}\right)+B\left(r\sin{\theta}\right) + C = 0$
    \item Normal Form: $x\cos{\alpha} + y\cos{\beta} = d$, d is the length of the normal to the origin, $\alpha$ is the angle of the normal to the x-axis, $\beta$ is the angle of the normal to the y-axis
    \item Determinant Form: $\begin{vmatrix}
x & y & 1\\
x_1 & y_1 & 1 \\
x_2 & y_2 & 1
\end{vmatrix}= 0$
\end{itemize}

\subsubsection{Distance from a Point} \label{pt-dist-2-space}
The distance to a line $ax+by+c=0$ from point $(x_1, y_1)$ is
$$d=\frac{\left|ax_1 + by_1 + c\right|}{\sqrt{a^2+b^2}}$$

\subsubsection{Area of a Triangle in 2-space}
Given points $(x_1, y_1), (x_2, y_2), (x_3, y_3)$, the area of the triangle formed by these points is
$$A = \frac{1}{2}\begin{vmatrix}
    x_1 & y_1 & 1 \\
    x_2 & y_2 & 1 \\
    x_3 & y_3 & 1 \\
\end{vmatrix} = \frac{1}{2}\left|(x_1-x_2)(y_2-y_3) - (x_2 - x_3)(y_1 - y_2)\right|$$
The latter form can be derived using the distance from a point formula. See section \ref{determinant} for more information about the former form.
This is also known as the shoelace formula, because it can be represented as taking the points in order (clockwise/counterclockwise) and ``lacing" the coordinates to form equivalent products.

\subsubsection{Dimensional Analysis of the Determinant Method}
In the case of the determinant representing a triangle's area, we can translate and rotate the triangle -- since these are rigid motions, area is preserved -- making a new figure with coordinates that form a matrix with the following determinant
$$\begin{vmatrix}
    0 & 0 & 1 \\
    x_1' & 0 & 1 \\
    x_2' & y_2' & 1 \\
\end{vmatrix} = x_1'y_2'$$
where $x_1'$ represents the base length (rotated onto the x-axis), and $y_2'$ represents the height (rotated to be parallel to the y-axis). This determinant represents area (square distance units).


\subsubsection{Projection}
The projection of a line segment with slope $m$ and length $l$ onto the $x$-axis is
$$\Delta x = l\sqrt{\frac{1}{1+m^2}}$$

\subsubsection{Angle Between Lines}
For two intersecting lines with slopes $m_1$ and $m_2$, $m_1 < m_2$ the angle between the two can be found by $$\arctan{m_2} - \arctan{m_1} = \theta$$

\subsection{Conic Sections}

\paragraph{General Equation} The set of points satisfying a degree 2 equation of the form $$Ax^2+By^2 + Cxy + Dx + Ey + F =0$$
There are 6 nominal coefficients and 5 effective coefficients. This explains why 5 points are sufficient to uniquely define a conic section, and distinct conic sections can intersect at a maximum of 4 points.

\subsection{Cubics}

\paragraph{Standard Form} The set of points satisfying a degree 3 equation of the form $$Ax^3 + By^3 + Cx^2y + Dxy^2 + Ex^2 + Fy^2 + Gxy + Hx + Jy + K = 0$$
There are 10 nominal coefficients and 9 effective coefficients. However, distinct conics can intersect at more than 8 points. This is known as the Euler-Cramer Paradox.

\subsubsection{The Euler-Cramer Paradox}
For polynomials of degree $\ge 3$, Because the system of intersection points may contain rows that are \textbf{not} linearly independent, it is possible that that the determinant of the matrix would be $0$: there is not a unique solution curve for the system. See section \ref{determinant} for more information.

\section{Determinants} \label{determinant}
A \textbf{matrix} is a rectangular array of numbers.\footnote{Tensors can be considered a generalization of matrices to higher dimensions.} The dimensions are denoted $N \times M$, with $N$ being the number of rows, and $M$ being the number of columns. By convention, matrix variables are denoted with capital letters. Matrix elements can be addressed with subscripts $a_{n,m}$, with the comma delimiter being optional.
$$A = \begin{bmatrix}
 a_{11} & a_{12} & a_{13} & \cdots & a_{1m} \\
 a_{21} & a_{22} & a_{23} & \cdots & a_{2m} \\
 \vdots & \vdots & \vdots & \vdots & \vdots \\
 a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nm} \\
\end{bmatrix}$$
A \textbf{determinant} is a function that maps a real number to every \textbf{square} matrix.
The determinant of a $1\times1$ matrix $\left[a\right]$ is the scalar $a$, and the determinant of a $2\times2$ matrix $\left[\begin{smallmatrix} a & b \\ c & d \end{smallmatrix}\right]$ is $ad - bc$.

\subsection{Recursive (Cofactor) Definition}
Given an element $a_{ij}$ of matrix $A$, the corresponding minor $M_{ij}$ is the determinant obtained by deleting row $i$ and column $j$ of matrix A.
The cofactor (co-factor) of $a_{ij}$ is $$C_{ij} = (-1)^{i+j}M_{ij}$$
The determinant of matrix A is $$\det{A} = \sum_{j=1}^n a_{ij}C_{ij} = \sum_{i=1}^m a_{ij}C_{ij}$$ given a fixed value for $i$ (selecting a row) or $j$ (selecting a column) respectively.
To perform this by hand, one can take an arbitrary row/column\footnote{It is usually helpful to select a row/column with values of $0$ or $1$ to minimize needed calculations.} and then take the minors of each element in that row/column, multiplying against the sign of the cofactor. Note that the sign of the cofactor alternates in a pattern like so:
$$\begin{bmatrix}
    + & - & + & - & \cdots \\
    - & + & - & + & \cdots \\
    + & - & + & - & \cdots \\
    \vdots & \vdots & \vdots & \vdots & \ddots \\
\end{bmatrix}$$

\subsection{Explicit Definition}
This definition is listed as a fun aside; it is useful for proving determinant properties for the general case.
For the set of all permutations $S_n$, with the signature function $\sgn(\sigma)$ denoting $(-1)^n$, with $n$ being the number of transpositions (element swaps) from the original ordering of elements to get the permutation $\sigma(i)$:
$$\det{A} = \sum_{\sigma \in S_n} \sgn{\sigma} \prod_{i=1}^n a_{i,\sigma(i)}$$

\subsection{Determinant Properties}
Note that many of these properties do not have standard names.
The sum property (\ref{det-sum}), combined with the switching property (\ref{det-switch}), are sometimes referred to as the \textbf{multilinearity} of determinants in columns/rows.

\subsubsection{Scalar Multiple Property}
If any row or column is multiplied by a constant $k$, the determinant value gets multiplied by $k$.

\paragraph{Proof} Consider the recursive determinant sum for the new matrix, using the row/column that was multiplied. $k$ can be factored out of the entire sum, leaving the original determinant sum multiplied by the scalar $k$.

\subsubsection{Sum Property} \label{det-sum}
If all the elements of a row or column are expressed as a summation of two or more addends, then the determinant can be broken down as a sum of corresponding smaller elements.
$$
    \begin{vmatrix}
        a_{11} + b_{11} & a_{12} & a_{13} \\
        a_{21} + b_{21} & a_{22} & a_{23} \\
        a_{31} + b_{31} & a_{32} & a_{33} \\
    \end{vmatrix}
    =
    \begin{vmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33} \\
    \end{vmatrix}
    +
    \begin{vmatrix}
        b_{11} & a_{12} & a_{13} \\
        b_{21} & a_{22} & a_{23} \\
        b_{31} & a_{32} & a_{33} \\
    \end{vmatrix}
$$
This property can be applied multiple times to separate rows.

\paragraph{Proof} Consider the explicit definition of the determinant. Choosing $i$ or $j$ to target the addends' row/column, we can use the distributive property to split the product into two, which represent the determinant of the separate matrices.

\subsubsection{Switching Property} \label{det-switch}
If we interchange any two rows/columns of the determinant, the determinant is multiplied by $-1$.

\subsubsection{Sum of Rows Property} \label{det-sum-rows}
Let $A$ be an $n\times n$ matrix and let B be a matrix which results from adding a multiple of a row/column to another row/column. Then $\det{A} = \det{B}$.

\subsubsection{Zero Property}\label{det-zero}
The determinant of a matrix with a row/column of zeroes is $0$.

\paragraph{Proof} Consider the recursive definition of the determinant. Choosing the row with $0$s will lead to $a_{ij}$ always being $0$, so the final sum is 0.
This property allows us to show that since the determinant form of a line results in an expression of the form $Ax+By+C=0$, and that the two points $(x_1,y_1)$ and $(x_2,y_2)$ satisfy this equation, the two points definitively show that the solutions to the equation form the line.

\subsubsection{Identical Column Zero Property}
If a (square) matrix has two identical rows/columns (or rows that are a linear combination of others), then its determinant is zero.

\paragraph{Proof} Using the sum of rows property (\ref{det-sum-rows}), we can manipulate the matrix such that the row is only $0$s, thus allowing us to apply the zero property (\ref{det-zero}).\footnote{Although not mentioned in class, the determinant of a transposed matrix is equal to the original matrix, making this applicable to columns too.}

\section{3D Figures}
To draw a 3D figure, it is helpful to consider its level curves and traces (cross-sections in planes such as the $xz$ or $yz$ plane) by setting one of the variables $=0$ or $=c$.

\subsection{Planes}
\subsubsection{Drawing a Plane}
Because planes extend infinitely, we draw a quadrilateral or triangular slice of a plane in 3-space to represent it.

\subsubsection{Special Cases}
A single variable equal to a constant represents a plane parallel to the other axes' plane. For instance, the $xz$-plane can be represented with $y=0$, and $y=c$ produces planes parallel to the $xz$-plane.

A linear equation in 2 variables (e.g. $y=mx+b$) produces a plane in 3-space that intersects the equivalent line on its corresponding plane (e.g. the $xy$-plane where $z=0$) and serves as a cylinder with rulings parallel to the third axis.

\subsubsection{General Linear Functions}
$$ax+by+cz=d$$
The contours of a general linear function are evenly spaced, parallel lines. (Proof: Consider $z=c$. A series of lines linearly dependent on $c$ is produced.)
The constant term $d$ corresponds with the z-intercept ($0$, $0$, $\frac{d}{c}$).

\subsubsection{Point-Slope-Slope}
Given the slopes $m_x=\frac{\Delta{z}}{\Delta{x}}$ and $m_x=\frac{\Delta{z}}{\Delta{y}}$ and a point ($x_1$, $y_1$, $z_1$) on the plane:
$$z-z_1 = m_x(x-x_1) + m_y(y-y_1)$$
$m_x$ and $m_y$ are most easily calculated when you have lines on the $xz$ and $yz$ planes respectively, or pairs of points on parallel points (i.e. ($x_1$, $c$, $z_1$) and ($x_2$, $c$, $z_2$) for $m_x$). Note that point-slope-slope form can be rearranged into standard form. $m_x$ and $m_y$ can also be solved for from standard form by isolating $z$ and dividing by $-c$.

\subsubsection{Intercept Form}
For a plane intersecting the x-axis at $(a,0,0)$, the y-axis at $(0,b,0)$, and the z-axis at $(0,0,c)$, the equation for the plane can be defined as:
$$\frac{x}{a}+\frac{y}{b}+\frac{z}{c}=1$$
Intercept form is useful for providing the three non-collinear points that can be used to draw a plane.

\subsubsection{``Slope" on a Plane}
For an arbitrary plane defined by a line with angle $\theta$ to the $x$-axis in the $xy$-plane, the slope $m_\theta$ of the intersection line between this vertical plane and a plane with slopes $m_x$ and $m_y$ is
$$m_\theta = m_x\cos{\theta} + m_y\sin{\theta}= m_x\frac{\Delta x}{\Delta t} + m_y\frac{\Delta y}{\Delta t}$$ as shown by the right triangle (with angle $\theta$) formed by the line, the $x$-axis, and the $y$-axis.

An alternative form for $m_\theta=\frac{\Delta z}{\Delta l}$, with $l$ being the distance along the line defined by $\theta$, is $$m_\theta = \frac{m_x+m_y\tan{\theta}}{\sqrt{1+\tan^2{\theta}}}$$

\subsubsection{Distance from a Point}
The distance to a plane $ax+by+cz+d=0$ from point $(x_1, y_1, z_1)$ in 3-space is similar to that of a point to a line in 2-space (\ref{pt-dist-2-space}).
\begin{align*}
d=\frac{\abs{ax_1+by_1+cz_1+d}}{\sqrt{a^2+b^2+c^2}} =
\comp_{\langle a, b, c \rangle} \vect{D} = \frac{\langle a, b, c \rangle \cdot \vect{D}}{\magnitude{\langle a, b, c \rangle}}
\end{align*}

This can be derived by taking the parallel component of the difference/distance vector $\vect{D} = \langle x-x_1, y-y_1, z-z_1\rangle$ between a point $(x,y,z)$ on the plane and the point $(x_1, y_1, z_1)$ and the perpendicular to the plane $\langle a, b, c \rangle$.

\paragraph{Distance between Planes}
The distance between planes $ax + by + cz + d = 0$ and $ax + by + cz + e = 0$ can be derived from the distance between the first plane and a point $(x_0, y_0, z_0)$ on the second plane.
\[
D=\frac{\abs{d-e}}{\sqrt{a^2+b^2+c^2}}
\]

\subsection{Cylinders}
Cylinders with rulings (straight lines) parallel to an axis are represented with equations in 2 variables for 3-space.
For instance, $1 = x^2 + y^2$ represents a circular cylinder, with rulings parallel to the omitted $z$-axis.
Other equations can produce non-circular cylinders.

\subsection{Quadric Surfaces}
Quadric surfaces are second-degree polynomials in 3-space. Equations are given for the surfaces on the $z$ axis, but these variables can be interchanged to produce surfaces along the $x$ or $y$ axes.

\subsubsection{Ellipsoids}
An ellipsoid has traces that are all ellipses. In analogy to 2D ellipses, the variables $a$, $b$, $c$ represents half the width of the ellipse along that axis.
$$\frac{x^2}{a^2} + \frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$

\paragraph{Spheres} are special cases of the ellipsoid (in which $a=b=c=r$), and they can also be defined as the set of points equidistant from a center, based on the 3D distance formula. A sphere of radius $r$ centered at point $(h, k, l)$ has equation
$$(x-h)^2+(y-k)^2+(z-l)^2=r^2$$

\subsubsection{Cones}
Cones have elliptical level curves, and hyperbolic vertical traces (or in the degenerate case at the origin, two crossing lines similar to the absolute value function).
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} = \frac{z^2}{c^2}$$
Notice that if instead of $z^2$, one uses $z=\sqrt{\ldots}$, a ``single" cone appears.

\subsubsection{Elliptic Paraboloid}
Elliptic paraboloids have parabolic vertical cross sections, and elliptical horizontal contours. Wow!
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} = \frac{z}{c}$$

\subsubsection{Hyperbolic Paraboloid}
Hyperbolic paraboloids have parabolic vertical cross sections, and elliptical horizontal contours. Also wow!
$$\frac{x^2}{a^2} - \frac{y^2}{b^2} = \frac{z}{c}$$
The hyperbolic paraboloid is also a ruled surface -- it is possible to run a series of straight lines across it.

\subsubsection{Hyperboloid of One Sheet}
Horizontal traces are ellipses, and vertical traces are hyperbolas (degenerate at the plane $y=b$ or $x=a$).
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2}=1$$

\subsubsection{Hyperboloid of Two Sheets}
Horizontal traces are ellipses, and vertical traces are hyperbolas. The textbook likes to say that 2 minus signs correlate with 2 sheets.
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2}= -1$$
$$-\frac{x^2}{a^2} - \frac{y^2}{b^2} + \frac{z^2}{c^2}= 1$$

\section{Vectors}

Vectors possess both magnitude and direction. They can be represented as arrows (directed line segments). They possess an initial point (tail) and a terminal point (arrow-head).
There is a one-to-one correspondence between points and (position) vectors.

\subsection{Notation and Terminology}
Vectors are usually represented with bolded or lowercase variables $\Vec{a}$, with the arrow head pointing towards the terminal point (e.g. $\overrightarrow{PQ}$). Examples in this section will be in 2-space, but can be generalized to 3-space.

Free vectors are vectors with initial points "freely roaming" about space. Position vectors are vectors with initial points on the original of space. These vectors are equivalent (see below).

The magnitude of a vector is denoted $\left|\Vec{a}\right|$ or $\left|\left|\Vec{a}\right|\right|$. This symbol is not the absolute value operator when vectors are involved.

A unit vector is a vector with a magnitude of $1$. Unit vectors are represented with a hat, such as the ortho-normal basis vectors\footnote{ortho(gonal -- perpendicular; normal -- magnitude 1; basis vectors -- vectors that can be linearly combined (i.e. added and scaled) to represent all of space} $\hat{i} = \langle1, 0$, $0\rangle$, $\hat{j} = \langle 0, 1, 0 \rangle$, $\hat{k} = \langle 0, 0, 1 \rangle$ pointing in the $x$, $y$, and $z$ directions.

The coordinates of the terminal point of a position vector are called the components of the vector $\left\langle a_1, a_2 \right\rangle$.

\subsection{Properties of Vectors}
\begin{itemize}
    \item $\Vec{a}=\Vec{b}$ if they have the same magnitude and direction. Thus, a vector is the same regardless of what specific points it originates/ends at.
    \item $-\overrightarrow{PQ} = \overrightarrow{QP}$, the negation of a vector reverses its direction.
    \item $\overrightarrow{0}$ is defined to have no direction and $0$ magnitude.
\end{itemize}

\subsection{Operations on Vectors}
\subsubsection{Addition and Subtraction}
Addition of vectors $\overrightarrow{a}$ and $\overrightarrow{b}$ can be defined as adding their components.
$$\langle a_1, a_2 \rangle + \langle b_1, b_2 \rangle = \langle a_1 + b_1, a_2 + b_2\rangle$$

Graphically, addition of two vectors can be represented as attaching the head of on vector to the tail of another, with their sum being the vector represented by the tail of the first vector and the head of the final vector. Alternatively, the parallelogram bounding the two vectors (with tails at the same point) contains the sum vector as its diagonal.

Subtraction of vectors is the addition of the negative vector.

\subsubsection{Magnitude}
The magnitude can be represented as the distance from the tail to the head. For a vector $\overrightarrow{a} = \langle a_1, a_2 \rangle$:
$$\left|\overrightarrow{a}\right| = \sqrt{a_1^2+a_2^2}$$

\paragraph{Unit Vector in the Direction of a Given Vector}
For an arbitrary vector $\overrightarrow{a}$:
$$\hat{a} = \frac{\overrightarrow{a}}{\left|\left|\overrightarrow{a}\right|\right|} = \left\langle \frac{a_1}{\left|\left|\overrightarrow{a}\right|\right|}, \frac{a_2}{\left|\left|\overrightarrow{a}\right|\right|}, \frac{a_3}{\left|\left|\overrightarrow{a}\right|\right|} \right\rangle$$
For a 2-space vector with direction described by angle $\alpha$ to the $x$-axis (and $\beta$ as its complement to the $y$-axis):
$$\hat{a} = \left\langle \cos{\alpha}, \sin{\alpha} \right\rangle = \left\langle \cos{\alpha}, \cos{\beta} \right\rangle$$
This can be generalized to 3-space by drawing perpendiculars from the terminal point of the vector to each of the axes in space, and taking the cosines of each obtained angle.
$$\hat{a} = \left\langle \cos{\alpha}, \cos{\beta}, \cos{\gamma} \right\rangle$$

\subsubsection{Scalar Multiplication}
Multiplying a vector by a scalar $k$ results in a vector in the same direction, but with the components scaled by $k$:
$$k\overrightarrow{a} = \langle ka_1, ka_2 \rangle$$
The magnitude of the new vector is $k\left|\left|\overrightarrow{a}\right|\right|$.

\section{Vector Products}
Vector "multiplication" encompasses multiple defined products between vectors.

\subsection{Dot Product}
The dot product ("inner product", "scalar product") of two vectors $\vect{a} = \langle a_1, a_2, a_3 \rangle$, $\vect{b} = \langle b_1, b_2, b_3 \rangle$, with an angle $\theta$ between them, is the scalar
$$\vect{a} \cdot \vect{b} = \magvect{a}\magvect{b}\cos{\theta} = a_1b_1 + a_2b_2 + a_3b_3$$
Using a geometric interpretation of the two vectors and their difference, one can use the law of cosines to prove the equivalence of the two definitions.

As will be shown in the next sections, it is useful to conceptualize sums of products as dot products.

\subsubsection{Projection}
The scalar projection of a vector \vect{a} onto \vect{b} is the "component" of \vect{a} on \vect{b}, delimited by a perpendicular to \vect{b} drawn from the tip of \vect{a}. With the angle between \vect{a} and \vect{b} being $\theta$, this is
$$\comp_{\vect{b}} \vect{a} = \magvect{a}\cos{\theta} = \frac{\vect{a} \cdot \vect{b}}{\magvect{b}}$$

The vector projection of a vector \vect{a} onto \vect{b} is the "projection" of \vect{a} on \vect{b} pointing in the same direction at \vect{b}.
$$\proj_{\vect{b}}\vect{a} = \left(\comp_{\vect{b}}\vect{a}\right)\hat{b} = \frac{\vect{a} \cdot \vect{b}}{\magvect{b}^2} \vect{b}$$

\subsubsection{Properties of the Dot Product}
\begin{enumerate}
    \item Commutative Property: $\vect{a} \cdot \vect{b} = \vect{b} \cdot \vect{a}$
    \item Distributive Property\footnote{Over addition/subtraction}: $c\left(\vect{a} \cdot \vect{b}\right) = c\vect{a} \cdot \vect{b} = \vect{a} \cdot c\vect{b}$
    \item Scalar Multiplication Property: $\vect{a} \cdot \left(\vect{b} + \vect{c}\right) = \vect{a} \cdot \vect{b} + \vect{a} \cdot \vect{c}$
    \item Zero Property: $\vect{a} \cdot \vect{0} = 0$
    \item ``Squaring" Theorem: $\vect{a} \cdot \vect{a} = \magvect{a}^2$
\end{enumerate}
Note that $\vect{0}$ is the zero vector, with $0$ magnitude and undefined direction.

\subsubsection{Theorem 1: The Cauchy-Schwartz Inequality}
$$\vect{a} \cdot \vect{b} \le \magvect{a}\magvect{b}$$

The Arithmetic Mean/Quadratic Mean (AM-QM) Inequality can be proven using a vector $\langle1, 1, 1, ...\rangle$:
$$\frac{a_1 + a_2 + \cdots + a_n}{n} \le \sqrt{\frac{a_1^2 + a_2^2 + \cdots + a_n^2}{n}}$$

\subsubsection{Theorem 2: Perpendicular Vectors}
$$\vect{a} \perp \vect{b} \iff \vect{a} \cdot \vect{b} = 0$$ for $\vect{a}, \vect{b} \ne 0$, such that $\magvect{a}, \magvect{b} \ne 0$, implying that for $\magvect{a}\magvect{b}\cos{\theta} = 0$, $\cos{\theta} = 0$. More generally, the solutions for $\theta$ indicate the angle between two vectors.

\subsubsection{Perpendiculars and Angles to Lines}
A line $Ax + By + C = 0$ is parallel to $Ax + By = 0$. This can be rewritten as the dot product $\langle A, B \rangle \cdot \langle x, y \rangle$, with $\langle x, y \rangle$ representing the set of vectors parallel to the line. From Theorem 2,
$$\langle A, B \rangle \perp Ax + By + C = 0$$
the vector $\langle A, B \rangle$, along with its family of scaled vectors, is perpendicular to the line.

One can also determine the angle between lines $A_1x + B_1y = C1$ and $A_2x + B_2y = C_2$ by taking the dot product between vectors parallel to the lines (or perpendicular vectors, i.e. $\langle A_1, B_1 \rangle \cdot \langle A_2, B_2 \rangle$).

\subsubsection{Perpendiculars to Planes}
The set of all vectors $\langle x, y, z \rangle$ perpendicular to a vector $\langle A, B, C \rangle$ in 3-space is a plane.
\begin{align*}
    \langle A, B, C \rangle \cdot \langle x, y, z \rangle &= 0 \\
    Ax + By + Cz &= 0
\end{align*}

To obtain the specific plane perpendicular to a vector passing through point $(x_0, y_0, z_0)$, one evaluates the dot product
\begin{align*}
    \langle A, B, C \rangle \cdot \langle x - x_0, y - y_0, z - z_0 \rangle &= 0 \\
    A\left(x - x_0\right) + B\left(y - y_0\right) + C\left(z - z_0\right) &= 0
\end{align*}

Note that a plane of the form $z = m_xx + m_yy$ can be rewritten as
\begin{align*}
    0 &= m_xx + m_yy - z \\
    0 &= \langle m_x, m_y, -1 \rangle \cdot \langle x, y, z \rangle
\end{align*}

\subsection{Cross Product}
The cross product ("vector product", "outer product") of $\vect{a} = \langle a_1, a_2, a_3 \rangle$ and $\vect{b} = \langle a_1, a_2, a_3 \rangle$ is the vector
$$\vect{a} \times \vect{b} = \left\langle a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1\right\rangle =
    \begin{vmatrix}
        \hat{i} & \hat{j} & \hat{k} \\
        a_1     & a_2     & a_3     \\
        b_1     & b_2     & b_3     \\
    \end{vmatrix}
$$
This vector is constructed such that $\vect{a} \times \vect{b} \perp \vect{a}$ and $\vect{a} \times \vect{b} \perp \vect{b}$, representing the family of vectors perpendicular to the perpendicular vectors of two planes, or the vector of the line of intersection between those planes. This third vector satisfies the right-hand rule.

\subsubsection{Properties of the Dot Product}
\begin{enumerate}
    \item Zero Property: $\vect{a} \times \vect{0} = \vect{0} = \vect{0} \times \vect{a}$
    \item $\vect{a} \times \vect{b} \perp \vect{a}$ and $\vect{a} \times \vect{b} \perp \vect{b}$
    \item Anti-Commutative Property: $\vect{a} \times \vect{b} = -\left(\vect{a} \times \vect{b}\right)$
    \item Distributive Property: $\vect{a} \times \left(\vect{b} + \vect{c}\right) = \vect{a} \times \vect{b} + \vect{a} \times \vect{c}$
\end{enumerate}
\subsubsection{Theorem 1: Magnitude of the Cross Product}\label{mag-cross-product}
The magnitude of the cross product is
$$\magnitude{\vect{a} \times \vect{b}} = \magvect{a}\magvect{b}\sin{\theta}$$

\paragraph{Lemma} Through algebraic expansion, it can be shown that $$\magnitude{\vect{a} \times \vect{b}}^2 = \magvect{a}^2\magvect{b}^2 - \left(\vect{a} \cdot \vect{b}\right)^2$$
Using the non-coordinate definition of the dot product, we can use the identity $1-\cos^2{x} = \sin^2{x}$ to prove the expression for the magnitude of a cross product where $0 \le \theta \le \pi$.

\paragraph{Corollaries}
The orthogonal component of $\vect{a}$ onto $\vect{b}$ is $$\frac{\magnitude{\vect{a}\times\vect{b}}}{\magvect{b}}$$
Using the orthogonal component as a height, it can be shown that cross product of two vectors also represents the area of the parallelogram bounded by the two vectors. (Half of this area is the triangle bounded by the two vectors.)

\subsubsection{Theorem 2: Parallel Vectors}
From \ref{mag-cross-product}, if $\vect{a}, \vect{b} \ne \vect{0}$, then $$\vect{a} \times \vect{b} = 0 \iff \vect{a} \parallel \vect{b}$$

\subsection{Lines in 3-space}
A line in 3-space can be defined as a vector-valued function ($\mathbb{R} \to \mathbb{R}^n$) or an equivalent parametric curve.
Lines in 3-space can be described as
\begin{itemize}
    \item Intersecting
    \item Parallel, if
    \begin{itemize}
        \item there exists a plane containing the two non-intersecting lines, and/or
        \item the two lines have direction vectors that are scalar multiples of one another, and/or
        \item the distance between the lines is constant.
    \end{itemize}
    \item Skew, if
    \begin{itemize}
        \item the lines are neither parallel or skew, and/or
        \item the lines cannot be contained by a single plane, and/or
        \item the lines can be contained in two parallel planes.
    \end{itemize}
\end{itemize}
intersect, be parallel, or skew (the lines live in different parallel planes).

They can be either intersecting or parallel with planes.

\subsubsection{Vector Form}
Treating the set of points $(x,y,z)$ as analogous vectors:
$$\langle x, y, z \rangle = \langle x_o, y_o, z_o \rangle + \langle a, b, c \rangle t = \vect{r}(t)$$
The initial point is $\langle x_o, y_o, z_o \rangle$, and $\langle a,b,c \rangle$ is the direction vector. $t \in \mathbb{R}$, it is a scalar that allows the direction vector to trace out the whole line.\\
There are two methods for finding the line of intersection between two points.
\paragraph{Method I} By setting the equations of the two planes equal to one another, we can obtain linear expressions for all three variables in terms of a fourth parameter $t$ (i.e. there is a single degree of freedom in this system of 2 equations and 3 unknowns). Any constants in these expressions are used in the initial point, any variable terms can have $t$ factored out to produce the direction vector term.

\paragraph{Method II} The cross product of the perpendiculars to two planes is parallel to the line of intersection, $\vect{n_1} \times \vect{n_2}$ can be used as a direction vector.

The offset can be determined by setting one of the variables $x, y, z = 0$, and solving for the other two variables through the system of 2 planes, leading to a point that can be used for the initial point.

\subsubsection{Parametric Form}
The linear expressions for $x,y,z$ mentioned in Method I can be used as functions in a parametric equation:
\begin{align*}
    x(t) &= x_o + at \\
    y(t) &= y_o + bt \\
    z(t) &= z_o + ct
\end{align*}

\paragraph{Symmetric Form}
We can also express the line as the set of points satisfying this equation:
$$
\frac{x-x_o}{a} = \frac{y-y_o}{b} = \frac{z-z_o}{c}
$$
The value of this ratio can be considered the ``value of $t$" in the functional forms of the line.

\subsubsection{Distance of Point to Line}
Using a difference/distance vector from the point to a point on the line, we can take the cross product to get the distance to the line.

\subsubsection{Distance of Skew Lines}
Using the cross product of the two lines' direction vectors, we can find the parallel planes that contain the lines. This can then be used to find the distance between the lines.

\subsection{Scalar Triple Product}
The scalar triple product is the product of three vectors
\[
    \vect{a}\cdot\vect{b}\times\vect{c}=\vect{a}\times\vect{b}\cdot\vect{c} =
    \begin{vmatrix}
        a_1 & a_2 & a_3\\
        b_1 & b_2 & b_3\\
        c_1 & c_2 & c_3\\
    \end{vmatrix}
\]
The value of the scalar triple product represents the volume of the parallelepiped (tilted box/prism with parallelogram faces) with sides represented by the three co-terminal vectors.

If the three vectors are co-planar, then the scalar triple product will be equal to $0$.

\section{3D Limits}
The definition for limits of functions $f: \mathbb{R}^2 \to \mathbb{R}$ is:
\[
    \lim_{(x,y)\to(a,b)}f(x,y) = L
\] if and only if $\forall\epsilon>0, \exists\delta>0 \ni 0<\sqrt{(x-a)^2+(y-b)^2}<\delta \Rightarrow \abs{f(x,y) - L}<\epsilon$.\footnote{The limit of a function $f(x,y)$ is equal to $L$ if and only if for all positive $\epsilon$, there exists a positive $\delta$ such that if the point $(x,y)$ is within a (circular, but could also be rectangular) neighborhood constrained by distance $\delta$, it is guaranteed that the difference between $f(x,y)$ and the value $L$ will be less than $\epsilon$.}

\subsection{Showing Limits DNE}
It is easier to prove that a limit does not exist than it is to show that one exists. In both cases, we choosing relations between $x$ and $y$ (e.g. setting $x=a$/$y=b$, or $y=f(x)$/$x=f(y)$) that constrain the limit to a single variable and allow us to use familiar methods of analysis. Note that L'Hôpital's rule only applies for single-variable limits.

\subsubsection{The One-Path Approach}
If \(\lim_{(x,y)\to(a,b)}f(x,y) = DNE\) along some path (a continuous curve) $c_1$, then \(\lim_{(x,y)\to(a,b)}f(x,y)\) does not exist.

As a reminder, a limit going to $\pm\infty$ does not exist.

\subsubsection{The Two-Path Approach}
If \(\lim_{(x,y)\to(a,b)}f(x,y) = L_1\) along some path $c_1$, and if \(\lim_{(x,y)\to(a,b)}f(x,y) = L_2\) along some path $c_2$, and if $L_1\ne L_2$, then \(\lim_{(x,y)\to(a,b)}f(x,y)\) does not exist.

\subsection{Limit Properties}
Given that $f(x,y)=L$, $g(x,y)=M$; $L,M,k\in\mathbb{R}$ and $n\in\mathbb{N}$:
\begin{align*}
    &\lim_{(x,y)\to(a,b)}f(x,y)\pm g(x,y) = L \pm M \\
    &\lim_{(x,y)\to(a,b)}kf(x,y) = kL \\
    &\lim_{(x,y)\to(a,b)}f(x,y) \cdot g(x,y) = L \cdot M \\
    &\lim_{(x,y)\to(a,b)}\frac{f(x,y)}{g(x,y)} = \frac{L}{M} & \text{for } M\ne0 \\
    &\lim_{(x,y)\to(a,b)}(f(x,y))^n = L^n \\
\end{align*}

\subsection{Evaluating Limits}
As suggested by the two-path approach to proving limits do not exist, limits should have consistent values for \emph{all} "paths" taken to approach the limiting point.

\subsubsection{Continuity}
$f(x,y)$ is continuous at $(a,b)$ iff $f(a,b)=\lim_{(x,y)\to(a,b)}f(x,y) = f(a,b)$. This property be proven using $\epsilon-\delta$ proofs that accommodate arbitrary points on $f(x,y)$, or by use of limit properties that combine existing continuous functions.

Based on the Limit Properties, we can determine that sums and products of continuous functions are also continuous.\\
Continuous functions include (considering their respective domains):
\begin{itemize}
    \item all polynomials in $x,y$ (sums of terms in the form $cx^ny^m$, with constants $c\in\mathbb{R}$, $n,m\in\mathbb{N}\cup\left\{0\right\}$)
    \item trigonometric functions $\sin{x}$, $\cos{x}$, etc.
    \item $e^x$ and its inverse $\ln{x}$
\end{itemize}

\paragraph{Algebraic Manipulation}
Canceling denominators in a rational function, like in single-variable limits, is a useful means of simplifying the evaluation of a limit to one that can be evaluated using continuity. For denominators with square-roots, it may be advantageous to use a conjugate to rationalize the denominator.

\subsubsection{Substitution}
Letting $u=g(x,y)$, and letting $u\to g(a,b)$ may simplify the form of a function composed of $g(x,y)$ terms. Take note of range restrictions on $g(x,y)$, such as $g(x,y)=x^2+y^2$ implying that $u > 0$, the limiting value will be $0^+$.

\paragraph{Polar Substitution}
Using $x = r\cos{\theta}$, $y=r\sin{\theta}$, with $0 \le r$ and $0\le\theta<2\pi$ (so as to allow for nearly one-to-one mapping of Cartesian coordinates to polar coordinates), one can restate a function in terms of $r$ and $\theta$.

In particularly fortunate cases, $\theta$ can be eliminated using identities such as $\sin^2{\theta}+\cos^2{\theta}=1$. In other cases, it is useful to consider the bounds of trigonometric functions and construct comparisons that can be used with the Squeeze Theorem.

\subsubsection{Squeeze Theorem}
The Squeeze Theorem is rather useful in cases where we know a function with a simpler expression that is bound to be greater/less than that, or we have a limiting value like $0$.

This simpler expression could be derived by replacing a difficult function (e.g. $\sin$ or $\cos$) with bounds (e.g. $-1$ and $1$) or by comparing the function to one with a simpler numerator/denominator (e.g. $\frac{1}{x^2+4x+5} < \frac{1}{x^2}$).

\section{Derivatives}
\subsection{Definition of a Partial Derivative}
If $z=f(x,y)$, then the partial derivative of $f$ with respect to $x$ at point $(a,b)$ is
\begin{align*}
    f_x(a,b) &= \lim_{h\to0} \frac{f(a+h,b) - f(a,b)}{h} \\
    f_x(a,b) &= \lim_{x \to a} \frac{f(x,b)-f(a,b)}{x-a}
\end{align*}
similarly, the partial derivative of $f$ with respect to $y$ at $(a,b)$ is
\begin{align*}
    f_y(a,b) &= \lim_{h\to0} \frac{f(a,b+h) - f(a,b)}{h} \\
    f_y(a,b) &= \lim_{y \to b} \frac{f(a,y)-f(a,b)}{y-b}
\end{align*}

The partial derivative indicates the slope of a tangent line to a surface along a specific direction in the xy-plane. It does not guarantee that a surface is differentiable at a point.

\subsubsection{Application of Partial Derivatives} \label{partials-to-tangent}
We can determine a \emph{potential} tangent plane/normal vector to a surface at point $(a,b, f(a,b))$ using partial derivatives.

The potential tangent plane has equation
\begin{align*}
	z - f(a,b) &= f_x(x - a) + f_y(y - b)\\
	L(x,y) &= f_x(x-a) + f_y(y-b) + f(a,b)
\end{align*}
$L(x,y)$ can also be referred to as the "linearization of $f$," "linear approximation of $f$," or "tangent plane approximation of $f$."

The normal vector is
$$\langle f_x(a,b), f_y(a,b), -1 \rangle$$

Note that the mere existence of partials does not imply continuity nor differentiability of a surface.

\subsection{Evaluating Partial Derivatives}
Instead of explicitly evaluating the limit, we will usually use differentiation rules to obtain partial derivatives. For $f_x$/$f_y$, the other variable ($y$ or $x$, respectively) is considered a \emph{constant}, not a variable, so it is not necessary to use the Chain Rule like for implicit differentiation (as seen in Calc I).

\subsubsection{Higher Order Partial Derivatives}
A higher order partial derivative can be taken in the same direction as the first order derivative (with additional results being analogous to their 2-space equivalents, i.e. "concavity" on the second order derivative).

Calculating a higher order partial derivative uses the same differentiation rules as for the first order partial derivative, keeping in mind that the partial we are taking may be in a different variable (e.g. $\frac{\partial}{\partial x}$ then $\frac{\partial}{\partial y}$). For a function $\mathbb{R}^2\to\mathbb{R}$, there are $2^n$ possible permutation of the axial variables for an $n$-th order derivative.

Under suitable conditions $f_{xy} = f_{yx}$.

\subsubsection{Implicit Differentiation}
Considering $x,y$ as independent variables, with $z$ being dependent on a relation to $x,y$, we can implicitly differentiate for $\frac{\partial z}{\partial x}$ or $\frac{\partial z}{\partial y}$ (with $\frac{\partial y}{\partial x}$ and $\frac{\partial x}{\partial y}$ both being $0$, as independence implies that $x$ and $y$ are constant with respect to one another) using a Chain Rule analogous to functions in 2-space.

\subsection{Notation}
The partial (derivative) of $f$ (or $z=f$) with respect to $x$ can be written as:
\[
f_x(x,y)=f_x=\frac{\partial}{\partial x}\left[f(x,y)\right] = \frac{\partial f}{\partial x} = \frac{\partial z}{\partial x}=D_xf=f_1=D_1f
\]
Note that the $_1$ subscripts represent the direction of the \emph{first} independent variable (i.e. $x$).\\
This can be replaced with a general unit vector $\hat{u}$ when taking partial derivatives in other directions.

\subsubsection{Higher Order Partials}
The order in which partials are taken is reversed between the two notations we use. Using Leibniz Notation, the partials are read from right to left, as if they were function compositions. Using subscript notation, the partials are read from left to right.
\[
    \frac{\partial}{\partial y} \left(\frac{\partial f}{\partial x}\right) = \frac{\partial^2f}{\partial y \partial x} = f_{xy}(x,y) = f_{12}
\]

\subsection{Partials}
For a differentiable function, we define the differentials $dx=\Delta x$ (differential of x), $dy=\Delta y$, and $dz = f_x(a,b)\,dx + f_y(a,b)\,dy$. For small values of $\Delta x$ and $\Delta y$, $dz\approx\Delta z$

\subsection{Differentiability}
$f(x,y)$ is differentiable at $(a,b)$ if and only if
\[
	\lim_{(x,y)\to(a,b)} \frac{f(x,y) - L(x,y)}{\sqrt{(x-a)^2 + (y-b)^2}} = 0
\]
The existence of the partial derivatives $f_x(x,y)$ and $f_y(x,y)$ is necessary, but not sufficient to ensure differentiability.

Differentiability implies continuity of the original function and existence of the partials (which might not be continuous).

\subsubsection{Justification}
In 2-space, there exists an analogous linear approximation definition of the derivative, which generalizes more easily to 3-space.

Expanding the linearization definition, we can find the following results, showing that the slope of the surface fits on the tangent plane. Letting $f_x(a,b) = m_x$ and $f_y(a,b) = m_y$, and $\theta$ being the angle in which an alternative partial is taken:
\begin{align*}
	\lim_{(x,y)\to(a,b)} \frac{f(x,y) - f(a,b) - (m_x(x - a) + m_y(y - b))}{\sqrt{(x-a)^2+(y-b)^2}} &= 0\\
	\lim_{x\to a} \frac{f(x,b) - f(a,b) - m_x(x - a)}{\sqrt{(x-a)^2}} =
	 \lim_{x\to a} \frac{f(x,b) - f(a,b)}{\sqrt{(x-a)^2}} - m_x &= 0\\
	\lim_{y\to a} \frac{f(a,y) - f(a,b) - m_y(y - b)}{\sqrt{(y-b)^2}} =
	 \lim_{y\to a} \frac{f(a,y) - f(a,b)}{\sqrt{(y-b)^2}} - m_y &= 0\\
	\lim_{(x,y)\to(a,b)} \frac{f(x,y) - f(a,b) - (m_x\Delta{x} + m_y\Delta{y})}{\sqrt{(\Delta{x})^2+(\Delta{y})^2}} &=\\
	 \lim_{(x,y)\to(a,b)} \frac{f(x,y) - f(a,b)}{\sqrt{(\Delta{x})^2+(\Delta{y})^2}} - (m_x\cos{\theta} + m_y\sin{\theta}) &=\\
	 \lim_{(x,y)\to(a,b)} \frac{f(x,y) - f(a,b)}{\sqrt{(\Delta{x})^2+(\Delta{y})^2}} - m_\theta &= 0
\end{align*}


\subsubsection{Finding Differentiable Functions}
\paragraph{Theorem I} If $f_x$ and $f_y$ are continuous at/in a neighborhood around/on some disk containing $(a,b)$ then $f$ is differentiable at $(a,b)$.

This theorem guarantees differentiability, but its conditions are stronger than necessary. This theorem excludes some differentiable functions.

\paragraph{Clairaut's Theorem} If all $n^{th}$ order partials of $f$ are continuous at a point, then the $n^{th}$ order mixed partials are equal. The order of differentiation is immaterial ($f_{xy}=f_{yx}$).

\subsection{Chain Rule}
For a differentiable function $f(x,y)=z$ and $x=g(t)$ and $y=h(t)$:
$$\frac{dz}{dt}=\frac{\partial z}{\partial x}\frac{dx}{dt}+\frac{\partial z}{\partial y}\frac{dy}{dt}$$
Similar results can be shown for $f(x,y,z)=w$ and $x=g(t)$, $y=h(t)$, $z=i(t)$, and for $x=g(t,s)$, $y=h(t,s)$ using $\frac{\partial{x}}{\partial{t}}$, $\frac{\partial{y}}{\partial{t}}$, $\frac{\partial{x}}{\partial{s}}$, and $\frac{\partial{y}}{\partial{s}}$.

\subsubsection{Proof}
If $f$ is differentiable at $(a,b)$ then
\begin{align*}
    \lim_{(x,y)\to(a,b)} \frac{f(x,y)-L(x,y)}{\sqrt{(x-a)^2+(y-b)^2}}&=0\\
    \frac{f(x,y)-L(x,y)}{\sqrt{(x-a)^2+(y-b)^2}}&=\epsilon\\
    f(x,y) = L(x,y) + \epsilon\sqrt{(x-a)^2+(y-b)^2}\\
    f(x,y) = L(x,y) + \epsilon_1\Delta{x} + \epsilon_2\Delta{y} \\
    \Delta{z} = \frac{\partial{z}}{\partial{x}}\Delta{x} + \frac{\partial{z}}{\partial{y}}\Delta{y} + \epsilon_1\Delta{x} + \epsilon_2\Delta{y}\\
    \lim_{\Delta{t}\to0} \frac{\Delta{z}}{\Delta{t}} = \frac{\partial{z}}{\partial{x}}\frac{\Delta{x}}{\Delta{t}} + \frac{\partial{z}}{\partial{y}}\frac{\Delta{y}}{\Delta{t}} + \epsilon_1\frac{\Delta{x}}{\Delta{t}} + \epsilon_2\frac{\Delta{y}}{\Delta{t}}\\
    \frac{dz}{dt}=\frac{\partial z}{\partial x}\frac{dx}{dt}+\frac{\partial z}{\partial y}\frac{dy}{dt}+0+0
\end{align*}
Note that $\epsilon_1, \epsilon_2\to0$ as $(\Delta{x},\Delta{y})\to(0,0)$.

\paragraph{Product/Quotient Rule} The product and quotient rule for $\mathbb{R}\to\mathbb{R}$ functions can be proven by defining a function $F(a,b)=ab$, with $a=f(x)$ and $b=g(x)$, applying the Chain Rule to $F$ produces the same results.

\subsection{Directional Derivatives}
To take the directional derivative in 3-space of a function $f$ at point $(a,b,\cdots)$ along some direction vector $\vect{u}=m\langle u_1, u_2\rangle$ $D_{\vect{u}}$ we take $\hat{u}=\langle u_1, u_2\rangle$ ($D_{\vect{u}} = D_{\hat{u}}$). We can then form parametric equations $x=a+hu_1$, $y=b+hu_2$,for the 2-space line going through $(a,b)$ in the direction of \vect{u} and define
$$D_{\hat{u}}f=\lim_{h\to0}\frac{f(a+hu_1,b+hu_2)-f(a,b)}{h}$$
Using the Chain Rule, $D_{\hat{u}}f = f_xu_1+f_yu_2=\langle f_x, f_y \rangle \cdot \langle u_1, u_2 \rangle$.

\subsubsection{Gradients}
We define the gradient of $f(x,y)$ to be
$$\nabla f(x,y) = \langle f_x(x,y), f_y(x,y) \rangle$$
The "nabla" is the gradient operator, which operates on functions to return a vector-valued function.

From the fact that $$D_{\hat{u}}f=\nabla f \cdot \hat{u}=\abs{\nabla f}\abs{\hat{u}}\cos{\theta}$$ we can warrant that $\nabla f$ points in the direction of \textbf{greatest ascent} (increase in $f$), $-\nabla f$ points in the direction of \textbf{greatest descent}, and that $\abs{\nabla f}$ is the \textbf{largest directional derivative}.

For a level curve $f(x,y)=c$, and assuming the existence of differentiable parameterizations $x(t), y(t)$ that can be used to determine a vector $\langle x'(t), y'(t)\rangle$ tangent to the contour (i.e. $\frac{dz}{dt}=0$), $$\nabla{f}\cdot\langle x'(t), y'(t)\rangle=\frac{\partial f}{\partial x}\frac{dx}{dt}+\frac{\partial f}{\partial y}\frac{dy}{dt}=\frac{dz}{dt}=0$$
indicating that the gradient is perpendicular to the tangent curve, and the level curve as a whole.

This result generalizes to higher dimensions as well, allowing us to more easily find normal vectors to surfaces.

Note that due to the linearity of differentiation, differentiation rules apply to gradients too.

\section{Optimization}
Extrema may exist at critical points $(a,b)$ where $\nabla f(a,b) = \vect{0}$ (i.e. $f_x(a,b)=0$, $f_y(a,b)=0$).

For some surfaces, it is possible to determine the nature of an extremum by analyzing the function (e.g. square values must be positive, suggesting the critical point of the paraboloid $f(x,y)=x^2+y^2$ at $(0,0)$ is a minimum; if you take different slices of a surfaces, and find that a point is a local minimum for some paths, but a local maximum along others, that point is a saddle point).

\subsection{Second Derivatives Test}
If $f(x,y)$ has continuous second-order partials and $\nabla f(a,b)=0$, then consider the determinant/discriminant
\[
D = f_{xx}(a,y)f_{yy}(a,b)-\left(f_{xy}(a,b)\right)^2
\]
\begin{enumerate}
    \item If $D>0$ and $f_{xx}(a,b)$ or $f_{yy}(a,b) > 0$ then $f(a,b)$ is a local minimum.
    \item If $D>0$ and $f_{xx}(a,b)$ or $f_{yy}(a,b) < 0$ then $f(a,b)$ is a local maximum.
    \item If $D<0$, then $f(a,b)$ is a saddle point.
    \item If $D=0$, then the test is inconclusive.
\end{enumerate}
$D$ can also be written as the determinant of the Hessian matrix \(D=\begin{vmatrix}
    f_{xx} & f_{xy}\\
    f_{yx} & f_{yy}
\end{vmatrix}\).

\paragraph{Proof}
The gradient must be $\vect{0}$, as the points in the direction of the gradient would be greater/lesser than the point we are investigating.

Assuming that $f(x,y)$ is infinitely differentiable function, $f$ can be defined as a Taylor Polynomial about point $(a,b)$ in two variables.
\begin{align*}
    f(x,y) = &f(a,b) + f_x(a,b)(x-a) + f_y(a,b)(y-b) +\\
             &\frac{f_{xx}(a,b)}{2}(x-a)^2+\frac{f_{yy}(a,b)}{2}(y-b)^2+f_{xy}(x-b)(y-b)+\cdots
\end{align*}
Moving $f(a,b)$ to the other side, and considering that $\nabla{f}(a,b)=\langle f_x, f_y \rangle = \langle 0,0 \rangle$:
\[
    f(x,y)-f(a,b)=\Delta{f} = \frac{f_{xx}(a,b)}{2}(x-a)^2+\frac{f_{yy}(a,b)}{2}(y-b)^2+f_{xy}(x-b)(y-b)+\cdots
\]
At points sufficiently close to $x$ and $y$, the cubic terms and beyond become negligible. Factoring out a $\frac{\left(y-b\right)^2}{2}$ (the choice of $y$ is arbitrary as $x$ works too; note that this value is always positive):
\[
    \Delta{f} = \frac{\left(y-b\right)^2}{2}\left[f_{xx}(a,b)\left(\frac{x-a}{y-b}\right)^2 + 2f_{xy}(a,b)\left(\frac{x-a}{y-b}\right)+f_{yy}(a,b)\right]
\]

Letting $\omega = \frac{x-a}{y-b}$, we can write the interior expression, which determine the sign of $\Delta{f}$ as a quadratic
\[
f_{xx}(a,b)\omega^2+2f_{xy}(a,b)\omega+f_{yy}(a,b)
\] with $A = f_{xx}(a,b)$, $B=2f_{xy}(a,b)$, $C=f_{yy}(a,b)$. From on the discriminant $B^2-4AC=4D$, we can determine if this quadratic has
\begin{enumerate}
    \item $D > 0$: two real roots, i.e. the expression is sometimes negative and sometimes positive
    \item $D < 0$: two complex roots, i.e. the expression is always positive or negative
    \item $D = 0$: one root
\end{enumerate}
From these results about the sign of the quadratic, we can sometimes determine the sign of $\Delta{f}$, allowing us to discern whether a point is a local maximum/minimum or a saddle point. If $D=0$, we cannot draw a comprehensive conclusion about whether the sign of $\Delta{f}$ changes.

\subsubsection{Optimization Example: Regression (Least-Squares) Line} A \textbf{Linear Regression} aims to produce coefficients for a line $y=Ax+B$ that minimizes the squares [to avoid sign issues] of the $\Delta{y}$s between the points and the line. One can write a function for the sum of the squares of the differences as a function of $A,B$ and determine a minimum point.

\subsection{Optimization on a Curve}
If we are optimizing a surface $f(x,y)$ with $x$ and $y$ being constrained to a curve $g(x,y)=c$ (i.e. a level curve/contour of $g$), critical values occur when $\nabla f(x,y) \parallel \nabla g(x,y)$. This can be expressed as
$$\nabla f(x,y) = \lambda\nabla g(x,y)$$ with $\lambda$ being a scalar known as the Lagrange Multiplier.

This is because the maxima will occur when the directional derivative of $f(x,y)$ is $0$ in the direction of (tangent to) $g(x,y)$, which happens when the gradient of $f$ is perpendicular to the tangent (``velocity") vector. The proof is situated using a chain rule expressing $g(x,y)=c$ as a parametric curve with $x(t)$ and $y(t)$.

Note that with certain constraints, it may be possible to reparameterize a function $f(x,y)$ into a single-variable function that can be easily optimized.

\subsection{Optimization in a Region}
See section \ref{regions} for an introduction to regions.
\subsubsection{Extreme Value Theorem}
If $f(x,y)$ is continuous on a closed and bounded region $D$, then $f$ must achieve both an absolute minimum and absolute maximum on $D$.\\
These extrema may occur at critical points or on the boundary of the region.

To apply the EVT to find absolute extrema:
\begin{enumerate}
    \item Find critical points of $f(x,y)$ within the region by solving for $\nabla{f} = \vect{0}$ (or undefined).
    \item Find extrema of $f$ subject to your bounding constraint, using Lagrange Multipliers or other techniques.
    \item Evaluate $f(x,y)$ at all of the relevant points to determine which are the absolute extremes.
\end{enumerate}

\section{Space Curves}
In general, a space curve is a function $f: \mathbb{R}\to\mathbb{R}^n$. We can consider the output of these functions to be vectors or points. The single parameter variable (conventionally $t$) may have domain restrictions, or the range of outputs may be constrained.

Although such functions can parameterize a curve, such a parameterization is not unique; there exist infinitely many functions that produce the same curve.

\subsection{Limits}
The limit of a vector function $\vect{r}(t)$, if it exists, is the limit of its components.

$$\lim_{t\to{a}}\vect{r}(t)=\lim_{t\to{a}}\langle x(t),y(t) \rangle = \langle \lim_{t\to{a}}x(t),\lim_{t\to{a}}y(t) \rangle$$

\subsection{Continuity}
$\vect{r}(t)$ is continuous at $\vect{r}(a)$ if and only if $$\lim_{t\to{a}}\vect{r}(t)=\vect{r}(a)$$

\subsection{Derivatives}
The definition of a vector function $\vect{r}(t)$'s derivative is analogous to that of a function in 2-space.
$$\vect{r}'(t)=\frac{d\vect{r}}{t}=\lim_{h\to0}\frac{\vect{r}(t+h)-\vect{r}(t)}{h}=\langle x'(t), y'(t) \rangle$$
The limit of the secant vector is the tangent vector; this vector is tangent to the curve at $t$ so long as it exists and $\ne\vect{0}$. In this case, the tangent line to the curve $\vect{r}(t)$ at $t=a$ is
$$\langle x,y,z \rangle = \vect{r}(a)+t\cdot\vect{r}'(a)$$ Functions that have continuous derivatives with $\vect{r}'(t)\ne0$ are \textbf{smoothly parameterized}.

\subsubsection{Properties of the Derivative}
Letting $\vect{r}(t), \vect{u}(t), \vect{v}(t)$ be differentiable vector-valued functions, and $f(t)$ be a differentiable scalar function:
\begin{enumerate}
    \item $\frac{d}{dt}\left(c\cdot\vect{r}(t)\right)=c\cdot\vect{r}'(t)$
    \item $\frac{d}{dt}\left(f(t)\cdot\vect{r}(t)\right)=f(t)\cdot\vect{r}'(t)+\vect{r}(t)\cdot f'(t)$
    \item $\frac{d}{dt}\left(\vect{u}(t)\cdot\vect{v}(t)\right)=\vect{u}(t)\cdot\vect{v}'(t)+\vect{u}'(t)\cdot \vect{v}(t)$
    \item $\frac{d}{dt}\left(\vect{u}(t)\times\vect{v}(t)\right)=\vect{u}(t)\times\vect{v}'(t)+\vect{u}'(t)\times \vect{v}(t)$ (Note that order matters due to the cross product being anti-commutative, and all functions must be $\mathbb{R}\to\mathbb{R}^3$.)
\end{enumerate}

\subsubsection{Derivative of the Magnitude}
Considering the derivative of the squared magnitude $\magnitude{\vect{r}(t)}^2$ for a non-zero vector function, we find that
\begin{align*}
    \frac{d}{dt}\left(\magnitude{\vect{r}}^2\right) = \frac{d}{dt}\left(\vect{r}\cdot\vect{r}\right) &= 2\vect{r}\cdot\vect{r}' = 2\magvect{r}\frac{d}{dt}\left(\magvect{r}\right)\\
    \frac{d}{dt}\left(\magvect{r}\right) &= \frac{\vect{r}\cdot\vect{r}'}{\magvect{r}}
\end{align*}
As a corollary, if $\magvect{r}=c$, the velocity vector is perpendicular to the position vector.

(note that $\frac{d}{dt}\left(\magvect{\vect{r}}\right) = \vect{r}'(t)\cos{\theta}$).

\subsection{Planes to a Curve}
\subsubsection{The Normal Plane}
The normal plane at a point on the curve $\vect{r}(t)$ is
$$\vect{r}' \cdot\langle x - r_x, y - r_y, z - r_z\rangle = 0$$

\subsubsection{The Unit Tangent}
The unit tangent at a point on the curve $\vect{r}(t)$ is $$\vect{T}=\frac{\vect{r}'}{\magnitude{\vect{r}'}}$$ which is the normalization of $\vect{r}'$.
Thus, both $\vect{r}'$ and $\vect{T}$ are parallel and point in the direction of the tangent line.

\paragraph{Derivative of the Unit Tangent}
To evaluate this derivative, we use the Product/Quotient Rule (keeping in mind that $\magvect{r'}$ is a non-constant scalar, so we must use the Derivative of the Magnitude).
$$\vect{T'}=\frac{1}{\magnitude{\vect{r}'}}\vect{r}''-\left(\frac{\vect{r}'\cdot\vect{r}''}{\magnitude{\vect{r}'}^3}\right)\vect{r}'$$
Note that because the unit tangent has constant magnitude, we know the unit tangent's derivative must be orthogonal to the unit tangent.

\subsubsection{The Normal Vector}
The normal vector at a point on the curve $\vect{r}(t)$ is $$\vect{N}=\frac{\vect{T}'}{\magnitude{\vect{T}'}}$$ which is the normalization of $\vect{T}$.
Thus, both $\vect{N}$ and $\vect{T'}$ are parallel and orthogonal to the tangent line.

In addition, because $\vect{N}$ is a linear combination of $\vect{r}'$ and $\vect{r}''$, we know that it is coplanar with $\vect{r}'$ and $\vect{r}''$.

\subsubsection{The Osculating Plane}
The osculating ("kissing") plane contains the vectors $\vect{r}'$, $\vect{r}''$, $\vect{T}$, $\vect{T}'$, and $\vect{N}$. Taking the cross product between two of these vectors (that are non-parallel!), we obtain a normal to the osculating plane, which also lies on the tangent plane. This plane is analogous to a 2nd-degree Taylor approximation for a curve, in the sense that it attempts to match $\vect{r}'$ and $\vect{r}''$.

\paragraph{Binormal} The Binormal vector is defined to specifically be
$$\vect{B}=\vect{T}\times\vect{N}$$
This is a unit vector (as it is the cross product of two orthogonal unit vectors), and its direction serves as the third dimension in a 3-space orthonormal basis (set of 3 orthogonal unit vectors which we can use for defining a coordinate system).

\subsubsection{The Rectifying Plane}
The rectifying plane contains $\vect{T}$ and $\vect{B}$.

\subsubsection{Skipped Topics}
Curvature and Arclength have been omitted at this time to transition to the Spring semester.

\section{Multiple Integrals}
\subsection{Review of Single Integrals}
A Riemann integral is the limit of a Riemann Sum with subintervals $x_n$ and random sample points from those intervals $x_n^*$
$$\int_a^bf(x)\,dx=\lim_{\substack{n\to\infty\\max\Delta{x_i}\to0}}\sum^n_{i=1}f(x_i^*)\Delta{x_i}$$
The value of the integral represents the (signed\footnote{If $f(x)>0$, then signed area and absoliute area are equivalent.}) area under the curve.

\subsection{Defining Double Integrals}
Some solids $S$ can be defined in terms of a surface $f(x,y)$ and a base region $R$ on the $z=0$ plane.
$$S=\left\{(x,y,z)|(x,y)\in R, 0\le z\le f(x,y)\right\}$$

For such solids, we can define subrectangles that are based on the Cartesian product of subintervals $x_n$ and $y_m$ along the $x$ and axes respectively. Taking Riemann sums of these infinitesmal areas, we define double integrals to be
\begin{align*}
    \mathop{\int\int}_Rf(x,y)\,dA
        &=\lim_{\substack{m,n\to\infty\\max\Delta{x_i},\Delta{y_j}\to0}}\sum_{j=1}^m\sum_{i=1}^nf(x_{ij}^*,y_{ij}^*)\cdot\Delta A\\
        &=\lim_{\substack{m,n\to\infty\\max\Delta{x_i},\Delta{y_j}\to0}}\sum_{j=1}^m\sum_{i=1}^nf(x_{ij}^*,y_{ij}^*)\cdot\Delta x\Delta y
\end{align*}

If $R=[a,b]\times[c,d]$, $$\mathop{\int\int}_Rf(x,y)\,dA=\mathop{\int\int}_{[a,b]\times[c,d]}f(x,y)\,dA$$

If we are finding the signed volume between two surfaces $f(x,y)$ and $g(x,y)$, we simply integrate with heights of $f(x,y)-g(x,y)$, obtaining the double integral $$\mathop{\int\int_R}f(x,y)-g(x,y)\,dA$$ Note that $g(x,y)=0$ is the simple special case.

\subsection{Estimating Double Integrals}
Just as one can estimate a single integral based on a finite number of rectangular subintervals in $\mathbb{R}^2$, it is possible to estimate a double integral based on a finite number of rectangular prism subintervals in $\mathbb{R^3}$ by doing two Riemann sums.

On a standard test question, you may be given a surface $f$ on rectangular region $[a,b]\times[c,d]$ split into $n \times m$ subintervals. The sum evaluates $f$ at a defined sample point from each subrectangle, multiplying it by the area of the subrectangle.

\subsection{Evaluating Double Integrals}
By integrating along a single axis (using the Fundamental Theorem of Calculus), we can define area functions $A(y)=\int_a^bf(x,y)\,dx$ or $A(x)=\int_c^df(x,y)\,dy$ for a slice of the surface along a fixed $y$ or $x$ respectively. If we iteratively integrate again along the other axis, these iterated integrals may evaluate to a numerical value for the volume integral.

\subsubsection{Fubini's Theorem}
If $f(x,y)$ is continuous on a rectangle $[a,b]\times[c,d]$, then
$$\mathop{\int\int}_Rf(x,y)\,dA=\int_a^b\int_c^df(x,y)\,dy\,dx=\int_c^d\int_a^bf(x,y)\,dx\,dy$$

The order in which the infinitesimal elements are written matters, with the innermost integral pairing with the innermost infinitesimal.

\subsubsection{The Very Nice Theorem}
From the linearity properties of double integrals, if $f(x,y)=g(x) \cdot h(y)$ and $R=[a,b]\times[c,d]$ then
\begin{align*}
    \mathop{\int\int}_Rf(x,y)\,dA &=\int_a^b\int_c^dg(x)\cdot h(y)\,dy\,dx\\
        &=\int_a^bg(x)\left(\int_c^d h(y)\,dy\right)\,dx\\
    &=\int_a^bg(x)\,dx\int_c^dh(y)\,dy
\end{align*}

\subsection{Properties of Double Integrals}
\begin{enumerate}
    \item If $f(x,y)>0 \forall(x,y)\in{R}$, then $\mathop{\int\int}_Rf(x,y)\,dA$ is the volume bounded by $f$ and the $xy$ plane.
    \item $dA=dx\,dy$ or $dA=dy\,dx$
    \item If $f(x,y)$ is continuous on $R$, then $\mathop{\int\int}_R f(x,y)\,dA$ exists.\footnote{Point and slit discontinuities on a function still allow integrability, assuming no singularities diverge to infinity, or the discontinuities are on broader region.}
\end{enumerate}
The following properties indicate the linearity of integration:
\begin{enumerate}
    \item $\mathop{\int\int}_Rf(x,y)\pm g(x,y)\,dA=\mathop{\int\int}_R f(x,y)\,dA\pm\mathop{\int\int}_R g(x,y)\,dA$
    \item $\mathop{\int\int}_Rcf(x,y)\,dA=c\mathop{\int\int}_R f(x,y)\,dA$
    \item If $f(x,y)\ge g(x,y)$ on $R$ then $\mathop{\int\int}_R f(x,y)\,dA\ge\mathop{\int\int}_R g(x,y)\,dA$
\end{enumerate}

\subsection{Double Integrals over Non-Rectangular Regions}
To evaluate a double integral on a Type I region $R_1$ between $[a,b]$ on the $x$-axis, and functions $g_1(x)$ and $g_2(x)$ on the $y$-axis (see section \ref{regions}), we can set the bounds for $y$ to be $g_1(x)$ and $g_2(x)$, then integrate as normal. This is analogous to integrating rectangles of height $g_2(x)-g_1(x)$ and width $dx$.

$$\mathop{\int\int}_{R_I} f(x,y)\,dA=\int^b_a\int^{g_2(x)}_{g_1(x)}f(x,y)\,dy\,dx$$

We can perform analogous operations on a Type II region, integrating rectangles of width $h_2(x)-h_1(x)$ and height $dy$.
$$\mathop{\int\int}_{R_{II}} f(x,y)\,dA=\int^d_c\int^{h_2(y)}_{h_1(y)}f(x,y)\,dx\,dy$$

For regions that are both Type I and II, we can choose to use either representation of the region to integrate the function. It may be harder (or even impossible!) to integrate along a certain axis, and in such cases, swapping the order of integration by rewriting the integration region along the other axis may be necessary.

The bounds may be provided, or they may need to be derived from the intercepts between relevant surfaces/curves.

\subsection{Average Value}
For surfaces, the average value of the surface over a region is
$$f_{avg}=\frac{\mathop{\int\int_Rf\,dA}}{\mathop{\int\int_R\,dA}}$$
with $\mathop{\int\int_R\,dA}=\mathop{\int\int_R1\,dA}$ being the area of the region.
A ``cylinder'' with a base of $R$ and height of $f_avg$ would contain the same (signed) volume as the surface over that region.

This statement generalizes to all dimensions, i.e. for the average value of a function $g: \mathbb{R^3}\to\mathbb{R}$ over a region of space $E$ is
$$g_{avg}=\frac{\mathop{\int\int\int_E g(x,y,z)}\,dV}{\mathop{\int\int\int_E\,dV}}$$

\subsection{Triple Integrals}
A triple integral over a region of space $E$ is defined as
\begin{align*}
    \mathop{\int\int\int}_E f(x,y,z)\,dV
    &=\lim_{\substack{l,m,n\to\infty\\max\Delta{x_i},\Delta{y_j},\Delta{z_k}\to0}}\sum^l_{k=1}\sum^m_{j=1}\sum^n_{i=1}f(x_i^*,y_j^*,z_k^*) \Delta{V}\\
    &=\lim_{\substack{l,m,n\to\infty\\max\Delta{x_i},\Delta{y_j},\Delta{z_k}\to0}}\sum^l_{k=1}\sum^m_{j=1}\sum^n_{i=1}f(x_i^*,y_j^*,z_k^*) \Delta{x}\Delta{y}\Delta{z}
\end{align*}

\subsubsection{Evaluating Triple Integrals}
Fubini's Theorem holds for evaluating along regions that are rectangular prisms, so bounds can be written in analogous manners to double and single integrals. The Very Nice Theorem remains true in general.

For integrals over general Type I regions between $z=g_1(x,y)$ and $z=g_2(x,y)$ and with an $xy$-plane projection $D$, we can rewrite the integral as
$$\mathop{\int\int}_D\int^{g_1(x,y)}_{g_2(x,y)} f(x,y,z)\,dz\,dA$$
and analogous expressions are used for Type II and III regions,\footnote{See definitions for regions of types I-III in section \ref{regions}.} which can be used to subdivide any non-simple region into suitable simple regions for use with iterated integrals.

\subsection{Polar Integrals in 2-Space}
\subsubsection{Polar Coordinates}
We can use $x=r\cos{\theta}$ and $y=r\sin{\theta}$ to convert from rectangular to polar, or $r^2=x^2+y^2$ and $\theta=\arctan{\frac{y}{x}}$ to convert from polar to rectangular. Note that we must be mindful if the values of $r$ and $\theta$ we choose lead to overlapping points (multiple polar points can be converted to one Cartesian point).

\subsubsection{Polar Area}
The basic unit of polar area is the sector, with area $A=\pi{r^2}\frac{\Delta\theta}{2\pi}=\frac{1}{2}r^2\Delta\theta$. A polar rectangle is the difference between two sectors with radii $r$ and $r+\Delta{r}$.

\paragraph{Area of Infinitesimal Polar Rectangles}
We begin with the area of polar rectangles defined in terms of $\Delta{\theta}$ and radii $R,r$, with $R=r+\Delta{r}$, then rearrange.
\begin{align*}
    A&=\frac{1}{2}\Delta{\theta}\left(R^2-r^2\right)\\
     &=\frac{1}{2}\Delta{\theta}\left(R-r\right)\left(R+r\right)\\
     &=\frac{R+r}{2}\Delta{\theta}\Delta{r}\\
\end{align*}
Letting $r_{median}=\frac{R+r}{2}$, because $R\to{r}$ as $\Delta{r}\to0$, $r_{median}\to{r}$, the differential area is
$$dA=r\,dr\,d\theta$$

\subsubsection{Definition}
We take the limit of a Riemann Sum consisting of infinitesimal polar rectangles. For a function $f(r,\theta)$ on a polar rectangle $a \le r \le b$, $\alpha \le \theta \le \beta$:
\begin{align*}
    \mathop{\int\int}_Rf(r,\theta)\,dA &=\lim_{\substack{m,n\to\infty\\max\Delta{r},\Delta{\theta}\to0}}\sum_{j=1}^m\sum_{i=1}^nf(r_{ij}^*,\theta_{ij}^*)\cdot\Delta A\\
    &=\lim_{\substack{m,n\to\infty\\max\Delta{r},\Delta{\theta}\to0}}\sum_{j=1}^m\sum_{i=1}^nf(r_{ij}^*,\theta_{ij}^*)\cdot r\Delta r\Delta\theta\\
    &=\int_\alpha^\beta\int_a^bf(r,\theta)\,r\,dr\,d\theta
\end{align*}

\subsection{Integrals in Cylindrical Coordinates}
\subsubsection{Cylindrical Coordinates}
Cylindrical coordinates are an extension of polar coordinates to $\mathbb{R}^3$ by adding a linear $z$ coordinate. Points are of the form $(r,\theta,z)$, and by convention $0\le{r}$.

To convert cylindrical to Cartesian coordinates, we use the expression
$$(x,y,z)=(r\cos\theta,r\sin\theta,z)$$
To convert Cartesian to cylindrical coordinates, we use the relations
\begin{align*}
    r^2&=x^2+y^2\\
    \tan\theta&=\frac{y}{x}\\
    z&=z
\end{align*}

\subsubsection{Cylindrical Volumes}
The basic unit of cylindrical area is an extruded polar rectangle.
Considering such a "cylinder"\footnote{Using the textbook definition of cylinder as any figure that has consistent cross sections aligned parallel to an axis.} with height $\Delta z$ and a base that is a polar rectangle $\frac{1}{2}\Delta\theta(\Delta R^2-r^2)$, with $R-r = \Delta{r}$, we can obtain an analogous expression to polar area, used for cylindrical integrals.
$$dV=r\,dr\,d\theta\,dz$$

\subsection{Integrals in Spherical Coordinates}
\subsubsection{Spherical Coordinates}
Spherical coordinates are an extension of polar coordinates to $\mathbb{R}^3$ by replacing the $R^2$ circle $x^2+y^2=r^2$ with an $\mathbb{R}^3$ sphere $x^2+y^2+z^2=\rho^2$. Two angles, $\theta$ on the $xy$-plane measured from the positive $x$-axis and $\phi$ measured from the positive $z$-axis, define a point on the sphere. Points are of the form $(\rho,\theta,\phi)$ in this course, but other fields may use alternative conventions. By convention, we use $0\le\rho$, $0\le\phi\le\pi$.

To convert spherical to Cartesian coordinates, we use $\rho\sin\phi$ for the radius of the circle on the $xy$-plane below the point and $\rho\cos\phi$ for the height of the point, and derive the expression
$$(x,y,z)=(\rho\sin\phi\cos\theta,\rho\sin\phi\sin\theta,\rho\cos\phi)$$
To convert Cartesian to spherical coordinates, we use the relations
\begin{align*}
    \rho^2&=x^2+y^2+z^2\\
    \tan\theta&=\frac{y}{x}\\
    \cos\phi&=\frac{z}{\rho}
\end{align*}

\subsubsection{Spherical Volumes}
The basic unit of spherical area is a spherical wedge, a solid bounded between two spherical coordinates. The ''length`` of the (interior) arc is $\rho\sin\phi\Delta\theta$, with the ''width`` of the solid being bound by radial lines separated by $\Delta\rho$, and the ''height`` being from an arc with length $\rho\Delta\phi$.

This produces a volume with approximate dimensions $\Delta{V}=\rho^2\sin\phi\cdot\Delta\theta\Delta\phi\Delta\rho$, which in the limit tends towards
$$dV=\rho^2\sin\phi\,d\rho\,d\theta\,d\phi$$

\subsection{The Jacobian}

\subsubsection{Transformations and Projections}
A transformation function $f: \mathbb{R}^n\to\mathbb{R}^n$ can be considered a change of coordinate systems within $n$-space. The pre-image refers to the points in the domain, and the image refers to the points in the range. It is helpful to conceptualize such functions as operating on vectors corresponding to points.\\
A projection function $f: \mathbb{R}^n\to\mathbb{R}^{n-1}$ can be considered flattening of a points from a higher dimension onto a lower one.

\subsubsection{Homeomorphisms}
A homeomorphism is a function which is continuous and has a continuous inverse.

\subsubsection{Definition of the Jacobian}
For a transformation $\vect{T}: \langle u,v \rangle\to\langle x,y \rangle$, with functions $x(u,v)$, $y(u,v)$ being differentiable homeomorphisms, an infinitesimal square area $dA\cdot\hat{k}$ in $uv$ maps to an area approximated by a parallelogram bounded by the vectors $\langle x_u, y_u\rangle\Delta u$, $\langle x_v, y_v\rangle\Delta v$ (for a rectangle in $u,v$ beginning at $(h,k)$, the vectors represent the displacements to the image of adjacent corners $(h+\Delta h,k)$, $(h,k+\Delta k)$).

The area of this parallelogram can be found using the cross-product (appending a $0 \hat{k}$ component to these vectors):
\begin{align*}
\Delta u \langle x_u, y_u,0\rangle\times \Delta v \langle x_v, y_v, 0\rangle &=
\begin{vmatrix}
\hat{t} & \hat{j} & \hat{k}\\
x_u & y_u & 0 \\
x_v & y_v & 0
\end{vmatrix}\Delta u\Delta v \\ &=
\begin{vmatrix}
    x_u & y_u \\
    x_v & y_v
\end{vmatrix}\Delta u \Delta v\,\hat{k}
\end{align*}
From this, we define the Jacobian to be the scale factor
$$\frac{\partial(x,y)}{\partial(u,v)}=\left|\begin{vmatrix}
    x_u & x_v\\
    y_u & y_v
\end{vmatrix}\right|$$
the absolute value of the determinant of the (transposed\footnote{Note that the determinant of a transposed matrix is the same as the original.}) cross-product matrix.

\subsubsection{Applying the Jacobian}
To apply the Jacobian, we will often need to find the inverse transformation $T^{-1}: (x,y)\to(u,v)$, which we obtain by solving the system $x=x(u,v)$, $y=y(u,v)$ for $u$ and $v$. Note that when both are well-defined, the Jacobian of the inverse transformation is the reciprocal of the Jacobian of the transformation: $$\frac{\partial(u,v)}{\partial(x,y)}=\frac{1}{\frac{\partial(x,y)}{\partial(u,v)}}$$

An integral in $xy$ over $R$ can be rewritten as an integral of $uv$ over $S$ (where $T$ maps $S$ to $R$ using functions $x(u,v)$, $y(u,v)$), scaled by the Jacobian.
$$\mathop{\int\int}_Rf(x,y)\,dx\,dy=\mathop{\int\int}_Sf(x(u,v), y(u,v))\frac{\partial(x,y)}{\partial(u,v)}\,du\,dv$$

\section{Line Integrals}
\subsection{Line Integrals of Scalar Functions}
We define the integral of a scalar function $f(x,y)$ along a path $C$ in the $xy$-plane (the "line") as
$$\int_C f(x,y)\,ds = \lim_{\substack{n\to\infty\\\max \Delta{s}\to0}}f(x_i^*,y_i^*)\,\Delta{s}_i$$
where $ds$ is an infinitesimal arclength element, and $\Delta{s}$ is a small arc length element.\footnote{In some notations, $\oint$ is used for all line integrals, but we choose to reserve this symbol for strictly loop (closed line) integrals.} In essence, we are integrating for the "area" of the "curtain" bounded by $z=0$ and $z=f(x,y)$ along the curve $C$.

For a smoothly parameterized curve $C=\langle x(t),y(t)\rangle$ from $a\le t \le b$,
\[
    \Delta{s}=\sqrt{(\Delta{x})^2+(\Delta y)^2} = \sqrt{\left(\frac{\Delta{x}}{\Delta t}\right)^2+\left(\frac{\Delta{y}}{\Delta t}\right)^2}\,\Delta t
\]
and in the limit,\footnote{Note that for $x=t$, $x'(t) = 1$, lending itself to the familiar arclength integral formula for a $f:\mathbb{R}\to\mathbb{R}$.} \[ds = \sqrt{\left(\frac{dx}{dt}\right)^2+\left(\frac{dy}{dt}\right)^2}\,dt = \sqrt{x'(t)^2 + y'(t)^2}\,dt\]
$$\int_Cf(x,y)\,ds=\int_a^bf(x(t),y(t))\sqrt{x'(t)^2 + y'(t)^2}\,dt$$
Analogous line integrals in space exist for $f: \mathbb{R}^3\to\mathbb{R}$ functions.

\subsubsection{Line Integrals with Respect to \texorpdfstring{$dx$ and $dy$}{dx and dy}}
Considering a smoothly parameterized curve $C=\langle x(t),y(t)\rangle$ from $a\le t \le b$, the signed area of the curtain's projection onto the $x$ or $y$ axis can be written as
\begin{align*}
    \int_Cf(x,y)\,dx&=\int_a^bf(x,y)\,x'(t)\,dt\\
    \int_Cf(x,y)\,dy&=\int_a^bf(x,y)\,y'(t)\,dt
\end{align*}
Note that $x'(t)\,dt=\frac{dx}{dt}dt$, and the same goes for $y$. Further, the orientation of $C$ matters, as we account whether the curve is moving in the positive or negative $x$/$y$ direction using the derivative's sign.

When these two integrals show up together, we use the following notation:
$$\int_Cf(x,y)\,dx+\int_Cg(x,y)\,dy=\int_cf(x,y)\,dx+g(x,y)\,dy$$
This represents a dot product of $\langle f(x,y),\,g(x,y)\rangle\cdot\langle dx,\,dy\rangle=\langle f(x,y),\,g(x,y)\rangle\cdot\langle \frac{dx}{dt},\,\frac{dy}{dt}\rangle\,dt$.
\subsubsection{Properties of Line Integrals}
\begin{itemize}
    \item The value of a line integral in $ds$ is independent of the parameterization used. A corollary is that a line integral along a curve $C$, or the same curve in the opposite direction $-C$, produce the same value. $\int_C f(x,y)\,ds=\int_{-C}f(x,y)\,ds$
    \begin{itemize}
        \item For line integrals on $dx$ and $dy$, this is not true, $\int_Cf(x,y)\,dx=-\int_{-C}f(x,y)\,dx$ and the same applies for $y$.
    \end{itemize}
    \item Like other integrals, a line integral can have its domain of integration be split into multiple (hopefully easier to integrate) sub-curves and added together.
\end{itemize}

\subsection{Vector Fields}
A vector field is a collection of vectors corresponding to each point on a plane. We graphically represent a sample of the vector field (such as on the lattice points, which have integer coordinates) with vectors on a plane (potentially scaled to fit).
\subsubsection{Work Done by a Vector Field}
In the model of physics, any movement of a particle along a curve that has a parallel component with elements of a vector field has work done on it, representing how the gradient "helps" (in the positive direction) or "hinders" (in the negative direction) the particle's motion. (Note that a particle moving on a curve tangent to the vectors in a vector field has 0 work done.) We take the integral of the dot product of field's "force" with the infinitesimal path vector element ($d\vect{s}$, or using the unit tangent $\vect{T}\,ds$).

$$\int_C\vect{F}\cdot d\vect{s}=\int_C\vect{F}\cdot d\vect{r}=\int_C\vect{F}\cdot\vect{T}\,\,ds$$

Considering that $\vect{T}=\frac{\vect{r}'(t)}{\left|\vect{r}'(t)\right|}$ and $ds = \left|\vect{r}'(t)\right|\,dt$, the integrals become
$$\int_C\vect{F}\cdot\vect{r}'(t)\,dt$$
So long as orientation (e.g. clockwise, counter-clockwise) is preserved, the value of $\int_C\vect{F}\cdot\vect{r}'(t)\,dt$ will be independent of the choice of parameterization of $C$.

For a vector field $\vect{F}(x,y)\langle f(x,y), g(x,y) \rangle$, and curve $C: \vect{r}(t)=\langle x(t),y(t) \rangle$ from $t_1$ to $t_2$, expanding the dot product produces
\begin{align*}
    \int_C\vect{F} \cdot \vect{r}&=\int_{t_1}^{t_2} f(x(t),y(t))\,x'(t)\,dt+\int_{t_1}^{t_2}g(x(t),y(t))\,y'(t)\,dt\\
    &=\int_C f(x,y)\,dx+g(x,y)\,dy
\end{align*}

\subsubsection{Conservative Vector Fields}
If $\vect{F}=\vect{\nabla}f$, then $\vect{F}$ is called a conservative vector field, and $f$ is called the potential of $\vect{F}$.

\paragraph{FTC for Line Integrals (Path Independence)}
If $f(x,y)$ is a differentiable function, and $\vect{r}(t)$ is a simple curve in the plane, then
\begin{align*}
    \int_C\vect{F}\cdot d\vect{r} &= \int_a^b\left\langle\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right\rangle\cdot\langle x'(t),y'(t) \rangle\,dt\\
    &= \int_a^b\left(\frac{\partial f}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial f}{\partial y}\frac{\partial y}{\partial t}\right)\,dt\\
    &=\int_a^b \frac{df}{dt}\,dt\\
    &= f(x(b),y(b)) - f(x(a),y(a))\\
    &= f(\vect{r}(b)) - f(\vect{r}(a))
\end{align*}
Thus,
\begin{itemize}
    \item line integrals of conservative vector fields are path independent; for curves $C_1,C_2$ with identical start and end points: $\int_{C_1}\vect{\nabla}f\cdot d\vect{r} = \int_{C_2}\vect{\nabla}f\cdot d\vect{r}$
    \item line integrals of conservative vector fields over simple closed loops are zero: $\oint \vect{\nabla}f\cdot d\vect{r} = 0$
\end{itemize}

\paragraph{The Path Independence Test for Conservative Vector Fields}
If $\vect{F}(x,y)=\langle P(x,y), Q(x,y) \rangle$ is a vector field defined on an open, connected set $D$ and if $\vect{F}$ is path independent on D, then $\vect{F}$ is conservative on $D$. This can be proven by defining a function $f(x,y)$ with a fixed origin point $(a,b)$ that also has the following properties (based on path independence):
$$f(x,y) = \int_{(a,b)}^{(x,y)}\vect{F}\cdot d\vect{r} = \int_{(a,b)}^{(c,y)}\vect{F}\cdot d\vect{r} + \int_{(c,y)}^{(x,y)}\vect{F}\cdot d\vect{r}$$
We can then warrant that
\begin{align*}
    \frac{\partial f}{\partial x} &= \frac{\partial}{\partial x}\left( \int_{(a,b)}^{(c,y)}\vect{F}\cdot d\vect{r} + \int_{(c,y)}^{(x,y)}\vect{F}\cdot d\vect{r} \right)\\
    &= \frac{\partial}{\partial x} \left(\int_{(c,y)}^{(x,y)}\vect{F}\cdot d\vect{r} \right) = \frac{\partial}{\partial x} \left(\int_{(c,y)}^{(x,y)}P(x,y)\,dx + Q(x,y)\,dy \right)\\
    &= \frac{\partial}{\partial x}\int_{(c,y)}^{(x,y)}P(x,y)\,dx\\&= P(x,y)
\end{align*}
because $a,b,c,y$ are all fixed with respect to $x$. By similar reasoning, we can show that $\frac{\partial f}{\partial y} = Q(x,y)$, and so $\vect{F} = \vect{\nabla} f$.

\paragraph{The Component Test for Conservative Vector Fields}
If $\vect{F} = \langle P(x,y),Q(x,y)\rangle$ is a vector field on a simply connected closed region $D$ and if $P(x,y), Q(x,y)$ have continuous partials (the conditions of Green's Theorem), and if $\frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x}$ then $\vect{F}$ is conservative. This is based on the converse of Clairaut's Theorem, or derived from Green's Theorem showing path independence.

\subsubsection{Green's Theorem}
Let $C$ be a piecewise smooth (i.e. has finite corners), positively oriented (i.e. counterclockwise), simple, closed curve  and let $D$ be the region enclosed by $C$. If $P(x,y),Q(x,y)$ have continuous partials on an open region containing $D$ (and the boundary, so that we avoid one-sided derivatives), then
$$\int_C P(x,y)\,dx+Q(x,y)\,dy=\iint_D\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\,dA$$
The proof is trivial for conservative vector fields. For a type I and II region, bounded by $y=g_1(x)$ and $y=g_2(x)$ ($x\in[a,b]$) as a type I region and $x=h_1(y)$ and $x=h_2(y)$ ($y\in[c,d]$) as a type II region:\footnote{For a purely type I or II region, one of these function pairs reduce to constants.}
\begin{align*}
    \iint -\frac{\partial P}{\partial y}\,dA &= -\int_a^b\int_{g_1(x)}^{g_2(x)}\frac{\partial P}{\partial y}\,dy\,dx\\
    &=-\int_a^bP(x,g_2(x))-P(x,g_1(x))\,dx
\end{align*}
we can parameterize the boundary of the type I region as $C_1:\langle t, g_1(t)\rangle$, $C_2:\langle t, g_2(t)\rangle$, any additional sides of the region are either nonexistent (in the case of a type I and II region) or irrelevant ($dx=0$ for $x=c$ bounding a type I region).
\begin{align*}
    \oint_CP\,dx &= \int_{C_1}P\,dx + \int_{C_2}P\,dx\\
    &=\int_a^bP(t,g_1(t))\,dt+\int_b^aP(t,g_2(t))\,dt\\
    &=-\int_a^bP(x,g_2(x))-P(x,g_1(x))\,dx
\end{align*}
By similar reasoning
\begin{align*}
    \iint_D\frac{\partial Q}{\partial x}\,dA &= \int_c^d\int_{h_1(y)}^{h_2(y)}\frac{\partial Q}{\partial x}\,dx\,dy\\
    &= \int_c^dQ(h_2(y),y)-Q(h_1(y),y)\,dy\\
    &= \int_d^cQ(h_1(t),t)\,dt+\int_c^dQ(h_2(t),t)\,dt\\
    &=\oint_CQ\,dy
\end{align*}

\paragraph{A note on notation}
For a simple closed curve $C$ that is oriented counterclockwise, and is the boundary of a region $D$, the following notations are equivalent:
$$\int_C=\oint_C=\ointctrclockwise_C=\int_{\partial D}$$

\subsection{Del: Curl and Divergence}
The del operator $\nabla$ (nabla) in $\mathbb{R}^3$  is
$$\vect{\nabla}=\left\langle \frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}\right\rangle$$

\subsubsection{Curl}
The curl of a vector field $\vect{F} = \langle P,Q,R \rangle$ is
$$\curl \vect{F} = \vect{\nabla}\times\vect{F}=\left\langle\frac{\partial R}{\partial y}-\frac{\partial Q}{\partial z},\frac{\partial P}{\partial z}-\frac{\partial R}{\partial x}, \frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right\rangle$$
Extending a $2-$vector field to $3-$space by defining $\vect{F}=\langle P, Q, 0 \rangle$, we find that the curl for a $2-$vector field is always
$$\curl \vect{F} = \left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right)\hat{k}$$
and that a conservative vector field is irrotational ($\curl \vect{F} = 0$).

The curl can be conceptualized as a measure of vorticity/rotational tendency/circulation density, with each component in $\hat{i}, \hat{j}, \hat{k}$ representing the tendency of the vector field to cause rotation of a "paddle" on the $yz$, $xz$, or $xy$ planes respectively.

We can define the circulation density of $\vect{F}$ at $(a,b)$ to be the limit of the work done on an infinitesimal loop around the point $(a,b)$ over its area. We choose the square $C$ of side length $2h$ centered at $(a,b)$ in the $xy$ plane, but alternative choices, like circles, should produce similar results.
$$\frac{\int_C \vect{F}\cdot d\vect{r}}{(2h)^2}$$
Using the parameterizations $\langle a-t, b+h \rangle$, $\langle a+h,b+t \rangle$, $\langle a+t, b+h \rangle$, and $\langle a-h,b-t\rangle$ for the four sides of the square, we can expand the former integral into the form
\begin{align*}
    \frac{\int_C \vect{F}\cdot d\vect{r}}{(2h)^2}=&\frac{1}{(2h)^2}\left[\int_{-h}^h-P(a-t,b+h)\,dt + \int_{-h}^hP(a+t,b-h)\,dt\right] +\\
    &\frac{1}{(2h)^2}\left[\int_{-h}^hQ(a+h,b+t)\,dt-\int_{-h}^hQ(a+h,b-t)\,dt\right]
\end{align*}
From the Mean Value Theorem for integrals, there must exist $t_1,t_2\in(a-h,a+h)$ and $t_3,t_4\in(b-h,b+h)$ such that
\begin{align*}
    \frac{\int_{-h}^h-P(a-t,b+h)\,dt}{2h}&=-P(t_1,b+h)\\
    \frac{\int_{-h}^h P(a+t,b-h)\,dt}{2h}&= P(t_2,b-h)\\
    \frac{\int_{-h}^h Q(a-h,b-t)\,dt}{2h}&=Q(a-h,t_3)\\
    \frac{\int_{-h}^h Q(a+h,b+t)\,dt}{2h}&=Q(a+h,t_4)
\end{align*}
$$\frac{\int_C \vect{F}\cdot d\vect{r}}{(2h)^2}=\frac{1}{2h}\left(Q(a+h,t_4)-Q(a-h,t_3)\right)-\frac{1}{2h}\left(P(t_1,b+h) - P(t_2,b-h)\right)$$
and taking the limit as $h\to0$ (which implies $t_1,t_2 \to a$, $t_3,t_4\to b$), we obtain
$$\lim_{h\to0}\frac{\int_C \vect{F}\cdot d\vect{r}}{(2h)^2} = \frac{\partial Q}{\partial x}\Bigr|_{(a,b)} - \frac{\partial P}{\partial y}\Bigr|_{(a,b)} = \curl{\vect{F}}(a,b)\cdot\hat{k}$$
By similar reasoning, we can justify the $\hat{i},\hat{j}$ components of the curl.

\paragraph{Relation to Green's Theorem}
Green's Theorem claims that integrating $\iint_D\curl{\vect{F}}\cdot\hat{k}\,dA$ is equal to the line integral on the boundary of $D$. This can be justified by splitting the region into infinitesimal regions bounded by tiny loops. The curls on the points (tiny loops) interior to the region cancel out with their neighbors, while the curls on the boundary points' loops do not fully cancel, adding up into the elements of the line integral.

\subsubsection{Divergence}
If $\vect{F}(x,y,z)=\langle P(x,y,z), Q(x,y,z), R(x,y,z)$,
$$\divergence \vect{F} = \vect{\nabla}\cdot\vect{F}= \frac{\partial P}{\partial x} + \frac{\partial Q}{\partial y} + \frac{\partial R}{\partial z}$$
which represents the magnitude of vector "creation"/"destruction".

\paragraph{Relation to Green's Theorem}
If, instead of integrating the field vectors tangent to the curve $C$ with parameterization $\vect{r}(t) = \langle x(t), y(t)\rangle$ (as we use with the curl form of Green's Theorem), we integrate the field vectors normal and outward to $C$, we find the normal unit vectors to be
$$\vect{n}(t) = \frac{1}{|r'(t)|}\left\langle y'(t), -x'(t)\right\rangle$$ and apply Green's Theorem as follows:
\begin{align*}
    \oint_C \vect{F}\cdot\vect{n}\,ds&=\int_a^b \vect{F}\cdot\vect{n}\,|r'(t)|\,dt\\
    &=\int_a^b P(x,y)(y'(t))+Q(x,y)(-x'(t))\,dt\\
    &=\int_a^b P(x,y)\,dy-Q(x,y)\,dx\\
    &=\iint_D \frac{\partial P}{\partial x} --\frac{\partial Q}{\partial x}\,dA\\
    &=\iint_D \divergence\vect{F}\,dA
\end{align*}

\section{Surface Integrals}
\subsection{Parametric Surfaces}
A parametric representation of a surface in space is a function $\vect{R}:\mathbb{R}^2\to\mathbb{R}^3$, mapping pairs of $u,v$ coordinates to vectors representing points.
$$\vect{r}(u,v)=\left\langle x(u,v).y(u,v), z(u,v) \right\rangle$$
We will work with differentiable parametric surfaces, which will be able to accept tangent planes at all points without gaps.

Parameterizations are not unique: there are many possible parameterizations for a single curve.

\subsubsection{Trivial Parameterization}
For a surface defined by $z=f(x,y)$, the trivial parameterization
$$\vect{r}(u,v)=\left\langle u,v,f(u,v) \right\rangle$$
is a possible parameterization, but maybe not always the most useful.

\subsubsection{Solids of Revolution}
For a solid of revolution of a function $f(x)$ about the $x$-axis from $a\le x\le b$, we can parameterize it as
$$\langle u, f(u)\cos v, f(u) \sin v\rangle$$
with $a \le u=x \le b$, $v$ restricted as needed to avoid redundant points on the surface.

\subsubsection{Grid Lines}
A grid line of a parametric surface is a curve obtained by fixing one of the variables, and letting the other variable vary. Instance, $\vect{r}(u,c)$, with $c$ being a constant in the $v$ domain, produces a grid line.

\subsection{Tangent Planes to a Parametric Surface}
Like how we take derivatives $f_x$ and $f_y$ of a surface $z=f(x,y)$ to represent vectors (askew to the $x$ and $y$ axes) on the tangent plane at that point, we take the derivatives $\vect{r_u}$ and $\vect{r_v}$ to represent vectors (approximating the grid lines at that point and) on the tangent plane at that point, then cross them to get a normal to the tangent plane. Which normal, $\vect{r_u}\times\vect{r_v}$ or $\vect{r_v}\times\vect{r_u}$, is used depends on the orientation of the surface, which is defined by convention.\footnote{For surfaces given by $z=f(x,y)$, we define $w=z-f(x,y)$ and let $\vect{\nabla}$ be the normal of choice.}

\paragraph{Trivial Parameterization Normals}
$$\vect{r_u}\times\vect{r_v}=\left\langle-\frac{\partial f}{\partial x},-\frac{\partial f}{\partial y}, 1\right\rangle$$
Compare this to the normal vector we have used in the past for $z=f(x,y)$ in section \ref{partials-to-tangent}.

\subsection{Surface Area Integrals}
For a smooth surface, the surface's area near a given point $\vect{r}(u,v)$ is approximately the area of the parallelogram bound by the point and its nearby neighbors $\vect{r}(u+,v+\Delta v), \vect{r}(u+\Delta u, v)$. This can be approximated with the cross product of the tangent vectors, scaled.\footnote{It may be helpful to think of the grid line tangents as $\frac{\partial \vect{r}}{\partial u}$ and $\frac{\partial \vect{r}}{\partial v}$, and the $\Delta u,\Delta v$ as scaling those to fit. Also note that we assume the cross product is not $0$ (an ill-defined area).}
$$\Delta S \approx \left|\vect{r_u}\times\vect{r_v}\right|\Delta u\Delta v$$
Taking this approximation to the limit, and integrating over the entire domain $D$ in $u,v$, we get
$$S = \iint_DdS=\iint_{(u,v)\in D}\left|\vect{r_u}\times\vect{r_v}\right|\,du\,dv=\iint_{(u,v)\in D}\left|\vect{r_u}\times\vect{r_v}\right|\,dv\,du$$

\subsection{Surface Integrals of Scalar Functions}
To integrate a function $w=f(x,y,z)$ ($f:\mathbb{R}^3\to\mathbb{R}$) over a region in space parameterized by $\vect{r}(u,v)=\langle x(u,v), y(u,v), z(u,v) \rangle$ for $u,v \in D$ (a finite domain), we can take a surface integral
$$\iint_{(u,v)\in D} f(x,y,z)\,dS=\iint_{(u,v)\in D}f(x(u,v),y(u,v),z(u,v))\left|\vect{r_u}\times\vect{r_v}\right|\,du\,dv$$
This can be conceptualized as summing elements of infinitesimal surface area multiplied by a "height"/"temperature"/"charge" at each point in space. Alternatively, it could be visualized as finding the volume of a thick crop of perfectly straight hairs/hedgehog quills with lengths given by $f(x,y,z)$.

\subsubsection{Special Cases}
\begin{itemize}
    \item The case of $f(x,y,z)=1$ is a surface area integral.
    \item If a trivially-parameterized surface is used, $dS$ resolves to $\sqrt{1+\left(f_x\right)^2+\left(f_y\right)^2}$, which is analogous to the $\sqrt{1+(f'(x))^2}$ term from arclength integrals.
\end{itemize}

\subsection{Surface Integrals of Vector-Valued Functions}
Let $S$ be an orientable surface with orientation given by $\hat{n}$, and $$\vect{F}(x,y,z)=\left\langle P(x,y,z),Q(x,y,z),R(x,y,z)\right\rangle$$ be a continuous vector field defined $\forall (x,y,z)\in S$. The surface integral (or flux) of $\vect{F}$ on $S$ is
$$\iint_S\vect{F}\cdot\vect{n}\,dS=\iint_S\vect{F}\cdot d\vect{S}$$
Because the normal vector is one of
$$\hat{n}=\pm\frac{\vect{r_u}\times\vect{r_v}}{\left|\vect{r_u}\times\vect{r_v}\right|}$$
we can simplify
$$\hat{n}\,dS=\pm\frac{\vect{r_u}\times\vect{r_v}}{\left|\vect{r_u}\times\vect{r_v}\right|}\left|\vect{r_u}\times\vect{r_v}\right|\,dA=\pm\left(\vect{r_u}\times\vect{r_v}\right)\,dA$$

\subsection[Divergence Theorem]{Divergence Theorem\footnote{Also known as Gauss's Theorem, or Ostrogradsky's Theorem.}}
Given a vector field $\vect{F}=\left\langle P(x,y,z), Q(x,y,z), R(x,y,z) \right\rangle$ that is differentiable on an open region\footnote{We stipulate that the region itself is open so that we do not have to worry about behavior at the boundaries (single-sided limits).} including the closed region $E$, we can say that the flux on the surface $S$ that bounds $E$ is equal to the integral of the divergence within the volume $E$:
$$\iint_S\vect{F}\cdot\ d\vect{S}=\iiint_E\divergence{\vect{F}}\,dV$$
We will prove this for a region that can be described as types I, II, and III, with the assumption being that all regions can be subdivided into such chunks, with their integrals then summed to produce the integral of their union. The integral of divergence is
\begin{align*}
    \iiint_E\divergence{\vect{F}}\,dV&=\iiint\frac{\partial P}{\partial x} + \frac{\partial Q}{\partial y} + \frac{\partial R}{\partial z}\,dV \\
    &= \iiint_E\frac{\partial P}{\partial x}\,dV + \iiint_E\frac{\partial Q}{\partial y}\,dV + \iiint_E\frac{\partial R}{\partial z}\,dV
\end{align*}
We will now focus on the last integral, running with the assumption that this is a Type I region, similar arguments can be applied for the other parts of the sum. Choosing $dV=dz\,dA$, where $z=f(x,y)$ for the top portion and $z=g(x,y)$ on the bottom, and each area element $dA$ is in the projection $D$ of the region on the $xy$-plane:
\begin{align*}
    \iiint_E\frac{\partial R}{\partial z}\,dV &= \iint_{(x,y)\in D}\int^{f(x,y)}_{g(x,y)}\frac{\partial R}{\partial z}\,dz\,dA\\
    &=\iint_{(x,y)\in D}R(x,y,f(x,y))-R(x,y,g(x,y))\,dA
\end{align*}
We will now show that the flux integral's $z$-component corresponds with this term. The integral of flux is
\begin{align*}
    \iint_S\vect{F}\cdot d\vect{S}&=\iint_S\vect{F}\cdot \hat{n}\,dS=\iint_S\left(P(x,y,z)\,\hat{i},Q(x,y,z)\,\hat{j}, R(x,y,z)\,\hat{k}\right)\cdot\hat{n}\,dS\\
    &=\iint_S P(x,y,z)\,\hat{i}\cdot\hat{n}\,dS + \iint_S Q(x,y,z)\,\hat{j}\cdot\hat{n}\,dS  + \iint_S R(x,y,z)\,\hat{k}\cdot\hat{n}\,dS
\end{align*}
Just like before, we will now focus on the last integral, assuming that this is a Type I region. Splitting the surface into two trivially parameterized surfaces $S_1:\vect{r_1}(x,y)=\left\langle x,y, f(x,y) \right\rangle$ and $S_2:\vect{r_2}(x,y)=\left\langle x,y,g(x,y) \right\rangle$, with \textit{outward} normal vectors $\frac{1}{\sqrt{f_x^2+f_y^2+1}}\left\langle -f_x,-f_y,1\right\rangle$ and $\frac{1}{\sqrt{g_x^2+g_y^2+1}}\left\langle g_x,g_y,-1\right\rangle$:
\begin{align*}
    \iint_S R \,\hat{k}\cdot\hat{n}\,dS =& \iint_{S_1} R\,\hat{k}\cdot\hat{n}\,dS + \iint_{S_2} R\,\hat{k}\cdot\hat{n}\,dS\\
    =& \iint_{(x,y)\in D} R\,\hat{k}\cdot\frac{1}{\sqrt{f_x^2+f_y^2+1}}\left\langle -f_x,-f_y,1\right\rangle\sqrt{f_x^2+f_y^2+1}\,dA + \\&\iint_{(x,y)\in D} R\,\hat{k}\cdot\frac{1}{\sqrt{g_x^2+g_y^2+1}}\left\langle g_x,g_y,-1\right\rangle\sqrt{g_x^2+g_y^2+1}\,dA\\
    =& \iint_{(x,y)\in D} R\,\hat{k}\cdot\left\langle -f_x,-f_y,1\right\rangle\,dA +\iint_{(x,y)\in D} R\,\hat{k}\cdot\left\langle g_x,g_y,-1\right\rangle\,dA\\
    =& \iint_{(x,y)\in D} R(x,y,f(x,y))\, dA +\iint_{(x,y)\in D} -R(x,y,g(x,y))\,dA\\
    =& \iint_{(x,y)\in D} R(x,y,f(x,y)) -R(x,y,g(x,y))\,dA
\end{align*}
By applying similar arguments to all 3 dimensions, we have shown that the flux through a closed surface is equivalent to the divergence in the volume bounded by the surface.

\subsection{Stokes' Theorem}
Given a vector field $\vect{F}=\left\langle P(x,y,z), Q(x,y,z), R(x,y,z) \right\rangle$ that is differentiable on an open region including the smooth surface $S$, we can say that the surface integral of the curl on $S$ is equal to the loop integral of $\vect{F}$ along the boundary curve $\partial S = C$:
$$\iint_S\curl{\vect{F}}\cdot d\vect{S}=\oint_C\vect{F}\cdot d\vect{r}$$
We assume that $S$ can be given by a function $z=f(x,y)$ (similar arguments apply for $y=g(x,z)$ and $x=h(y,z)$, and by summing we can extend to more general surfaces), the normal vector $\vect{n}$ to $S$ has an upward orientation, $C$ will be oriented counterclockwise to be consistent with $\hat{n}$ (via the right-hand rule), with $C$'s projection onto the $xy$-plane being $D$. We will prove that the loop integral is equivalent to the expansion of the surface integral:
$$\iint_S\curl{\vect{F}}\cdot d\vect{S}=\iint_D \left\langle\left(\frac{\partial R}{\partial y} - \frac{\partial Q}{\partial z}\right),\left(\frac{\partial P}{\partial z} - \frac{\partial R}{\partial x}\right),\left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right)\right\rangle\cdot d\vect{S}$$
Since $d\vect{S} = \left\langle -f_x,-f_y, 1\right\rangle$ for a trivially parameterized surface $z=f(x,y)$ with upward orientation, we can expand this dot product to be
$$\iint_D -\left(\frac{\partial R}{\partial y} - \frac{\partial Q}{\partial z}\right)\frac{\partial z}{\partial x}-\left(\frac{\partial P}{\partial z} - \frac{\partial R}{\partial x}\right)\frac{\partial z}{\partial y}+\left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right)\,dA$$
For the loop integral, we consider $C: \vect{r}(t)=\left\langle x(t), y(t), f(x(t), y(t)) \right\rangle$ with $a\le t \le b$, and apply the Chain Rule. $C_1: \vect{r}(t)\cdot\langle1,1,0\rangle$, the projection of $C$ onto the $xy$-plane.
\begin{align*}
    \oint_C\vect{F}\cdot d\vect{r} =& \int_a^b P\frac{dx}{dt}+Q\frac{dy}{dt}+R\frac{dz}{dt}\,dt\\
    =& \int_a^b P\frac{dx}{dt}+Q\frac{dy}{dt}+R\left(\frac{\partial z}{\partial x}\frac{dx}{dt}+\frac{\partial z}{\partial y}\frac{dy}{dt}\right)\,dt\\
    =& \int_a^b \left(P + R\frac{\partial z}{\partial x} \right)\frac{dx}{dt}+\left(Q + R\frac{\partial z}{\partial y}\right)\frac{dy}{dt}\,dt\\
    =& \oint_{C_1} \left(P + R\frac{\partial z}{\partial x} \right)\,dx+\left(Q + R\frac{\partial z}{\partial y}\right)\,dy
\end{align*}
By Green's Theorem, the above expression is equivalent to (keeping the Chain and Product Rules in mind)
\begin{align*}
    \iint_D&\frac{\partial}{\partial x}\left(Q + R\frac{\partial z}{\partial y}\right) - \frac{\partial}{\partial y}\left(P + R\frac{\partial z}{\partial x} \right)\,dA\\
    = \iint_D&\frac{\partial Q}{\partial x} + \frac{\partial Q}{\partial z}\frac{\partial z}{\partial x}+ R\frac{\partial^2z}{\partial x\,\partial y}+\frac{\partial z}{\partial y} \left(\frac{\partial R}{\partial x} + \frac{\partial R}{\partial z}\frac{\partial z}{\partial x}\right) \\-&\frac{\partial P}{\partial y} - \frac{\partial P}{\partial z}\frac{\partial z}{\partial y} - R\frac{\partial^2 z}{\partial y\,\partial x} - \frac{\partial z}{\partial x}\left(\frac{\partial R}{\partial y } + \frac{\partial R}{\partial z}\frac{\partial z}{\partial y}{}\right)\,dA
\end{align*}
By Clairaut's Theorem, we can cancel the double partials, and we can still cancel out two more terms that have the same factors, rearranged, with opposite signs.
\[
    \iint_D\frac{\partial Q}{\partial x} +\frac{\partial Q}{\partial z}\frac{\partial z}{\partial x} + \frac{\partial z}{\partial y} \frac{\partial R}{\partial x} - \frac{\partial P}{\partial y} -  \frac{\partial P}{\partial z}\frac{\partial z}{\partial y} - \frac{\partial z}{\partial x}\frac{\partial R}{\partial y} \,dA
\]
This is the expanded form of the curl integral.

\subsubsection{Implications of Stokes' Theorem}
Stokes' Theorem implies that any two surfaces $S_1,S_2$ with the same boundary curve $C$ will have the same result for $$\iint_{S_1}\curl\vect{F}\cdot d\vect{S} = \oint_C\vect{F}\cdot d\vect{r} = \iint_{S_2}\curl\vect{F}\cdot d\vect{S}$$
As a corollary of this, any closed surface $S$ with an orientation (outward/inward) can be subdivided into two surfaces $S_1,S_2$ with the same boundary curve $C$ and normal vectors in opposite directions, so
\begin{align*}
    \oiint_S \curl\vect{F}\cdot d\vect{S} =& \iint_{S_1}\curl\vect{F}\cdot\hat{n}\,dS + \iint_{S_2}\curl\vect{F}\cdot-\hat{n}\,dS \\=& \oint_C\vect{F}\cdot d\vect{r} - \oint_C\vect{F}\cdot d\vect{r} = 0
\end{align*}
Stokes' Theorem is also the generalization of (a vector form of) Green's Theorem to 3-space, for if $S$ lies entirely within the $xy$-plane, Stokes' Theorem resolves to:
$$\iint_S \curl\vect{F}\cdot\hat{k}\,dA = \oint_C \vect{F}\cdot d\vect{r}$$

\section{Appendix}
\subsection{Cartesian Products} \label{set-multiplication}
For sets $A, B$, their Cartesian product is $$A \times B = \left\{ (a,b) | a \in A, b \in B\right\}$$
This produces a set of (ordered) 2-tuples/vectors.
The ``$n$-ary Cartesian product" generalizes this process to produce $n$-tuples/vectors, leading to a definition for ``exponentiating" sets, as in $\mathbb{R}^n$.

\subsection{Curves}
A \textbf{simple} curve has no cusps (has continuous derivatives) and does not intersect itself.

A \textbf{closed} curve bounds a region (for any starting point, a parameterization of the curve would end at itself).

\subsection{Regions} \label{regions}
A \textbf{region} $D$ in $\mathbb{R}^2$ is said to be bounded if it can be enclosed (or contained) in a circle (or other figure) with finite radius.\footnote{Note that a region in $\mathbb{R}$ is more familiarly known as an interval.}\\
A point $P$ is:
\begin{itemize}
    \item \textbf{inside} (interior to) $D$ if there exists a neighborhood of $P$ that is also contained in $D$.
    \item \textbf{outside} (exterior to) $D$ if there exists a neighborhood of $P$ that consists of only points $\notin D$.
    \item \textbf{on the boundary} of $D$ if every neighborhood of $P$ contains points inside and outside of $D$.
\end{itemize}
A region is \textbf{closed} if it contains all of its boundary points and \textbf{open} if it contains none of its boundary points.\footnote{In other words, for all points in an open set, there exists a circle surrounding the point that is wholly within the region.}

A set in $\mathbb{R}^2$ is \textbf{connected} if there exists a path connecting any two points within the set via a path inside said set. Such a region is \textbf{simply connected} if there are no holes within said set.

\subsubsection{Type I and II Regions}
A Type I region is an $\mathbb{R}^2$ region bounded between two (continuous) curves $y=g_1(x)$ and $y=g_2(x)$:
$$D=\left\{(x,y)|a\le x\le b, g_1(x)\le y\le g_2(x)\right\}$$

A Type II region is an analogous region bounded between functions of $y$:
$$D=\left\{(x,y)|c\le y\le d, h_1(y)\le x\le h_2(y)\right\}$$

Some regions can be both Types I and II.

\subsubsection{Type I -- III Regions in Space}
A Type I solid region is a $\mathbb{R}^3$ region bounded between two (continuous) surfaces $z=g_1(x,y)$ and $z=g_2(x,y)$ that can be projected onto an $xy$-plane region $D$:
$$E_I=\left\{(x,y,z)|(x,y)\in D, g_1(x,y)\le z \le g_2(x,y)\right\}$$

Type II and III are analogous but with plane projections on the $xy$ and $yz$-planes.\footnote{We do not usually care what the specific name is, but the textbook uses the following convention.}
\begin{align*}
E_{II}=\left\{(x,y,z)|(y,z)\in D, h_1(y,z)\le x \le h_2(y,z)\right\}\\
E_{III}=\left\{(x,y,z)|(x,z)\in D, i_1(x,z)\le y \le i_2(x,z)\right\}
\end{align*}

\subsection{Orientable Surfaces}
If a continuous field of unit normals can be defined for a surface (excluding edges/boundaries), then it is orientable. All non-orientable surfaces contain a Möbius strip (a non-orientable surface that can be constructed by twisting a ribbon and taping the ends together).

\end{document}
