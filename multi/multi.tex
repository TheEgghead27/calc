\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx} % Required for inserting images
\usepackage[hidelinks]{hyperref}
\usepackage{pgfplots}
\pgfplotsset{width=3in,compat=1.9}


\DeclareMathOperator{\sgn}{sgn}

\DeclareMathOperator{\comp}{comp}
\DeclareMathOperator{\proj}{proj}

\newcommand{\vect}[1]{\ensuremath{\overrightarrow{#1}}}
\newcommand{\magnitude}[1]{\ensuremath{\lVert #1 \rVert}}
\newcommand{\magvect}[1]{\magnitude{\vect{#1}}}
\newcommand{\abs}[1]{\left|#1\right|}

\title{Calculus! Calculus!}
\author{David Chen}
\date{September 2024}

\begin{document}

\maketitle

\section{Multivariable Functions and Space}

\subsection{Functions}
$$f: \mathbb{R}^n \to \mathbb{R}^m$$
Functions map a \textbf{domain} (e.g. $\mathbb{R}^n$, read ``R N") to a \textbf{codomain} (e.g. $\mathbb{R}^m$ -- see section \ref{set-multiplication} for more information). In the scope of this course, we will usually have $n$ or $m = 1$, and $n,m \in \mathbb{N}$.\footnote{In generalized multivariable calculus, more profound matrix transformations are used.}
Common function domain/codomain pairings we consider are
\begin{itemize}
    \item $f: \mathbb{R}^2 \to \mathbb{R}$ -- can be represented as mapping points on the 2D plane to label values or as a 3D \textbf{surface}.
    \item $f: \mathbb{R} \to \mathbb{R}^2$ -- can be represented as a parametric \textbf{curve}, with 1 time dimension and 2 space dimensions.
    \item $f: \mathbb{R}^3 \to \mathbb{R}$ -- can be represented as labelling each point in 3-space with a value (e.g. temperature).
    \item $f: \mathbb{R}^2 \to \mathbb{R}^2$ -- can be considered a transformation in 2-space.
\end{itemize}

\subsection{Representing N-space on Cartesian Axes}
A figure in N-space represents N values (from both the domain and codomain) per point. For instance, a number line represents 1-space, and the xy-plane represents 2-space.

\subsubsection{Axes}
For 3-space, we conventionally use 3 mutually perpendicular axes -- $x, y, z$, meeting at an origin $O(0,0,0)$ -- to represent the three dimensions. By convention, these axes follow the ``right-hand rule", with the alternative ``left-hand rule" leading to certain negations in formulae.
This model is similar to the axes in 2-space, where the RHR does not make a difference, but is harder to visualize in 4-space.\footnote{Most of us cannot visualize 4-space with 4 orthogonal axes. Please talk to your doctor if this is not the case...}

\paragraph{Sections}
There are 4 quadrants in 2-space, and 8 octants in 3-space.

\subsubsection{Line Formulae}
These familiar formulae for 2D lines extend to 3D in intuitive ways.

\paragraph{Distance}
The 2D distance formula
$$d = \sqrt{(\Delta x)^2 + (\Delta y)^2}$$
generalizes readily into the 3D distance formula
$$d = \sqrt{(\Delta x)^2 + (\Delta y)^2 + (\Delta z)^2}$$
based on the Pythagorean Theorem.

\paragraph{Midpoint}
The formula for a midpoint on a 2D line between $(x_1, y_1)$ and $(x_2, y_2)$
$$M\left(\frac{x_1+x_2}{2},\frac{y_1+y_2}{2}\right)$$
generalizes readily into the 3D midpoint formula for points $(x_1, y_1, z_2)$ and $(x_2, y_2, z_2)$
$$M\left(\frac{x_1+x_2}{2},\frac{y_1+y_2}{2}, \frac{z_1+z_2}{2}\right)$$
Alternative coordinate systems are possible, and will be discussed in a future section.

\paragraph{Partitioning}
The point P partitioning a line segment AB determined by points $A(x_1,y_1)$ and $B(x_2,y_2)$ into segments with lengths in the ratio $a:b$ has coordinates
$$P\left(x_1+\frac{a}{a+b}\left(x_2-x_1\right), y_1+\frac{a}{a+b}\left(y_2-y_1\right)\right) = P\left(\frac{ax_2 + bx_1}{a+b}, \frac{ay_2 + by_1}{a+b}\right)$$
Similarly, the point P partitioning AB in 3-space between $A(x_1, y_1, z_1)$ and $B(x_2, y_2, z_2)$ into segments with lengths in the ratio $a:b$ has coordinates
$$P\left(\frac{ax_2 + bx_1}{a+b}, \frac{ay_2 + by_1}{a+b}, \frac{az_2 + bz_1}{a+b}\right)$$

\subsubsection{Contours}
Contours allow for the construction of a $N$-dimensional representation/sampling of a $N+1$-dimensional function. For instance, contour surfaces are 3D samples of a 4D space.
We use contour curves\footnote{Although the textbook uses the term level curve interchangably it may be helpful to depict the ``level" as the $z$-value in 3-space.} to represent a 3D surface with points $(x,y,z)$ by projecting the curves produced for a fixed $z=c$ onto the $xy$-plane.
\iffalse
Below is a (low resolution) contour plot of the top half of a sphere:
\begin{center}
\begin{tikzpicture}
\begin{axis}
[
    title={Contour plot of $x^2 + y^2 + z^2 = 1$},
    view={0}{90}
]
\addplot3[
    contour gnuplot={levels={1, 0.8, 0.6, 0.4, 0.2, 0}}
]
{sqrt(x^2+y^2)};
\end{axis}
\end{tikzpicture}
\end{center}
\fi
It is useful to have constant $\Delta z$ intervals so that the contours can visually represent the steepness of the surface.

\section{Notable 2D Figures}

\subsection{Lines}

\subsubsection{Standard Form} The set of points satisfying a linear equation of the form $$Ax + By + C = 0$$
Although there are 3 nominal coefficients, there are only 2 effective coefficients because (assuming at least one of $A,B$ is non-zero), we can divide by a coefficient, thus leaving 2 unknown ratios as coefficients. This explains why 2 points are sufficient to uniquely define a line, and distinct lines can only intersect at 1 point.

\subsubsection{Additional Equations}
\begin{itemize}
    \item Point-Slope: $y-y_1 = m(x-x_1)$
    \item Slope-Intercept: $y=mx+b$
    \item Slope Form: $m=\frac{y-y_1}{x-x_1}$
    \item Intercept Form: $\frac{x}{a} + \frac{y}{b} = 1$, axis intercepts at $(a, 0)$ and $(0, b)$
    \item Polar Form: $A\left(r\cos{\theta}\right)+B\left(r\sin{\theta}\right) + C = 0$
    \item Normal Form: $x\cos{\alpha} + y\cos{\beta} = d$, d is the length of the normal to the origin, $\alpha$ is the angle of the normal to the x-axis, $\beta$ is the angle of the normal to the y-axis
    \item Determinant Form: $\begin{vmatrix}
x & y & 1\\
x_1 & y_1 & 1 \\
x_2 & y_2 & 1
\end{vmatrix}= 0$
\end{itemize}

\subsubsection{Distance from a Point}
The distance to a line $Ax+By+C=0$ from point $(x_1, y_1)$ is
$$d=\frac{\left|Ax_1 + By_1 + C\right|}{\sqrt{A^2+B^2}}$$

\subsubsection{Area of a Triangle in 2-space}
Given points $(x_1, y_1), (x_2, y_2), (x_3, y_3)$, the area of the triangle formed by these points is
$$A = \frac{1}{2}\begin{vmatrix}
    x_1 & y_1 & 1 \\
    x_2 & y_2 & 1 \\
    x_3 & y_3 & 1 \\
\end{vmatrix} = \frac{1}{2}\left|(x_1-x_2)(y_2-y_3) - (x_2 - x_3)(y_1 - y_2)\right|$$
The latter form can be derived using the distance from a point formula. See section \ref{determinant} for more information about the former form.
This is also known as the shoelace formula, because it can be represented as taking the points in order (clockwise/counterclockwise) and ``lacing" the coordinates to form equivalent products.

\subsubsection{Dimensional Analysis of the Determinant Method}
In the case of the determinant representing a triangle's area, we can translate and rotate the triangle -- since these are rigid motions, area is preserved -- making a new figure with coordinates that form a matrix with the following determinant
$$\begin{vmatrix}
    0 & 0 & 1 \\
    x_1' & 0 & 1 \\
    x_2' & y_2' & 1 \\
\end{vmatrix} = x_1'y_2'$$
where $x_1'$ represents the base length (rotated onto the x-axis), and $y_2'$ represents the height (rotated to be parallel to the y-axis). This determinant represents area (square distance units).


\subsubsection{Projection}
The projection of a line segment with slope $m$ and length $l$ onto the $x$-axis is
$$\Delta x = l\sqrt{\frac{1}{1+m^2}}$$

\subsubsection{Angle Between Lines}
For two intersecting lines with slopes $m_1$ and $m_2$, $m_1 < m_2$ the angle between the two can be found by $$\arctan{m_2} - \arctan{m_1} = \theta$$

\subsection{Conic Sections}

\paragraph{General Equation} The set of points satisfying a degree 2 equation of the form $$Ax^2+By^2 + Cxy + Dx + Ey + F =0$$
There are 6 nominal coefficients and 5 effective coefficients. This explains why 5 points are sufficient to uniquely define a conic section, and distinct conic sections can intersect at a maximum of 4 points.

\subsection{Cubics}

\paragraph{Standard Form} The set of points satisfying a degree 3 equation of the form $$Ax^3 + By^3 + Cx^2y + Dxy^2 + Ex^2 + Fy^2 + Gxy + Hx + Jy + K = 0$$
There are 10 nominal coefficients and 9 effective coefficients. However, distinct conics can intersect at more than 8 points. This is known as the Euler-Cramer Paradox.

\subsubsection{The Euler-Cramer Paradox}
For polynomials of degree $\ge 3$, Because the system of intersection points may contain rows that are \textbf{not} linearly independent, it is possible that that the determinant of the matrix would be $0$: there is not a unique solution curve for the system. See section \ref{determinant} for more information.

\section{Determinants} \label{determinant}
A \textbf{matrix} is a rectangular array of numbers.\footnote{Tensors can be considered a generalization of matrices to higher dimensions.} The dimensions are denoted $N \times M$, with $N$ being the number of rows, and $M$ being the number of columns. By convention, matrix variables are denoted with capital letters. Matrix elements can be addressed with subscripts $a_{n,m}$, with the comma delimiter being optional.
$$A = \begin{bmatrix}
 a_{11} & a_{12} & a_{13} & \cdots & a_{1m} \\
 a_{21} & a_{22} & a_{23} & \cdots & a_{2m} \\
 \vdots & \vdots & \vdots & \vdots & \vdots \\
 a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nm} \\
\end{bmatrix}$$
A \textbf{determinant} is a function that maps a real number to every \textbf{square} matrix.
The determinant of a $1\times1$ matrix $\left[a\right]$ is the scalar $a$, and the determinant of a $2\times2$ matrix $\left[\begin{smallmatrix} a & b \\ c & d \end{smallmatrix}\right]$ is $ad - bc$.

\subsection{Recursive (Cofactor) Definition}
Given an element $a_{ij}$ of matrix $A$, the corresponding minor $M_{ij}$ is the determinant obtained by deleting row $i$ and column $j$ of matrix A.
The cofactor (co-factor) of $a_{ij}$ is $$C_{ij} = (-1)^{i+j}M_{ij}$$
The determinant of matrix A is $$\det{A} = \sum_{j=1}^n a_{ij}C_{ij} = \sum_{i=1}^m a_{ij}C_{ij}$$ given a fixed value for $i$ (selecting a row) or $j$ (selecting a column) respectively.
To perform this by hand, one can take an arbitrary row/column\footnote{It is usually helpful to select a row/column with values of $0$ or $1$ to minimize needed calculations.} and then take the minors of each element in that row/column, multiplying against the sign of the cofactor. Note that the sign of the cofactor alternates in a pattern like so:
$$\begin{bmatrix}
    + & - & + & - & \cdots \\
    - & + & - & + & \cdots \\
    + & - & + & - & \cdots \\
    \vdots & \vdots & \vdots & \vdots & \ddots \\
\end{bmatrix}$$

\subsection{Explicit Definition}
This definition is listed as a fun aside; it is useful for proving determinant properties for the general case.
For the set of all permutations $S_n$, with the signature function $\sgn(\sigma)$ denoting $(-1)^n$, with $n$ being the number of transpositions (element swaps) from the original ordering of elements to get the permutation $\sigma(i)$:
$$\det{A} = \sum_{\sigma \in S_n} \sgn{\sigma} \prod_{i=1}^n a_{i,\sigma(i)}$$

\subsection{Determinant Properties}
Note that many of these properties do not have standard names.
The sum property (\ref{det-sum}), combined with the switching property (\ref{det-switch}), are sometimes referred to as the \textbf{multilinearity} of determinants in columns/rows.

\subsubsection{Scalar Multiple Property}
If any row or column is multiplied by a constant $k$, the determinant value gets multiplied by $k$.

\paragraph{Proof} Consider the recursive determinant sum for the new matrix, using the row/column that was multiplied. $k$ can be factored out of the entire sum, leaving the original determinant sum multiplied by the scalar $k$.

\subsubsection{Sum Property} \label{det-sum}
If all the elements of a row or column are expressed as a summation of two or more addends, then the determinant can be broken down as a sum of corresponding smaller elements.
$$
    \begin{vmatrix}
        a_{11} + b_{11} & a_{12} & a_{13} \\
        a_{21} + b_{21} & a_{22} & a_{23} \\
        a_{31} + b_{31} & a_{32} & a_{33} \\
    \end{vmatrix}
    =
    \begin{vmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33} \\
    \end{vmatrix}
    +
    \begin{vmatrix}
        b_{11} & a_{12} & a_{13} \\
        b_{21} & a_{22} & a_{23} \\
        b_{31} & a_{32} & a_{33} \\
    \end{vmatrix}
$$
This property can be applied multiple times to separate rows.

\paragraph{Proof} Consider the explicit definition of the determinant. Choosing $i$ or $j$ to target the addends' row/column, we can use the distributive property to split the product into two, which represent the determinant of the separate matrices.

\subsubsection{Switching Property} \label{det-switch}
If we interchange any two rows/columns of the determinant, the determinant is multiplied by $-1$.

\subsubsection{Sum of Rows Property} \label{det-sum-rows}
Let $A$ be an $n\times n$ matrix and let B be a matrix which results from adding a multiple of a row/column to another row/column. Then $\det{A} = \det{B}$.

\subsubsection{Zero Property}\label{det-zero}
The determinant of a matrix with a row/column of zeroes is $0$.

\paragraph{Proof} Consider the recursive definition of the determinant. Choosing the row with $0$s will lead to $a_{ij}$ always being $0$, so the final sum is 0.
This property allows us to show that since the determinant form of a line results in an expression of the form $Ax+By+C=0$, and that the two points $(x_1,y_1)$ and $(x_2,y_2)$ satisfy this equation, the two points definitively show that the solutions to the equation form the line.

\subsubsection{Identical Column Zero Property}
If a (square) matrix has two identical rows/columns (or rows that are a linear combination of others), then its determinant is zero.

\paragraph{Proof} Using the sum of rows property (\ref{det-sum-rows}), we can manipulate the matrix such that the row is only $0$s, thus allowing us to apply the zero property (\ref{det-zero}).\footnote{Although not mentioned in class, the determinant of a transposed matrix is equal to the original matrix, making this applicable to columns too.}

\section{3D Figures}
To draw a 3D figure, it is helpful to consider its level curves and traces (cross-sections in planes such as the $xz$ or $yz$ plane) by setting one of the variables $=0$ or $=c$.

\subsection{Planes}
\subsubsection{Drawing a Plane}
Because planes extend infinitely, we draw a quadrilateral or triangular slice of a plane in 3-space to represent it.

\subsubsection{Special Cases}
A single variable equal to a constant represents a plane parallel to the other axes' plane. For instance, the $xz$-plane can be represented with $y=0$, and $y=c$ produces planes parallel to the $xz$-plane.

A linear equation in 2 variables (e.g. $y=mx+b$) produces a plane in 3-space that intersects the equivalent line on its corresponding plane (e.g. the $xy$-plane where $z=0$) and serves as a cylinder with rulings parallel to the third axis.

\subsubsection{General Linear Functions}
$$ax+by+cz=d$$
The contours of a general linear function are evenly spaced, parallel lines. (Proof: Consider $z=c$. A series of lines linearly dependent on $c$ is produced.)
The constant term $d$ corresponds with the z-intercept ($0$, $0$, $\frac{d}{c}$).

\subsubsection{Point-Slope-Slope}
Given the slopes $m_x=\frac{\Delta{z}}{\Delta{x}}$ and $m_x=\frac{\Delta{z}}{\Delta{y}}$ and a point ($x_1$, $y_1$, $z_1$) on the plane:
$$z-z_1 = m_x(x-x_1) + m_y(y-y_1)$$
$m_x$ and $m_y$ are most easily calculated when you have lines on the $xz$ and $yz$ planes respectively, or pairs of points on parallel points (i.e. ($x_1$, $c$, $z_1$) and ($x_2$, $c$, $z_2$) for $m_x$). Note that point-slope-slope form can be rearranged into standard form. $m_x$ and $m_y$ can also be solved for from standard form by isolating $z$ and dividing by $-c$.

\subsubsection{Intercept Form}
For a plane intersecting the x-axis at $(a,0,0)$, the y-axis at $(0,b,0)$, and the z-axis at $(0,0,c)$, the equation for the plane can be defined as:
$$\frac{x}{a}+\frac{y}{b}+\frac{z}{c}=1$$
Intercept form is useful for providing the three non-collinear points that can be used to draw a plane.

\subsubsection{``Slope" on a Plane}
For an arbitrary plane defined by a line with angle $\theta$ to the $x$-axis in the $xy$-plane, the slope $m_\theta$ of the intersection line between this vertical plane and a plane with slopes $m_x$ and $m_y$ is
$$m_\theta = m_x\cos{\theta} + m_y\sin{\theta}= m_x\frac{\Delta x}{\Delta t} + m_y\frac{\Delta y}{\Delta t}$$ as shown by the right triangle (with angle $\theta$) formed by the line, the $x$-axis, and the $y$-axis.

An alternative form for $m_\theta=\frac{\Delta z}{\Delta l}$, with $l$ being the distance along the line defined by $\theta$, is $$m_\theta = \frac{m_x+m_y\tan{\theta}}{\sqrt{1+\tan^2{\theta}}}$$

\subsection{Cylinders}
Cylinders with rulings (straight lines) parallel to an axis are represented with equations in 2 variables for 3-space.
For instance, $1 = x^2 + y^2$ represents a circular cylinder, with rulings parallel to the omitted $z$-axis.
Other equations can produce non-circular cylinders.

\subsection{Quadric Surfaces}
Quadric surfaces are second-degree polynomials in 3-space. Equations are given for the surfaces on the $z$ axis, but these variables can be interchanged to produce surfaces along the $x$ or $y$ axes.

\subsubsection{Ellipsoids}
An ellipsoid has traces that are all ellipses. In analogy to 2D ellipses, the variables $a$, $b$, $c$ represents half the width of the ellipse along that axis.
$$\frac{x^2}{a^2} + \frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$

\paragraph{Spheres} are special cases of the ellipsoid (in which $a=b=c=r$), and they can also be defined as the set of points equidistant from a center, based on the 3D distance formula. A sphere of radius $r$ centered at point $(h, k, l)$ has equation
$$(x-h)^2+(y-k)^2+(z-l)^2=r^2$$

\subsubsection{Cones}
Cones have elliptical level curves, and hyperbolic vertical traces (or in the degenerate case at the origin, two crossing lines similar to the absolute value function).
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} = \frac{z^2}{c^2}$$
Notice that if instead of $z^2$, one uses $z=\sqrt{\ldots}$, a ``single" cone appears.

\subsubsection{Elliptic Paraboloid}
Elliptic paraboloids have parabolic vertical cross sections, and elliptical horizontal contours. Wow!
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} = \frac{z}{c}$$

\subsubsection{Hyperbolic Paraboloid}
Hyperbolic paraboloids have parabolic vertical cross sections, and elliptical horizontal contours. Also wow!
$$\frac{x^2}{a^2} - \frac{y^2}{b^2} = \frac{z}{c}$$
The hyperbolic paraboloid is also a ruled surface -- it is possible to run a series of straight lines across it.

\subsubsection{Hyperboloid of One Sheet}
Horizontal traces are ellipses, and vertical traces are hyperbolas (degenerate at the plane $y=b$ or $x=a$).
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2}=1$$

\subsubsection{Hyperboloid of Two Sheets}
Horizontal traces are ellipses, and vertical traces are hyperbolas. The textbook likes to say that 2 minus signs correlate with 2 sheets.
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2}= -1$$
$$-\frac{x^2}{a^2} - \frac{y^2}{b^2} + \frac{z^2}{c^2}= 1$$

\section{Vectors}

Vectors possess both magnitude and direction. They can be represented as arrows (directed line segments). They possess an initial point (tail) and a terminal point (arrow-head).
There is a one-to-one correspondence between points and (position) vectors.

\subsection{Notation and Terminology}
Vectors are usually represented with bolded or lowercase variables $\Vec{a}$, with the arrow head pointing towards the terminal point (e.g. $\overrightarrow{PQ}$). Examples in this section will be in 2-space, but can be generalized to 3-space.

Free vectors are vectors with initial points "freely roaming" about space. Position vectors are vectors with initial points on the original of space. These vectors are equivalent (see below).

The magnitude of a vector is denoted $\left|\Vec{a}\right|$ or $\left|\left|\Vec{a}\right|\right|$. This symbol is not the absolute value operator when vectors are involved.

A unit vector is a vector with a magnitude of $1$. Unit vectors are represented with a hat, such as the ortho-normal basis vectors\footnote{ortho(gonal -- perpendicular; normal -- magnitude 1; basis vectors -- vectors that can be linearly combined (i.e. added and scaled) to represent all of space} $\hat{i} = \langle1, 0$, $0\rangle$, $\hat{j} = \langle 0, 1, 0 \rangle$, $\hat{k} = \langle 0, 0, 1 \rangle$ pointing in the $x$, $y$, and $z$ directions.

The coordinates of the terminal point of a position vector are called the components of the vector $\left\langle a_1, a_2 \right\rangle$.

\subsection{Properties of Vectors}
\begin{itemize}
    \item $\Vec{a}=\Vec{b}$ if they have the same magnitude and direction. Thus, a vector is the same regardless of what specific points it originates/ends at.
    \item $-\overrightarrow{PQ} = \overrightarrow{QP}$, the negation of a vector reverses its direction.
    \item $\overrightarrow{0}$ is defined to have no direction and $0$ magnitude.
\end{itemize}

\subsection{Operations on Vectors}
\subsubsection{Addition and Subtraction}
Addition of vectors $\overrightarrow{a}$ and $\overrightarrow{b}$ can be defined as adding their components.
$$\langle a_1, a_2 \rangle + \langle b_1, b_2 \rangle = \langle a_1 + b_1, a_2 + b_2\rangle$$

Graphically, addition of two vectors can be represented as attaching the head of on vector to the tail of another, with their sum being the vector represented by the tail of the first vector and the head of the final vector. Alternatively, the parallelogram bounding the two vectors (with tails at the same point) contains the sum vector as its diagonal.

Subtraction of vectors is the addition of the negative vector.

\subsubsection{Magnitude}
The magnitude can be represented as the distance from the tail to the head. For a vector $\overrightarrow{a} = \langle a_1, a_2 \rangle$:
$$\left|\overrightarrow{a}\right| = \sqrt{a_1^2+a_2^2}$$

\paragraph{Unit Vector in the Direction of a Given Vector}
For an arbitrary vector $\overrightarrow{a}$:
$$\hat{a} = \frac{\overrightarrow{a}}{\left|\left|\overrightarrow{a}\right|\right|} = \left\langle \frac{a_1}{\left|\left|\overrightarrow{a}\right|\right|}, \frac{a_2}{\left|\left|\overrightarrow{a}\right|\right|}, \frac{a_3}{\left|\left|\overrightarrow{a}\right|\right|} \right\rangle$$
For a vector with direction described by angle $\alpha$ to the $x$-axis (and $\beta$ as its complement to the $y$-axis):
$$\hat{a} = \left\langle \cos{\alpha}, \sin{\alpha} \right\rangle = \left\langle \cos{\alpha}, \cos{\beta} \right\rangle$$
This can be generalized to 3-space by drawing perpendiculars from the terminal point of the vector to each of the axes in space, and taking the cosines of each obtained angle.

\subsubsection{Scalar Multiplication}
Multiplying a vector by a scalar $k$ results in a vector in the same direction, but with the components scaled by $k$:
$$k\overrightarrow{a} = \langle ka_1, ka_2 \rangle$$
The magnitude of the new vector is $k\left|\left|\overrightarrow{a}\right|\right|$.

\section{Vector Products}
Vector "multiplication" encompasses multiple defined products between vectors.

\subsection{Dot Product}
The dot product ("inner product", "scalar product") of two vectors $\vect{a} = \langle a_1, a_2, a_3 \rangle$, $\vect{b} = \langle b_1, b_2, b_3 \rangle$, with an angle $\theta$ between them, is the scalar
$$\vect{a} \cdot \vect{b} = \magvect{a}\magvect{b}\cos{\theta} = a_1b_1 + a_2b_2 + a_3b_3$$
Using a geometric interpretation of the two vectors and their difference, one can use the law of cosines to prove the equivalence of the two definitions.

As will be shown in the next sections, it is useful to conceptualize sums of products as dot products.

\subsubsection{Projection}
The scalar projection of a vector \vect{a} onto \vect{b} is the "component" of \vect{a} on \vect{b}, delimited by a perpendicular to \vect{b} drawn from the tip of \vect{a}. With the angle between \vect{a} and \vect{b} being $\theta$, this is
$$\comp_{\vect{b}} \vect{a} = \magvect{a}\cos{\theta} = \frac{\vect{a} \cdot \vect{b}}{\magvect{b}}$$

The vector projection of a vector \vect{a} onto \vect{b} is the "projection" of \vect{a} on \vect{b} pointing in the same direction at \vect{b}.
$$\proj_{\vect{b}}\vect{a} = \left(\comp_{\vect{b}}\vect{a}\right)\hat{b} = \frac{\vect{a} \cdot \vect{b}}{\magvect{b}^2} \vect{b}$$

\subsubsection{Properties of the Dot Product}
\begin{enumerate}
    \item Commutative Property: $\vect{a} \cdot \vect{b} = \vect{b} \cdot \vect{a}$
    \item Distributive Property\footnote{Over addition/subtraction}: $c\left(\vect{a} \cdot \vect{b}\right) = c\vect{a} \cdot \vect{b} = \vect{a} \cdot c\vect{b}$
    \item Scalar Multiplication Property: $\vect{a} \cdot \left(\vect{b} + \vect{c}\right) = \vect{a} \cdot \vect{b} + \vect{a} \cdot \vect{c}$
    \item Zero Property: $\vect{a} \cdot \vect{0} = 0$
    \item ``Squaring" Theorem: $\vect{a} \cdot \vect{a} = \magvect{a}^2$
\end{enumerate}
Note that $\vect{0}$ is the zero vector, with $0$ magnitude and undefined direction.

\subsubsection{Theorem 1: The Cauchy-Schwartz Inequality}
$$\vect{a} \cdot \vect{b} \le \magvect{a}\magvect{b}$$

The Arithmetic Mean/Quadratic Mean (AM-QM) Inequality can be proven using a vector $\langle1, 1, 1, ...\rangle$:
$$\frac{a_1 + a_2 + \cdots + a_n}{n} \le \sqrt{\frac{a_1^2 + a_2^2 + \cdots + a_n^2}{n}}$$

\subsubsection{Theorem 2: Perpendicular Vectors}
$$\vect{a} \perp \vect{b} \iff \vect{a} \cdot \vect{b} = 0$$ for $\vect{a}, \vect{b} \ne 0$, such that $\magvect{a}, \magvect{b} \ne 0$, implying that for $\magvect{a}\magvect{b}\cos{\theta} = 0$, $\cos{\theta} = 0$. More generally, the solutions for $\theta$ indicate the angle between two vectors.

\subsubsection{Perpendiculars and Angles to Lines}
A line $Ax + By + C = 0$ is parallel to $Ax + By = 0$. This can be rewritten as the dot product $\langle A, B \rangle \cdot \langle x, y \rangle$, with $\langle x, y \rangle$ representing the set of vectors parallel to the line. From Theorem 2,
$$\langle A, B \rangle \perp Ax + By + C = 0$$
the vector $\langle A, B \rangle$, along with its family of scaled vectors, is perpendicular to the line.

One can also determine the angle between lines $A_1x + B_1y = C1$ and $A_2x + B_2y = C_2$ by taking the dot product between vectors parallel to the lines (or perpendicular vectors, i.e. $\langle A_1, B_1 \rangle \cdot \langle A_2, B_@ \rangle$).

\subsubsection{Perpendiculars to Planes}
The set of all vectors $\langle x, y, z \rangle$ perpendicular to a vector $\langle A, B, C \rangle$ in 3-space is a plane.
\begin{align*}
    \langle A, B, C \rangle \cdot \langle x, y, z \rangle &= 0 \\
    Ax + By + Cz &= 0
\end{align*}

To obtain the specific plane perpendicular to a vector passing through point $(x_0, y_0, z_0)$, one evaluates the dot product
\begin{align*}
    \langle A, B, C \rangle \cdot \langle x - x_0, y - y_0, z - z_0 \rangle &= 0 \\
    A\left(x - x_0\right) + B\left(y - y_0\right) + C\left(z - z_0\right) &= 0
\end{align*}

Note that a plane of the form $z = m_xx + m_yy$ can be rewritten as
\begin{align*}
    0 &= m_xx + m_yy - z \\
    0 &= \langle m_x, m_y, -1 \rangle \cdot \langle x, y, z \rangle
\end{align*}

\subsection{Cross Product}
The cross product ("vector product", "outer product") of $\vect{a} = \langle a_1, a_2, a_3 \rangle$ and $\vect{b} = \langle a_1, a_2, a_3 \rangle$ is the vector
$$\vect{a} \times \vect{b} = \left\langle a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1\right\rangle =
    \begin{vmatrix}
        \hat{i} & \hat{j} & \hat{k} \\
        a_1     & a_2     & a_3     \\
        b_1     & b_2     & b_3     \\
    \end{vmatrix}
$$
This vector is constructed such that $\vect{a} \times \vect{b} \perp \vect{a}$ and $\vect{a} \times \vect{b} \perp \vect{b}$, representing the family of vectors perpendicular to the intersection of two planes. This third vector satisfies the right-hand rule.

\subsubsection{Properties of the Dot Product}
\begin{enumerate}
    \item Zero Property: $\vect{a} \times \vect{0} = \vect{0} = \vect{0} \times \vect{a}$
    \item $\vect{a} \times \vect{b} \perp \vect{a}$ and $\vect{a} \times \vect{b} \perp \vect{b}$
    \item Anti-Commutative Property: $\vect{a} \times \vect{b} = -\left(\vect{a} \times \vect{b}\right)$
    \item Distributive Property: $\vect{a} \times \left(\vect{b} + \vect{c}\right) = \vect{a} \times \vect{b} + \vect{a} \times \vect{c}$
\end{enumerate}
\subsubsection{Theorem 1: Magnitude of the Cross Product}\label{mag-cross-product}
The magnitude of the cross product is
$$\magnitude{\vect{a} \times \vect{b}} = \magvect{a}\magvect{b}\sin{\theta}$$

\paragraph{Lemma} Through algebraic expansion, it can be shown that $$\magnitude{\vect{a} \times \vect{b}}^2 = \magvect{a}^2\magvect{b}^2 - \left(\vect{a} \cdot \vect{b}\right)^2$$
Using the non-coordinate definition of the dot product, we can use the identity $1-\cos^2{x} = \sin^2{x}$ to prove the expression for the magnitude of a cross product where $0 \le \theta \le \pi$.

\paragraph{Corollaries}
The orthogonal component of $\vect{a}$ onto $\vect{b}$ is $$\frac{\magnitude{\vect{a}\times\vect{b}}}{\magvect{b}}$$
Using the orthogonal component as a height, it can be shown that cross product of two vectors also represents the area of the parallelogram bounded by the two vectors.

\subsubsection{Theorem 2: Parallel Vectors}
From \ref{mag-cross-product}, if $\vect{a}, \vect{b} \ne \vect{0}$, then $$\vect{a} \times \vect{b} = 0 \iff \vect{a} \parallel \vect{b}$$

\subsection{Lines in 3-space}
A line in 3-space can be defined as a vector-valued function or an equivalent parametric curve.

Lines in 3-space can intersect, be parallel, or skew (the lines live in different parallel planes).

\subsubsection{Vector Form}
Treating the set of points $(x,y,z)$ as analogous vectors:
$$\langle x, y, z \rangle = \langle x_o, y_o, z_o \rangle + \langle a, b, c \rangle t = \vect{r}(t)$$
The initial point is $\langle x_o, y_o, z_o \rangle$, and $\langle a,b,c \rangle$ is the direction vector. $t \in \mathbb{R}$, it is a scalar that allows the direction vector to trace out the whole line.\\
There are two methods for finding the line of intersection between two points.
\paragraph{Method I} By setting the equations of the two planes equal to one another, we can obtain linear expressions for two variables (e.g. $x,y$) in terms of the third variable ($z$), or expressions for all three variables in terms of a fourth parameter $t$. Any constants in these expressions are used in the initial point, any variable terms can have $t$ factored out to produce the direction vector term.

\paragraph{Method II} The cross product of the perpendiculars to two planes is parallel to the line of intersection, $\vect{n_1} \times \vect{n_2}$ can be used as a direction vector.

The offset can be determined by setting one of the variables $x, y, z = 0$, and solving for the other two variables, leading to a given point that can be used for the initial point.

\subsubsection{Parametric Form}
The linear expressions for $x,y,z$ mentioned in Method I can be used as functions in a parametric equation:
\begin{align*}
    x(t) &= x_o + at \\
    y(t) &= y_o + bt \\
    z(t) &= z_o + ct
\end{align*}

\paragraph{Symmetric Form}
We can also express the line as the set of points satisfying this equation:
$$
\frac{x-x_o}{a} = \frac{y-y_o}{b} = \frac{z-z_o}{c}
$$
The value of this ratio can be considered the ``value of $t$" in the functional forms of the line.

\section{Appendix}
\subsection{Cartesian Products} \label{set-multiplication}
For sets $A, B$, their Cartesian product is $$A \times B = \left\{ (a,b) | a \in A, b \in B\right\}$$
This produces a set of (ordered) 2-tuples/vectors.
The ``$n$-ary Cartesian product" generalizes this process to produce $n$-tuples/vectors, leading to a definition for ``exponentiating" sets, as in $\mathbb{R}^n$.

\end{document}
