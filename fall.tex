\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsfonts} % for mathbb
\usepackage{amsmath}
\usepackage{hyperref}
\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arccsc}{arccsc}

\title{Honner's Honors Calculus (Fall Semester)}
\author{David Chen}
\date{September 2023 - January 2024}

\begin{document}

\maketitle

\section{Functions}
Functions map a domain (a set of $x$ ``inputs'') to one and only one element of the range a set of ($y$ ``outputs''). Usually, we are discussing functions of \underline{x} to \underline{y}.
Functions are a subset of Relations (which relate a set of inputs to at least 1 output -- see \ref{vertical-line}).

\subsection{Representations of a Function}
There are many ways to represent a function, such as a
\begin{itemize}
    \item Table
    \item Graph
    \item Rule (including piecewise representations)
    \item Set
\end{itemize}

\subsection{Finding the Range/Domain}
For our purposes, we will usually be dealing only with real-valued functions.

A common domain will be $x \in \mathbb{R}$, all real numbers.

In this case, for every vertical line ($x=c$), that line will intercept the graph (there is a value for the function defined at that point).

Similarly, to show that the range is $y \in \mathbb{R}$, use horizontal lines ($y=c$). In general, the range will be dependent on the domain.

"Holes" in the domain, represented by open circles in graphs, are artifacts of the exclusion of some value from the domain.

\subsection{The Vertical Line Test} \label{vertical-line}
To verify that a graph of a relation is a function, one can use the vertical line test.

If for any vertical line ($x=c$), there are multiple intersections with the relation, the relation is NOT a function. Otherwise, the relation is a function.

\subsubsection{The Horizontal Line Test}\label{horizontal-line}
To verify that a function is invertible, we check if any horizontal line ($y=c$) has multiple intercepts with the graph of the function. If not, every output ($c$) will correspond with one input $x$, so $f^{-1}(y)=x$ is a valid function.

\subsection{Transformations}
Transformations alter a function. They can be composed, but function composition is not commutative,\footnote{Subtraction is anti-commutative, since switching the order of the terms will make the result negative.} so the order matters.


\subsubsection{f(x + c)}
This shifts (translates) the domain by $c$ ("moving left $c$" on the graph).
\subsubsection{f(x) + c}
This shifts (translates) the range by $c$ ("moving up $c$" on the graph).
\subsubsection{f(-x)}
This alters the domain ("reflecting over the y-axis" on the graph).
\subsubsection{-f(x)}
This alters the range ("reflecting over the x-axis" on the graph).
\subsubsection{cf(x)}
This is an axial scaling, for only one dimension is changed.

This is not a translation, nor is it a dilation (which is based on a center point and a scale factor).

\section{Limits}
If the difference between two numbers is smaller than can be imagined (arbitrarily/indistinguishable close), then the difference is $0$, and they are the same number.
$$\lim_{x\to a}f(x)=L$$
As $x$ approaches $a$ (in the domain), $f(x)$ gets arbitrarily close to $L$ (in the range).

We say that $\lim_{x\to a}\limits f(x)$ exists if
\begin{enumerate}
    \item $\lim_{x\to a^+}\limits f(x)$ exists (approaching from the positive side)
    \item $\lim_{x\to a^-}\limits f(x)$ exists (approaching from the negative side)
    \item $\lim_{x\to a^+}\limits f(x) = \lim_{x\to a^-}\limits f(x)$
\end{enumerate}

Limits may not exist for odd reasons, such as the domain being integers ($x\in\mathbb{Z}$), since limits are meant to be connected paths through the domain.

In addition, the function may exist as points the limit of the function does not, while the limit may exist at points the function may not.

\subsection{The (Delta-Epsilon) Definition of a Limit}
For a function $f(x)$ and a real number L, the statement $\lim_{x\to a}\limits f(x) = L$ means that for every $\epsilon > 0$, there exists a $\delta > 0$ such that $0 < |x-a| < \delta$ implies that $|f(x) - L| < \epsilon$.

$\epsilon$ (epsilon) represents a tolerance in the range, and $\delta$ (delta) represents a tolerance in the domain.

\subsection{Limit Laws}\label{limit-laws}
Limits are not actually numbers, but behave similarly to numbers in that there are algebraic properties related to them.

For instance, letting $b,c \in \mathbb{R}$, $n \in \mathbb{N}$, and
\begin{align*}
    \lim_{x\to c} f(x) = L  & & \lim_{x \to c} g(x) = K
\end{align*}

Then the following properties hold:
\begin{enumerate}
    \item Scalar multiple\quad$\lim_{x\to c}\limits\left[bf(x)\right] = bL$
    \item Sum/Difference\quad$\lim_{x\to c}\limits\left[f(x)\pm g(x)\right] = L \pm K$
    \item Product\quad$\lim_{x\to c}\limits\left[f(x)g(x)\right] = LK$
    \item Quotient\quad$\lim_{x\to c}\limits\left[f(x) / g(x)\right] = L / K$, if $K\ne 0$
    \item Power\quad$\lim_{x\to c}\limits\left[f(x)\right]^n = L^n$
\end{enumerate}

\subsection{Infinity}
Infinity is greater than any real number.

$$\lim_{x\to a}f(x) = \infty$$ means that as $x$ gets arbitrarily close to $a$, the value of $f(x)$ grows without bound. For any $N>0$, $\exists\delta$ s.t. $0 < |x-a| < \delta$ implies that $f(x) > N$.

To say that a limit $= \infty$ is to say that this limit does not exist in a particular way (and so the Limit Laws do not apply).

$$\lim_{x\to\infty}f(x) = L$$ means that for every $\epsilon > 0$ there exists an $N > 0$ such that $x>N$ implies that $|f(x)-L|<\epsilon$.

\subsection{Indeterminate Forms}
The indeterminate forms $\frac{0}{0}$ and $\frac{\infty}{\infty}$ are undefined, they could be anything.

For instance:
\begin{align*}
\lim_{x\to 0} \frac{\sin{x}}{x} = 1 & & \lim_{x\to 0}\frac{x}{x^2} = \infty
\end{align*}

This is not about how small/large the values get, but how fast the values get smaller/larger.

\subsection{Polynomials}
Based on the Limit Laws (\ref{limit-laws}), and the fact that $\forall c \lim_{x\to c}\limits x = c$, every polynomial\footnote{or rational function so long as f(c) exists} has the property that $\forall c \lim_{x\to c}\limits f(x)  = f(x)$.

\subsubsection{Rational Functions}
Rational functions are all of the form $\frac{P(x)}{Q(x)}$, with $P, Q$ being polynomials where
\begin{align*}
    P(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0 \\
    Q(x) = b_nx^n + b_{n-1}x^{n-1} + \ldots + b_1x + b_0
\end{align*}

$$\lim_{x\to\infty}\frac{P(x)}{Q(x)} = \begin{cases}
    0 & deg(q) > deg(p) \\
    \frac{a_n}{b_n} & deg(q) = deg(p)\\
    \pm \infty & deg(q) < deg(p)
\end{cases}$$
The last case depends on the signs of $a_n$ and $b_n$.

When $x$ is infinitely large, all that matters is the leading degree term, but when $x$ is small (near $0$), the smaller degree terms matter more.

\subsection{Asymptotes}
Any function can be an asymptote to another function. The asymptote need not appear at $\pm\infty$, such as in the case of vertical asymptotes (e.g. $y=\log{x}$ has a vertical asymptote $x=0$), and the graph of the function may intersect the asymptote at times.

\subsection{Important Theorem I}
If $f$ and $g$ agree everywhere, except possibly at $x=a$, then \[ \lim_{x \to a} f(x) = \lim_{x \to a} g(x) \] assuming these limits exist.

This allows us to treat rational functions like polynomials.

\subsection{The Squeeze Theorem}
Suppose $h(x) \le f(x) \le g(x)$ $\forall x \in$ an open interval $I$ that contains $x=c$, except possibly $x=c$ itself. If $\lim_{x\to c}\limits h(x) = L = \lim_{x\to c}\limits g(x)$, then $\lim_{x \to c}f(x) = L.$

\subsection{Special Trig Limits}\label{trig-limits}
$$\lim_{x\to 0} \frac{\sin{x}}{x} = 1$$ or in engineer terms, $\sin{x} \approx x$ for small $x$.

This is proven by the fact that $\sin{\theta}\cos{\theta} \le \theta \le \frac{\sin{\theta}}{\cos{\theta}}$. The former statement is based on the area of the arc compared to the area of the right-triangle, and the latter statement is based on the length of the arc compared to the right-triangle with the tangent line.

Because we are working in $0 \le \theta \le \frac{\pi}{2}$, $\sin{\theta} > 0$, we can flip this expression around and show via the Squeeze Theorem that the limit is 1.

$$\lim_{x\to 0} \frac{1-\cos{\theta}}{\theta} = 0$$

\subsection{Continuity}
The function $f(x)$ is continuous at $x=c$\footnote{Notice that continuity must be a local property about a point.} if
\begin{enumerate}
    \item f(c) exists
    \item the limit $\lim_{x\to c}\limits f(x)$ exists
    \item $\lim_{x\to c}\limits f(x) = f(c)$
\end{enumerate}
A function $f(x)$ is continuous on a set $S$ if $\forall a \in S$, $f(x)$ is continuous at $x=a$.\footnote{This definition may be adjusted for endpoints to use only one-sided limits.} To say that ``$f$ is continuous'' is usually to say that $f$ is continuous on $\mathbb{R}$.

Informally, a continuous function can be drawn without lifting one's pencil.

\subsubsection{Discontinuities}
\paragraph{Point Discontinuity} One point is missing in the curve. This is also called a removable discontinuity because adding that one point can fix the discontinuity.
\paragraph{Jump Discontinuity} There is a vertical gap between one portion of a function and the next.
\paragraph{Infinite Discontinuity} The function goes to $\pm \infty$ as $x \to c$.

\subsubsection{Continuous Functions}
\begin{itemize}
    \item All polynomials are continuous.
    \item All rational functions are continuous where they are defined.
    \item All trigonometric functions are continuous where they are defined.
\end{itemize}

If $f,g$ are continuous, then $f+g$, $fg$, $\frac{f}{g}$ ($g\ne0$), and $cf$ are all continuous. The composition of continuous functions is continuous.

If $f(x)$ is continuous at $x=b$ and $\lim_{x\to a}g(x) = b$, then $\lim_{x\to a}f(g(x)) = f(b)$. Limits can pass through continuous functions.

\subsubsection{The Intermediate Value Theorem (IVT)}
Suppose $f(x)$ is continuous on the \underline{closed} interval $[a,b]$, and $f(a)\ne f(b)$. If $f(a) < k < f(b)$, $\exists c \in (a,b)$ s.t. $f(c)=k$. There may be multiple such intersections, but we only require one.

\paragraph{Related Theorem A}
Suppose $f(x)$ is continuous on $[a,b]$. If $f(x)$ has no zeroes on the interval, then $f$ is either entirely positive or entirely negative on the interval.

\paragraph{Related Theorem B}
Suppose $f(x)$ and $g(x)$ are continuous on $[a,b]$. If $f(a) < g(a)$ and $f(b) > g(a)$, then $\exists c$ s.t. $f(c) = g(c)$, based on the IVT for the continuous function $f-g$.

\section{Differentiation}
\subsection{Secant Lines}
A secant line is the line joining two points $(a, f(a))$ and $(b, f(b))$ on the graph of a function. Its slope is $\frac{f(a)-f(b)}{a-b}$.

This slope can be considered the average rate of change of the function between $a$ and $b$, or an approximation to the derivative.


\subsection{Tangent Lines}
If $f(x)$ is defined on an open interval containing $c$ and if $\lim_{x\to c}\limits \frac{f(x) - f(c)}{x-c}=m$ exists, then the line passing through $(a, f(a))$ with slope $m$ is the tangent line to the graph of $y=f(x)$ at $x=c$. In this case, we write $f'(c) = m$ (f prime of c) can call $f'(c)$ the derivative of $f(x)$ at $x=c$.

A tangent line is the best local linear approximation to a function at a point. Thus, given the derivative at a point $(x, f(x))$, one can approximate $f(x\pm a) \approx f'(x)(a) + f(x)$.

Tangency is a property of a graph, not a function.

A linear approximation is usually far easier to work with compared to the original form of a function.

\subsubsection{The Vertical Difference Method}
If $g(x)=mx+b$ is tangent to $f(x)$ at $x=a$, then we expect $V(x) = f(x) - g(x)$ to have a double root at $x=a$.

This places us at the unstable singularity between the region with no intersections and 2 intersections.

Notice that $P(x) = (x-a)^2Q(x) + (mx+b)$, the remainder of dividing a polynomial $P(x)$ by $(x-a)^2$ provides the tangent line at $x=a$.
\subsection{Definition of the Derivative}
Differentiability is a local property. ``$f$ is differentiable at $x=c$'' if the limit (described below) exists.
\paragraph{At a point}
$$f'(c) = \lim_{x\to c} \frac{f(x) - f(c)}{x-c}$$

\paragraph{As a function}
$$f'(x) = \lim_{\Delta x\to 0} \frac{f(x) - f(x+\Delta x)}{\Delta x}$$

\paragraph{Symmetric Difference Quotient}
This is not quite the same as the derivative.
$$\lim_{h\to 0} \frac{f(x+h) - f(x-h)}{2h}$$

\paragraph{Leibniz Notation}
$$\frac{dy}{dx}$$
\subsubsection{Important Theorem II}
Differentiability implies continuity.\\
If $f(x)$ is differentiable at $x=c$, then $f(x)$ is continuous at $x=c$.

$|x|$ is a good example of a continuous function that is not differentiable, due to its cusp.

$x^\frac{1}{3}$ is also not differentiable at $x=0$, due to the fact that the tangent line is vertical.

\paragraph{Right/Left Differentiability}
If we wished to define a function to be differentiable on a closed interval $[a,b]$, we would have to use one-sided limits.


\subsection{Differentiation Rules}
\begin{itemize}
    \item $\frac{d}{dx}\left[ax+b\right] = a$\quad"The derivative of a linear line is its slope."
    \item $\frac{d}{dx}\left[c\right] = 0$\quad"The derivative of a constant function is 0.
    \item $\frac{d}{dx}\left[f(x) \pm g(x)\right] = \frac{d}{dx}\left[f(x)\right] \pm \frac{d}{dx}\left[g(x)\right]$
    \item $\frac{d}{dx}\left[cf(x)\right] = c\frac{d}{dx}\left[f(x)\right]$
    \item $\frac{d}{dx}\left[x^n\right] = nx^{n-1}$\quad The Power Rule -- proven using the Binomial Theorem for integers, applicable to all real exponents \emph{except $n=-1$}.
    \item $\frac{d}{dx}\left[\sin{x}\right] = \cos{x}$\quad Proven using the limits from \ref{trig-limits}.
    \item $\frac{d}{dx}\left[\cos{x}\right] = -\sin{x}$
    \item $\frac{d}{dx}\left[\tan{x}\right] = \frac{1}{\cos^2{x}} = \sec^2{x}$
    \item $\frac{d}{dx}\left[\cot{x}\right] = \frac{-1}{\sin^2{x}} = -\csc^2{x}$
\end{itemize}

\subsubsection{The Product Rule}
$$\frac{d}{dx}\left[f(x)g(x)\right] = f(x)g'(x) + g(x)f'(x)$$
"First times the derivative of the second plus the second times the derivative of the first."

\subsubsection{The Quotient Rule}
$$\frac{d}{dx}\left[\frac{p}{q}\right] = \frac{d}{dx}\left[p\cdot q^{-1}\right] = \frac{g(x)f'(x) - f(x)g'(x)}{\left(g(x)\right)^2}$$
"Low $d$ high minus high $d$ low over low squared."

\subsubsection{The Chain Rule}\label{chain}
$$\frac{d}{dx}\left[f(g(x))\right] = f'(g(x))g'(x)$$

\subsubsection{Note} The derivative of even functions is odd, and vice versa.

\subsection{Implicit Differentiation}
If you cannot solve for an explicit form of $y=f(x)$, you can still differentiate to find an expression for $\frac{dy}{dx}$ (in terms of $x$ and $y$) by differentiating the two sides of an equation, considering the chain rule.

For instance $\sin{y}=x$ has derivative $\frac{dy}{dx}=\sec{y}$ from implicit differentiation.

This method is helpful in solving related rates problems.

\section{Induction}
Induction can be proven by the Well Ordering Principle (from Number Theory), that there must be a minimum element of a set, and thus the set of $n$ where $P(n)$ is false would have a contradiction if a greater $n$ failed.

\subsection{Weak Induction}
Let $P(n)$ be a statement about the natural number $n$. If the following are true
\begin{enumerate}
    \item $P(0)$ is true
    \item $P(k)$ is true implies $P(k+1)$ is true
\end{enumerate}
Then $P(n)$ is true for all $n\in\mathbb{N}$

In short, a base case must be proven true, and then the inductive step is to assume $P(n)$ is true for some $k$, and then prove part 2.

"Since $P(k)$ is true implies $P(k+1)$ is true, and $P(1)$ is true, we know $P(n)$ is true for all $n\ge1$."

\subsection{Strong (Complete) Induction}
Let $P(n)$ be a statement about the natural number $n$. If the following are true
\begin{enumerate}
    \item $P(0)$ is true
    \item $P(0), P(1), P(2), \ldots, P(k)$ all being true implies $P(k+1)$ is true
\end{enumerate}
Then $P(n)$ is true for all $n\in\mathbb{N}$

Strong Induction explicitly states that all the statements prior to $P(k)$ are true. (The assumption, not the conclusion, is stronger.) It is equivalent to Weak Induction.

\subsection{Useful Results} \label{induction-closed}
\begin{itemize}
    \item The Power Rule
    \item $\sum_{i=0}^{n-1} 2i+1 = n^2$
    \item $\sum_{i=1}^{n} i= \frac{n(n+1)}{2}$
\end{itemize}

\subsubsection{Binomial Theorem}
$$(x+y)^n=\binom{n}{0}x^n + \binom{n}{1}x^{n-1}y^1 + \binom{n}{2}x^{n-2}y^2 + \ldots + \binom{n}{n-1}x^1y^{n-1}+\binom{n}{n}y^n$$

\paragraph{Binomial Coefficient} ``n choose k'' or ``nCk'' or $\binom{n}{k}= \frac{n!}{\left(n-k\right)!k!}$

Pascal's Triangle encodes the binomial coefficients.

Note that $\binom{n}{0}=1$, $\binom{n}{1} = n$.

\subsubsection{The Division Algorithm}
Given $n,m \in \mathbb{Z}$ and $n\ge m > 0$, $\exists q, r \in \mathbb{Z}$ with $n =qm+r$,$q>0$, $m>r\le0$. Proven by strong induction, cases are $n-m<m$ in which case $r$ is incremented, or $n-m \ge m$ so $q$ is incremented.
\
\paragraph{The Division Algorithm for Polynomials}
Given $P(x)\ge M(x) > 0$ in degree, $\exists Q(x), R(x)$ with $P(x) = Q(x)M(x) + R(x)$. This can be proven by induction on the degree of polynomial.

Thus, we can use polynomial long division.

\section{Polynomials}
Polynomials are a generalization of numbers, but with base $x$.

\subsection{Factoring}
One way of factoring is division with $q(x)$ having degree $1$, leaving $r(x)$ with degree 0 (a constant). (see \ref{remainder})

\subsubsection{The Location Principle}
An application of the IVT on polynomials to find positions where $P(x) = 0$.

\subsubsection{The Zero Product Property}
If $a,b \in \mathbb{R}$ and $ab=0$, then $a=0$ or $b=0$. This extends to complex numbers, but not modular arithmetic.

This is why factoring reveals roots.

\paragraph{The Remainder Theorem}\label{remainder}
If you divide a polynomial $p(x)$ by $(x-a)$, the remainder it $p(a)$.
$$P(a) = (a-a)q(x) + R = 0 + R = R$$

\paragraph{The Factor Theorem} $(x-a)$ is a factor of $p(x)$ if and only if $P(a) = 0$. A factor has no remainder upon division.

\paragraph{The Conjugate Root Theorem} If the polynomial $P(x)$ has real coefficients, then non-real roots occur in conjugate pairs.

Further ``Tools for Root Finding'' are left to the reader to recall.

\subsection{Complex Numbers}
$\mathbb{C}=\left\{a+bi|a,b\in\mathbb{R}\right\}$ is the set of complex numbers, where $i$ has the property that $i^2 = -1$.

$i$ is the imaginary unit. $a$ is the real part, and $b$ is the imaginary part.

\subsubsection{The Argand Plane} Also known as the "complex plane". Uses a real and imaginary axis, and all complex can be represented as vectors/points on this plane.

\subsubsection{Operations on Complex Numbers}
\paragraph{Conjugate}
If $z=a+bi$, $\overline{z}=a-bi$ (a reflection over the real axis).

\paragraph{Addition}
$$(a+bi) + (c+di) = (a+c) + (b+d)i$$

$z+\overline{z} \in \mathbb{R}$

\paragraph{Multiplication}
$$(a+bi) \cdot (c+di) = (a+bi)c + (a+bi)di$$

Multiplication by $i$ can be thought of as a rotation by $90\deg$ in the complex plane.

$z\cdot\overline{z} = a^2 + b^2 = |z|^2 \in \mathbb{R}$

\paragraph{Division}
Division is multiplication by the multiplicative inverse.
$$\frac{1}{z} = \frac{1}{|z|^2}\overline{z}$$

\paragraph{Additional Conjugate Identities}
\begin{enumerate}
    \item $\overline{\overline{z}} = z$.
    \item $z = \overline{z}$ iff $z\in\mathbb{R}$
    \item $\overline{z+w} = \overline{z} + \overline{w}$
    \item $\overline{zw} = \overline{z} \cdot \overline{w}$
    \item $\overline{\frac{z}{w}} = \frac{\overline{z}}{\overline{w}}$
    \item $\overline{z^n} = \overline{z}^n$ for $n \in\mathbb{N}$
\end{enumerate}

\subsubsection{Relation to Polynomials}
All complex numbers can be thought of as polynomials in $i$.

\paragraph{Fundamental Theorem of Algebra} Any polynomial (with complex coefficients) as at least $1$ complex root. By induction, we can prove that a polynomial of degree $n$ has $n$ roots (perhaps non-unique).\footnote{This was Gauss's PhD thesis when he was 16/17...}


\section{Differentiation Applications}
\subsection{Extrema}
Let $f(x)$ be defined on an interval $I$ containing $c$.\\
We say that $f(c)$ is a maximum for $f$ on $I$ if $\forall x \in I$ $f(c) \ge f(x)$.
We say that $f(c)$ is a minimum for $f$ on $I$ if $\forall x \in I$, $f(c) \le f(x)$.

\subsubsection{The Extreme Value Theorem} If $f$ is continuous on a closed interval $I$, then $f$ takes a maximum and a minimum value on $I$.

Open intervals may not have a definite maximum/minimum, whether that be because the function veered off to infinity or because the function can only approach some bounding value.

The EVT is an existence theorem, which guarantees that extrema exist, but not where they are.

\paragraph{Finding extrema}
Given a continuous function $f(x)$ on a closed interval $I$ (satisfying the EVT), extrema can only occur at
\begin{enumerate}
    \item endpoints of the interval
    \item $f'(x) = 0$
    \item $f'(x)$ is undefined
\end{enumerate}
The values of $x$ pertaining to \emph{the latter two cases} are called the critical numbers of $f$.

The general strategy for finding extreme values of $f$ on $I = [a,b]$ is to find the critical numbers, and then evaluate for $f(x)$ at the critical numbers at $a,b$.

\subsubsection{Local Minima/Maxima}
If there is an open interval containing $c$ on which $f(c)$ is a maximum/minimum, then $f(x)$ is a local/relative maximum/minimum.

Because the interval is open, the endpoints of such an interval do \emph{not} count.

If $f(x)$ is differentiable, then relative extrema occur at critical numbers.

\paragraph{Fermat's Theorem (on Stationary Points)}
If $f$ has a local maximum or minimum at $x=c$ and $f'(c)$ exists, then $f'(c)=0$.

The converse is \emph{not} true, if $f'(x) = 0$, $f(c)$ might not be a local maximum or minimum. A canonical example is $f(x) = x^3$, where $x=0$ is not a local minimum/maximum, the graph simply "flattens out".

\paragraph{The First Derivative Test} Suppose $f$ is continuous on an open interval $I$ about $c$, a critical number of $f$. If $f$ is differentiable on $I$ (except possibly at $x=c$), then $f(c)$ can be classified as follows:
\begin{itemize}
    \item If $f'$ changes from $+$ to $-$ at $x=c$, then $f(c)$ is a relative maximum.
    \item If $f'$ changes from $-$ to $+$ at $x=c$, then $f(c)$ is a relative minimum.
    \item If $f'$ does not change sign, then no conclusion can be drawn.
\end{itemize}

This is based on \ref{inc-dec-test} and the fact that going from increasing to decreasing would produce a relative minimum, and decreasing to increasing would produce a relative maximum.

\paragraph{The Second Derivative Test} Suppose $f''(x)$ is continuous near $c$. Then \begin{enumerate}
    \item If $f'(c) = 0$ and $f''(c) > 0$, then f has a local minimum at $x=c$.
    \item If $f'(c) = 0$ and $f''(c) < 0$, then f has a local maximum at $x=c$.
\end{enumerate}


\subsection{Rolle's Theorem}
Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$. If $f(a)=f(b)=d$, then $\exists c \in (a,b)$ s.t. $f'(c) = 0$.
This is trivial to prove for the constant case $f(x)=d$.

For the non-constant case, there is a maximum/minimum value $f(c)$, $c\in[a,b]$, $f(c) \ne d$, with $c$ not being an endpoint. Since the absolute maximum is also a local maximum, $x=c$ is a critical number. Since $f$ is differentiable, $f'(c)$ exists, so $f'(c)=0$.

In short, the EVT allows us to find an absolute maximum, which can be found to be a local maximum (as it is not at an endpoint), and thus Fermat's Theorem applies, and so $f'(c)=0$.

\subsubsection{The Mean Value Theorem}\label{MVT} A generalization of Rolle's Theorem. Also a Very Important Theorem.\\
Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$. $\exists c \in (a,b)$ s.t. $$f'(c) = \frac{f(b)-f(a)}{b-a}$$
The slope of the tangent line (instantaneous rate of change at $c\in(a,b)$) will be equal to the slope of the secant line (average rate of change on $[a,b]$).
This can be proven by considering $h(x) = f(x) - g(x)$, where $g(x)$ is the secant line through $(a,f(a))$ and $(b,f(b))$.

Like the EVT, the MVT is also an existence theorem.

\paragraph{Theorem A}
Suppose $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$. If $f'(x)=0$ $\forall x\in(a,b)$, then $f(x)$ is constant.

\paragraph{Theorem B}
Suppose $f,g$ are continuous on $[a,b]$ and differentiable on $(a,b)$. If $f'(x)=g'(x)$ $\forall x\in(a,b)$, then $f(x) - g(x)$ is constant. (Thus, $f(x) = g(x) + C$.)

\subsection{Increasing/Decreasing}
A function $f(x)$ is increasing on an interval $I$ if for $x_1, x_2 \in I$, $x_1 < x_2$ implies $f(x_1) < f(x_2)$.\\
A function $f(x)$ is decreasing on an interval $I$ if for $x_1, x_2 \in I$, $x_1 < x_2$ implies $f(x_1) > f(x_2)$.

\subsubsection{Increasing/Decreasing Test}\label{inc-dec-test}
If $f'(x) > 0$ on $I$, then $f$ is increasing on $I$.\\
If $f'(x) < 0$ on $I$, then $f$ is decreasing on $I$.

These can be proven using MVT-justified linearization ($c \in (x_1,x_2)$).

\subsection{Concavity}
The \underline{graph} of $f(x)$ is concave upwards on an interval $I$ if all tangent lines of $f$ on $I$ lie below the graph.\\
The \underline{graph} of $f(x)$ is concave downwards on an interval $I$ if all tangent lines of $f$ on $I$ lie below the graph.

\subsubsection{Test for Concavity}
Let $f$ be a function whose second derivative exists on an open interval $I$.\\
If $f''(x) > 0$ for $x \in I$, then $f$ is concave up on $I$.\\
If $f''(x) < 0$ for $x \in I$, then $f$ is concave down on $I$.

In other words, tangent lines at concave up sections of a curve are underapproximations, while tangent lines at concave down sections of a curve are overapproximations.

\subsubsection{Point of Inflection} A point $P$ on a curve $y=f(x)$ is called an inflection point if $f$ is continuous at $P$ and the curve changes from concave up to concave down (or vice versa) at P.\footnote{Notice that $f$ need not be differentiable.}

Lines have no points of inflection.

\subsection{Optimization}
The maximization/minimization of a function modeling another quantity with constraints.

First, we go from an objective function $A(x,y)$ that we wish to optimize, and constrain the domain, then find the extrema we want on this domain.

We subject the objective functions to constraints such as $x = f(y)$ so as to ensure make the problem solvable with single-variable calculus.

\paragraph{Note} To minimize $d(x) = \sqrt{f(x)}$, it is sufficient to minimize $f(x)$.

\section{Integration}
\subsection{Summation Notation}
$$\sum_{i=0}^{u}$$
$i$ is the index of summation, $0$ is the lower limit/bound, and $u$ is the upper limit/down.

The open form of a sum writes out the terms explicitly ($a_1 + a_2 + a_3 + \ldots + a_n$), and the closed form of a sum has an explicit formula in terms of anything but the index of summation (usually the upper limit).

Sums can be re-indexed by replacing the index of summation, or by offsetting the upper/lower bounds and expression. For instance:
$$\sum_{j=2}^{10} j^2 = \sum_{k=2}^{10} k^2 = \sum_{n=7}^{15} (n-5)^2 =$$

If the upper limit is a variable, perhaps $n$, that expression is now a function of $n \in \mathbb{Z}, n \ge k$, with $k$ being the index.

\paragraph{Closed Formulas}can be proven using induction, leading to results such as those in \ref{induction-closed}.

\subsubsection{Algebra of Summations}
\begin{itemize}
    \item $\sum_{i=1}^{n}\limits Ca_i = C\sum_{i=1}^{n}\limits a_i$
    \item $\sum_{i=1}^{n}\limits \left(a_i \pm b_i\right) = \sum_{i=1}^{n}\limits a_i \pm \sum_{i=1}^{n}\limits b_i$
    \item $\sum_{i=1}^{n}\limits a_i = \sum_{i=1+p}^{n+p}\limits a_{i-p}$
\end{itemize}

However, $$\sum_{i=1}^{n}\limits a_ib_i \ne \left(\sum_{i=1}^{n}\limits a_i\right)\left(\sum_{i=1}^{n}\limits b_i\right)$$

\subsubsection{Finite Difference Method}
For sequences that can be modeled by a $k$-degree polynomial, by taking the sequence of the difference between terms, and recursively repeating for $k$ iterations until there is a finite difference between terms, we can find the value of $k$ and solve the system of equations for the coefficients of the polynomial.

\subsection{Anti-Differentiation}
Suppose $F(x)$ is a function with derivative $F'(x)$. Then $$\int F'(x)dx = F(x) + C$$

We call $F(x)$ the anti-derivative of $F'(x)$ and $\int F'(x)dx$ the indefinite integral of $F'(x)$.

Refer to section \ref{MVT} to reaffirm why $+C$, the constant of integration, is needed to select a function $F(x)$ from a family of anti-derivative functions.\footnote{Note that every indefinite integral equation is equivalent to a differential function.}

\subsection{Area Approximations}
To approximate area underneath a curve, we can add up rectangles (or right trapezoids) that correspond with the space under the curve.

\subsubsection{Endpoint Approximations}
If we were to inscribe the rectangles in the region, we would have an underapproximation, but if we were to circumscribe the rectangles, we would have an overapproximation.

For increasing functions, a left-endpoint sum is an underapproximation, and a right-endpoint sum is an overapproximation. The reverse is the case for decreasing functions.

For an increasing function, $\Delta x = 1$, $$\sum_{n=0}^{3} f(n) \Delta x < A < \sum_{n=1}^{4} f(n) \Delta x$$ would be a bounding of the true area of $f(x)$ from $0$ to $4$ based on a left-endpoint approximation with 4 rectangles, and a similar right-endpoint approximation.

If we were to allow $\Delta x \to 0$, we would get more and more accurate approximations. If corresponding limits existed, we could use a Squeeze Theorem argument to find the true value of A.

\paragraph{Midpoint/Trapezoidal Approximations}
If we were to instead use the average of $f(a)$ and $f(b)$ at the two endpoints, we may have a more accurate representation of the area using trapezoids. Whether the trapezoids over or under-approximate the function depends on the concavity of the function (consider the tops of the trapezoids to be very small secant lines).

\subsubsection{Upper/Lower Sums}
On each subinterval $[x_i, x_{i+1}]$, we choose $x_i^*$ so $f(x_i^*)$ is the maximum on that subinterval. Thus, we get an upper sum. Similarly, we choose a different $x_i^*$ for the minimum on the subinterval to get the lower sum.

More rectangles are not always, unless you have enough of them, such as in the case of a constant function, or in cases where the addition of rectangles introduces additional error (and so you need thinner slices).

\subsubsection{Definite Integrals -- Riemann Sums} A generalization of the left/right/midpoint approximation method.
$$\sum_{i=0}^{n}f(c_i)\Delta x_i$$
$\Delta x_i$ is the width of the $i$th subinterval ($x_{i+1} - x_i$).\\The set $\left\{x_i\right\}$ partitions the interval $[a,b]$ into a mesh of (potentially unequal) subpartitions.\\
$\left\{c_i\right\}$ is the set of representatives picked from the interval.

We define
$$\lim_{n\to\infty}\sum_{i=0}^{n}f(c_i)\Delta x_i = \int_a^b f(x)dx$$ to be the definite integral of $f(x)$ from $a$ to $b$ provided that the limit exists and is the same for \emph{any choice of} $\left\{c_i\right\}$.

Definite Integrals are simultaneously
\begin{enumerate}
    \item The limit of a sum approaching infinite terms
    \item Area under a curve (sort of)
    \item A difference of anti-derivatives (by the FTC)
\end{enumerate}

When the definite integral exists, we say that $f$ is \emph{integrable} on $[a,b]$.

Integrable functions are a superset of the continuous functions, as non-continuous functions with a finite number of jump discontinuities are also integrable.

\paragraph{Properties of the Definite Integral}
Because the Definite Integral is in its heart a Riemann Sum, these properties are related to properties of sums. Further, \underline{accumulation problems} may rely on the application of integration.
\begin{itemize}
    \item If $f(x)$ is defined at $x=a$, then $$\int_a^a f(x)dx=0$$
    \item If $f(x)$ is integrable on $[a,b]$, then $$\int_a^b f(x)dx=-\int_b^a\limits f(x)dx$$
    \item If $f(x)$ is integrable on the intervals of $a, b, c$ then $$\int_a^b f(x)dx = \int_a^c f(x)dx + \int_b^c f(x)dx$$
    \item If $f(x)$ is integrable on $[a, b]$, then $$\int_a^b kf(x)dx = k\int_a^b f(x)dx$$
    \item If $f(x)$ and $g(x)$ are integrable on $[a, b]$, then $$\int_a^bf(x)\pm g(x)dx = \int_a^b f(x)dx \pm \int_a^b g(x)dx$$.
\end{itemize}

\subsection{The Fundamental Theorem of Calculus}
If $f$ is continuous on $[a,b]$ and $F$ is an anti-derivative of $f$ on $[a,b]$, then $$\int_a^bf(x)dx=F(b)-F(a)$$

\subsubsection{Proof}
Suppose $f$ is continuous on $[a,b]$ and we want to compute $$\int_a^bf(x)dx = \lim_{n\to\infty}\sum_{i=1}^nf(c_i)\Delta x$$
We partition $[a,b]$ to be $\left\{x_0,x_1,x_2,\ldots,x_n\right\}$ with $x_0=a$, $x_n=b$, so the $i$\textsuperscript{th} subinterval is $[x_{i-1},x_i]$.

Assuming $f$ has an anti-derivative $F(x)$ such that $F'(x)=f(x)$, we can use the MVT ($F$ is differentiable and thus continuous as it has derivative $f$) to claim that we can always pick a $c_i\in(x_{i-1},x_i)$ such that $$F'(c_i)=f(c_i)=\frac{F\left(x_i\right)-F\left(x_{i-1}\right)}{x_i-x_{i-1}}$$

We can then construct a telescoping sum that leaves only $F(x_n)-F(x_0)=F(b)-F(a)$

\subsubsection{The Second Fundamental Theorem of Calculus}
If $f(x)$ is continuous on $[a,b]$ then for every $x\in[a,b]$, the function defined by $g(x)=\int_0^xf(t)dt$ is continuous on $[a,b]$ and differentiable on $(a,b)$, and $g'(x)=f(x)$.

\subsubsection{Average Value of a Function (The MVT for Integrals)}
If $f$ is continuous on $[a,b]$ then there exists a $c\in[a,b]$ s.t. $f(c)(b-a)=\int_a^bf(x)dx$. This is simply an application of the MVT (\ref{MVT}) to the integrals (continuous functions).
$$f(c) = \frac{1}{b-a}\int_a^bf(x)dx=\frac{F(b)-F(a)}{b-a}$$
$f(c)$ is the average value of the function over the interval $[a,b]$ ("reshaping" the area under the curve to be a rectangle).

\subsection{Inverse Functions}
For two functions $f,g$, $g=f^{-1}$ (``$f$ inverse'') if and only if $g(f(x)) = f(g(x)) = x$. When $f$ and $g$ are composed, they form the identity function.

If the domain of $f$ is $A$, and its range is $B$, the domain of $g$ is $B$ and the range is $A$.

A function is not invertible if the process of inversion produces a non-function relation. This can be tested for with the Horizontal Line Test (\ref{horizontal-line}).

\subsubsection{One-to-One}
A function is one-to-one if $f(a)=f(b)$ implies $a=b$. A function is invertible if and only if it is one-to-one.

\paragraph{Useful Theorem} If $f$ is defined on $[a,b]$ and $f$ is increasing/decreasing on $[a,b]$, then $f$ is one-to-one on $[a,b]$.

Even functions cannot be one-to-one (unless they are only defined at $x=0$).

\subsubsection{Derivatives of Inverse Functions} \label{inverse-derivatives}
If $f(x)$ is differentiable and $f^{-1}(x)$ exists, then if $f(a)=b$, we know that
$$(f^{-1})'(b)=\frac{1}{f'(f^{-1}(b))}=\frac{1}{f'(a)}$$
so long as $f'(a)\ne0$.

If $f$ is increasing/decreasing on its domain, $f^{-1}$ is also increasing/decreasing on its domain.

\subsection{The Natural Logarithm}
$f(x)=\frac{1}{x}$ is its own inverse. Thus, $F(x)=\int_1^x\frac{1}{x}$ has interesting properties:
\begin{enumerate}
    \item $F(1)=0$
    \item $F(ab)=F(a)+F(b)$
    \item $F(a^n)=nF(a)$
    \item $F(\frac{a}{b})=F(a)-F(b)$
\end{enumerate}
We call $F(x)$ the natural logarithm.\footnote{$\ln{x}$ can be considered "log base $e$." Because $F(x)$ is a one-to-one function for $x>0$, there exists some unique "base" $e$ such that $F(x)=1$.} We write $\ln{x}=\int_1^x\limits\frac{1}{t}dx$ for $x>0$.\\
For all $x$ in the domain of $\frac{1}{x}$:
\begin{align*}
  \frac{d}{dx}\ln{|x|}=\frac{1}{x}\\
  \int\frac{1}{x}dx=\ln{|x|}+C
\end{align*}

Because the logarithm function is one-to-one on its domain of $x>0$, it can be helpful for solving equations involving the exponential function.

\subsubsection{Change of Base}
Logarithms base anything other than $e$ are simply a domain scaling of the natural logarithm.

$\log_bx=\frac{\ln{x}}{\ln{b}}$

\subsubsection{\texorpdfstring{$e$ as a Limit}{e as a Limit}}
$$e=\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n$$

\subsubsection{Exponential Functions}
We define $e^x=\ln^{-1}x$. Based on the derivative of inverse functions (\ref{inverse-derivatives}), $\frac{d}{dx}e^x=\frac{1}{\frac{1}{e^x}}=e^x$

$e^x$ has properties corresponding with the logarithm, such as $e^xe^y=e^{x+y}$.

\paragraph{Change of Base}
Because $e^{\ln{b}}=b$ (these are inverses), any exponential function can be rewritten as
$$b^x=\left(e^{\ln{b}}\right)^x$$

\subsection{Integration Techniques}
\subsubsection{u-Substitution}
We define a variable $u$ to be equal to an expression in terms of $x$. We then differentiate, and treating $du$ and $dx$ like variables\footnote{Every serious math person is crying right now... thankfully I am an engineer...}, we can substitute $dx$ (and a bunch of other stuff, perhaps cancelling as needed) for $du$.
This attempts to invert the Chain Rule (\ref{chain}).

This method requires some amount of trial and error.

This method is not to be confused for substituting \emph{equivalent} expressions (such as $\sin^2{\theta} = \frac{1-\cos{2\theta}}{2}$, $\cos^2{\theta} = \frac{1+\cos{2\theta}}{2}$).

\subsubsection{Integration by Parts, or a Method of Successive Approximation}
$$\int u\,dv=uv-\int v\,du$$

Integration by Parts is useful for the integral of products of simpler functions. You define one factor to be $u$ and another (including the $dx$ to be $dv$)\footnote{You can even use $dv=dx$, $v=x$}. You then define $v$ to be the anti-derivative of $dv$.

For integrals/derivatives that appear to "cycle" ($\sin{x}$,$\cos{x}$,$e^x$), one may be able to repeatedly integrate by parts, then solve with algebra.

This attempts to invert the Product Rule.

\subsubsection{Partial Fraction Decomposition}
When trying to integrate something of the form
$$\int\frac{1}{(x-a)(x-b)}\,dx$$
it is possible to split the fraction into a sum of the two factors of the denominators like so
$$\frac{1}{(x-a)(x-b)}=\frac{A}{x-a}+\frac{B}{x-b}=\frac{A(x-b)+B(x-a)}{(x-a)(x-b)}$$

Leveraging the two following facts:
\begin{enumerate}
    \item Two equivalent rational functions with the same denominator must have the same numerator.
    \item Two polynomials are equal if and only if their corresponding coefficients are equal.
\end{enumerate}
Because polynomials are only equal when their coefficients are equal, there are actually two equations encoded by $0x+1=Ax+Bx+Ab+aB$,
\begin{align*}
    0=A+B
    1=Ab+aB
\end{align*}
and you can solve for $A$ and $B$.

\subsubsection{Trigonometric Substitution}
See \ref{trig-sub}.

\section{Trigonometry}
Unit Circle Trigonometry extends Right-Triangle Trigonometry -- triangle measuring -- to $\theta \notin [0, 2\pi]$.
Angles are placed in standard position, with the initial side pointing along the $x$-axis at $\theta=0$, and the terminal side pointing at $\theta$.\\
$\cos{\theta}$ is defined to be the $x$-coordinate of the intersection of the terminal side and the unit circle.\\
$\sin{\theta}$ is defined to be the $y$-coordinate of the intersection of the terminal side and the unit circle.\\

The circular nature of the unit circle explains the periodicity of the trigonometric functions, the terminal side will simply go round and around, leaving us with "circular functions."

\subsection{Definitions of other Trigonometric Functions}
\begin{align}
    \tan{\theta} = \frac{\sin{\theta}}{\cos{\theta}}
    \cot{\theta} = \frac{\cos{\theta}}{\sin{\theta}}
    \csc{\theta} = \frac{1}{\sin{\theta}}
    \sec{\theta} = \frac{1}{\cos{\theta}}
\end{align}

\subsection{Periodicity}
A function $f(x)$ is $p$-periodic if there exists $p>0$ such that $\forall x \in \mathbb{R}$, $f(x+p)=f(x)$.

If $f(x)$ is $p$-periodic, then $f(x)$ is $np$-periodic, $n\in\mathbb{N}$.

The minimal period of $f(x)$ is called the fundamental period of $f(x)$, if it exists. The fundamental frequency of a period is $f=\frac{1}{p}$, where $p$ is the fundamental period.

The fundamental period of the sum/product of two functions $f(x)$ and $g(x)$, with fundamental periods $p,q$ respectively, is the least common multiple of $p$ and $q$.

If $m(x)$ is $p$-periodic, $f(m(x))$ is also $p$-periodic.

The derivative of a periodic function is also periodic, but the anti-derivative is not guaranteed to be so.

\subsubsection{Anti-Periodicity}
We say that a function is anti-periodic when $f(x+P)=-f(x)$.

\subsubsection{Modulo}
mod${\left(x,p\right)}$ is the remainder when $x$ is divided by $p$. The range is $[0,p)$. mod${\left(x,p\right)}$ is $p$-periodic.

\subsection{Trigonometric Identities}
\subsubsection{The Pythagorean Identity} \label{pythagorean}
$$\sin^2{\theta}+\cos^2{\theta}=1$$
\begin{align*}
    1+\tan^2{\theta}=\sec^2{\theta}&&
    1+\cot^2{\theta}=\csc^2{\theta}
\end{align*}

\subsubsection{Periodicity}
\begin{align*}
    \sin{\left(\theta+2\pi k\right)} = \sin{\theta} &&
    \cos{\left(\theta+2\pi k\right)} = \cos{\theta}
\end{align*}
\subsubsection{Even/Odd}
\begin{align*}
    \sin{\left(-\theta\right)} = -\sin{\theta}&&
    \cos{\left(-\theta\right)} = \cos{\theta}
\end{align*}
\subsubsection{Phase Shift}
\begin{align*}
    \sin{\left(\frac{\pi}{2} + \theta\right)} = \cos{\theta} &&
    \cos{\left(\frac{\pi}{2} + \theta\right)} = -\sin{\theta}\\
    \sin{\left(\pi + \theta\right)} = -\sin{\theta} &&
    \cos{\left(\pi + \theta\right)} = -\cos{\theta}\\
    \sin{\left(\pi - \theta\right)} = \sin{\theta} &&
    \cos{\left(\pi - \theta\right)} = -\cos{\theta}\\
\end{align*}
The latter two are referred to as the Supplement Identities.

\subsubsection{Angle Sum and Difference Formulas} "sine cosine cosine sine, cosine cosine sine sine!"
\begin{align*}
    \sin{\left(\alpha+\beta\right)} &= \sin{\alpha}\cos{\beta}+\cos{\alpha}\sin{\beta}\\
    \cos{\left(\alpha+\beta\right)} &= \cos{\alpha}\cos{\beta}-\sin{\alpha}\sin{\beta}\\
    \sin{\left(\alpha-\beta\right)} &= \sin{\alpha}\cos{\beta}-\cos{\alpha}\sin{\beta}\\
    \cos{\left(\alpha-\beta\right)} &= \cos{\alpha}\cos{\beta}+\sin{\alpha}\sin{\beta}
\end{align*}
\paragraph{Double Angle Formulas} immediately result from the Angle Sum Formula.
\begin{align*}
    \sin{\left(2\alpha\right)} &= 2\sin{\alpha}\cos{\alpha}\\
    \cos{\left(2\alpha\right)} &= \cos^2{\alpha}-\sin^2{\alpha}\\
                                &= 2\cos^2{\alpha}-1\\
                                &= 1-2\sin^2{\alpha}
\end{align*}

\subsubsection{Product to Sum Formulas}
\begin{align*}
    \cos{\alpha}\cos{\beta}&=\frac{1}{2}\left(\cos\left(\alpha-\beta\right)+\cos{\left(\alpha+\beta\right)}\right)\\
    \sin{\alpha}\sin{\beta}&=\frac{1}{2}\left(\cos\left(\alpha-\beta\right)-\cos{\left(\alpha+\beta\right)}\right)\\
    \sin{\alpha}\cos{\beta}&=\frac{1}{2}\left(\sin\left(\alpha-\beta\right)+\sin{\left(\alpha+\beta\right)}\right)\\
\end{align*}

\subsubsection{Sum to Product Formulas}
\begin{align*}
    \sin{\alpha}+\sin{\beta}&=2\sin{\left(\frac{\alpha+\beta}{2}\right)}\cos{\left(\frac{\alpha-\beta}{2}\right)}\\
    \sin{\alpha}-\sin{\beta}&=2\sin{\left(\frac{\alpha-\beta}{2}\right)}\cos{\left(\frac{\alpha+\beta}{2}\right)}\\
    \cos{\alpha}+\cos{\beta}&=2\cos{\left(\frac{\alpha+\beta}{2}\right)}\cos{\left(\frac{\alpha-\beta}{2}\right)}\\
    \cos{\alpha}-\cos{\beta}&=-2\sin{\left(\frac{\alpha+\beta}{2}\right)}\sin{\left(\frac{\alpha-\beta}{2}\right)}\\
\end{align*}

\subsection{Inverse Trig Functions}
\subsubsection{Arc-sine}
$$\arcsin{x}=\sin^{-1}x\ne\left(\sin{x}\right)^{-1}=\frac{1}{\sin{x}}$$
In order to make $\sin{x}$ "invertible", we restrict the domain to one of infinitely many valid intervals. By convention, we restrict the domain to $\left[-\frac{\pi}{2},\frac{\pi}{2}\right]$. Thus, the domain of $\arcsin{x}$ is $[-1,1]$, and its range is $\left[-\frac{\pi}{2},\frac{\pi}{2}\right]$.

$$\frac{d}{dx}\left[\arcsin{x}\right]=\frac{1}{\sqrt{1-x^2}}$$

\subsubsection{Arc-cosine}
The domain of $\arccos{x}$ is $[-1,1]$, and its range is from $[0, \pi]$.

$$\frac{d}{dx}\left[\arccos{x}\right]=\frac{-1}{\sqrt{1-x^2}}$$

\subsubsection{Arc-tangent}
The domain of $\arctan{x}$ is $\mathbb{R}$, and its range is from $[-\frac{\pi}{2},\frac{\pi}{2}]$.

$$\frac{d}{dx}\left[\arctan{x}\right]=\frac{1}{x^2+1}$$

\subsubsection{Arc-secant}
The domain of $\arcsec{x}$ is $\left\{x\in\mathbb{R}|x\notin(-1,1)\right\}$, and its range is from $\left\{y\in[0,\pi]|y\ne\frac{\pi}{2}\right\}$.

$$\frac{d}{dx}\left[\arcsec{x}\right]=\frac{1}{|x|\sqrt{x^2-1}}$$

\subsection{Trigonometric Integrals}
\begin{align*}
    \int \sin^n{x}\cos^m{x}dx && \int \sec^n{x}\tan^m{x}dx
\end{align*}
To solve integrals of products of trigonometric functions:
\begin{enumerate}
    \item Set aside a $\sin{x}$/$\cos{x}$/$\tan{x}$/$\sec{x}\tan{x}$ for u-Substitution
    \item Use the Pythagorean Identities
    \item Use the Double Angle Formulas
\end{enumerate}

\subsubsection{Anti-Derivatives}
\begin{itemize}
    \item $\int\sin{x}\,dx=-\cos{x}+C$
    \item $\int\cos{x}\,dx=\sin{x}+C$
    \item $\int\tan{x}\,dx=\int\frac{\sin{x}}{\cos{x}}dx=-\ln{|\cos{x}|}+C$
    \item $\int\sec{x}\,dx=\int\frac{1}{\cos{x}}dx=\ln{|\sec{x}+\tan{x}|}+C$
    \item $\int\sec^2{x}\,dx=\tan{x}+C$
    \item $\int\sec{x}\tan{x}\,dx=\sec{x}+C$
\end{itemize}

\subsubsection{Inverse Trigonometric Integrals}
\begin{itemize}
    \item $\int\frac{1}{x\sqrt{x^2-a^2}}\,dx=\frac{1}{a}\arcsec{\frac{|x|}{a}}+C$
    \item $\int\frac{1}{\sqrt{a^2-x^2}}\,dx=\frac{1}{a}\arcsin{\frac{x}{a}}+C$
    \item $\int\frac{1}{a^2+x^2}\,dx=\frac{1}{a}\arctan{\frac{x}{a}}+C$
\end{itemize}

\subsubsection{Trigonometric Substitution} \label{trig-sub}
For integrals with the form $$\int \sqrt{1-x^2}$$ it may be helpful to substitute for $x=\sin{\theta}$ and use \ref{pythagorean}.

Similarly, for integrals with the form $$\int \sqrt{1+x^2}$$ it may be helpful to use $x=\tan{\theta}$.
\subsubsection{Fourier Series}
Every bounded periodic functions can be written as a sum of sines and cosines of different frequency, perhaps infinitely many of them.\\
We assume that $$f(x)=a_0+\sum_{n=1}^{\infty}\left(a_n\cos{nx}+b_n\sin{nx}\right)$$
Notice that $f(x)$ is $2\pi$-periodic.

\begin{align*}
    a_0&=\frac{1}{2\pi}\int_{-\pi}^\pi f(x)\,dx\\
    a_n&=\frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos{nx}\,dx\\
    b_n&=\frac{1}{\pi}\int_{-\pi}^\pi f(x)\sin{nx}\,dx
\end{align*}


\paragraph{Important Fact}
For $n,m\in\mathbb{Z}$
$$\int_{-\pi}^\pi \sin{\left(nx\right)}\sin{\left(mx\right)}\,dx=\begin{cases}
    0&n\ne m\\
    \pi&n=m
\end{cases}$$

\paragraph{Proof for $\sin{nx}$ terms}
Based on the Important Fact, we can tell that
$$\int_{-\pi}^\pi f(x)\sin{nx}\,dx=\int_{-\pi}^\pi b_n \sin^2{\left(nx\right)}\,dx$$ because all other terms are $0$. And so,
$$\frac{\int_{-\pi}^\pi f(x)\sin{nx}\,dx}{\int_{-\pi}^\pi\sin^2{\left(nx\right)}}= b_n$$ and because $\int_{-\pi}^\pi\sin^2{\left(nx\right)}=\pi$,

General Fourier Series require the use of $\cos{nx}$ in similar manners, as the series of $\sin{nx}$ can only encode \emph{odd} functions.

\paragraph{Convergence}
If $f(x)$ is periodic with period $2\pi$ and $f(x)$ and $f'(x)$ are piecewise continuous on $[-\pi,\pi]$, then the Fourier Series converges, $f(x)$ exactly equals the Fourier Series for all $x$ where $f(x)$ is continuous. When $f(x)$ is discontinuous, the Fourier Series is equal to the average of the right and left-hand limits, $\frac{1}{2}\left[f(x^+)+f(x^-)\right]$.

\section{Appendix}
\subsection{Notation}
\subsubsection{\texorpdfstring{The Universal Quantifier $\forall$}{The Universal Quantifier}}
This symbol stands for "for all ..."

For instance, $\forall x$ means "for all x", the following statement is true.

To describe that a function is odd, one can write $\forall x \left(f\left(-x\right) = -f\left(x\right)\right)$.

\subsubsection{\texorpdfstring{The Existential Qualifier $\exists$}{The Existential Qualifier}}
This symbol stands for "there exists ..."

For instance $\exists x$ means that there exists (at least one value of) $x$ such that the following statement is true.

\subsubsection{Sets}
$\left\{\right\}$ represent set builder notation.

$\in$ is the "in" symbol.

$|$ represents "such that".

$\cup$ represents a union of sets (inclusive "or").\\
Example: $\left\{x \in \mathbb{R} | 1 \le x \le 5 \right\}$ and $\left\{\theta \in \mathbb{R} | 1 \le \theta \le 5 \right\}$ represent the same set, but with different labels.

$\subset$ and $\subseteq$ are the subset (or equal to) symbols.

\subsection{Additional Functions}
\subsubsection{ceil(x)}
The ceiling function returns the smallest integer ($y\in\mathbb{Z}$) such that $y \ge x$.\\
View a graph on \href{https://www.desmos.com/calculator/cpay9r9g5w}{Desmos}.

\subsubsection{sgn(x)}
The sign function. Defined as
sgn$(x) = \frac{x}{|x|}$ or sgn$(x) = \begin{cases}
    -1 &x<0\\
    +1 &x>0
\end{cases}$\\
A similar function, the pulse function, defines pulse$(0)=0$.

\subsubsection{The Dirichlet Function}
This is a pathological function constructed to be nowhere continuous.

D$(x) = \begin{cases}
    1 & x\in\mathbb{Q}\\
    0 & x\notin\mathbb{Q}
\end{cases}$

\subsubsection{Hyperbolic Trig}
"sinch"
$$\sinh{x}=\frac{e^x-e^{-x}}{2}$$\\
"kosh"
$$\cosh{x}=\frac{e^x+e^{-x}}{2}$$

These can be considered the "odd" and "even" portions of $f(x)=e^x$.

\subsection{Circle Facts}
\begin{itemize}
    \item Subtends is a word for the relationship between a central angle and its arc.
    \item All circles are similar.
    \item To find the arclength $s$ of a circle's arc from the central angle measure $\theta$ and the circle's radius $r$: $s=\theta r$.
    \item The sector area of a circle is $A=\frac{1}{2}r^2\theta$.
    \item The Isoperimetric Theorem shows that the most area enclosed by a set perimeter will be in a circle. In essence, the maximization of symmetry maximizes efficiency.
\end{itemize}

\subsection{Additional Area Formulae}
\begin{itemize}
    \item Triangle with angle C and sides a, b -- $A=\frac{1}{2}ab\sin{C}$
    \item Rhombus with angle C and side length a -- $A=a^2\sin{C}$
\end{itemize}

\subsection{Important Inequalities}
\subsubsection{The General Triangle Inequality}
$$|x+y| \le |x| + |y|$$
\subsubsection{The AM-GM Inequality}
$\forall a,b \ge 0$:
$$\frac{a+b}{2}\ge\sqrt{ab}$$

The arithmetic mean-geometric mean inequality is proven by considering $(a-b)^2\ge0$.

\end{document}
