\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx} % Required for inserting images
\usepackage[hidelinks]{hyperref}
\usepackage{pgfplots}
\pgfplotsset{width=3in,compat=1.9}


\DeclareMathOperator{\sgn}{sgn}

\DeclareMathOperator{\comp}{comp}
\DeclareMathOperator{\proj}{proj}

\newcommand{\vect}[1]{\ensuremath{\overrightarrow{#1}}}
\newcommand{\magnitude}[1]{\ensuremath{\lVert #1 \rVert}}
\newcommand{\magvect}[1]{\magnitude{\vect{#1}}}
\newcommand{\abs}[1]{\left|#1\right|}

\renewcommand{\labelitemii}{\textopenbullet}

\title{Calculus! Calculus!}
\author{David Chen}
\date{September 2024 -- June 2025}

\begin{document}

\maketitle

\section{Multivariable Functions and Space}

\subsection{Functions}
$$f: \mathbb{R}^n \to \mathbb{R}^m$$
Functions map a \textbf{domain} (e.g. $\mathbb{R}^n$, read ``R N") to a \textbf{codomain} (e.g. $\mathbb{R}^m$ -- see appendix \ref{set-multiplication} for more information). In the scope of this course, we will usually have $n$ or $m = 1$, and $n,m \in \mathbb{N}$.\footnote{In generalized multivariable calculus, more profound matrix transformations are used.}
Common function domain/codomain pairings we consider are
\begin{itemize}
    \item $f: \mathbb{R}^2 \to \mathbb{R}$ -- can be represented as mapping points on the 2D plane to label values or as a 3D \textbf{surface}.
    \item $f: \mathbb{R} \to \mathbb{R}^2$ -- can be represented as a parametric \textbf{curve}, with 1 time dimension and 2 space dimensions.
    \item $f: \mathbb{R}^3 \to \mathbb{R}$ -- can be represented as labelling each point in 3-space with a value (e.g. temperature).
    \item $f: \mathbb{R}^2 \to \mathbb{R}^2$ -- can be considered a transformation in 2-space.
\end{itemize}

\subsection{Representing N-space on Cartesian Axes}
A figure in N-space represents N values (from both the domain and codomain) per point. For instance, a number line represents 1-space, and the xy-plane represents 2-space.

\subsubsection{Axes}
For 3-space, we conventionally use 3 mutually perpendicular axes -- $x, y, z$, meeting at an origin $O(0,0,0)$ -- to represent the three dimensions. By convention, these axes follow the ``right-hand rule", with the alternative ``left-hand rule" leading to certain negations in formulae.
This model is similar to the axes in 2-space, where the RHR does not make a difference, but is harder to visualize in 4-space.\footnote{Most of us cannot visualize 4-space with 4 orthogonal axes. Please talk to your doctor if this is not the case...}

\paragraph{Sections}
There are 4 quadrants in 2-space, and 8 octants in 3-space.

\subsubsection{Line Formulae}
These familiar formulae for 2D lines extend to 3D in intuitive ways.

\paragraph{Distance}
The 2D distance formula
$$d = \sqrt{(\Delta x)^2 + (\Delta y)^2}$$
generalizes readily into the 3D distance formula
$$d = \sqrt{(\Delta x)^2 + (\Delta y)^2 + (\Delta z)^2}$$
based on the Pythagorean Theorem.

\paragraph{Midpoint}
The formula for a midpoint on a 2D line between $(x_1, y_1)$ and $(x_2, y_2)$
$$M\left(\frac{x_1+x_2}{2},\frac{y_1+y_2}{2}\right)$$
generalizes readily into the 3D midpoint formula for points $(x_1, y_1, z_2)$ and $(x_2, y_2, z_2)$
$$M\left(\frac{x_1+x_2}{2},\frac{y_1+y_2}{2}, \frac{z_1+z_2}{2}\right)$$
Alternative coordinate systems are possible, and will be discussed in a future section.

\paragraph{Partitioning}
The point P partitioning a line segment AB determined by points $A(x_1,y_1)$ and $B(x_2,y_2)$ into segments with lengths in the ratio $a:b$ has coordinates
$$P\left(x_1+\frac{a}{a+b}\left(x_2-x_1\right), y_1+\frac{a}{a+b}\left(y_2-y_1\right)\right) = P\left(\frac{ax_2 + bx_1}{a+b}, \frac{ay_2 + by_1}{a+b}\right)$$
Similarly, the point P partitioning AB in 3-space between $A(x_1, y_1, z_1)$ and $B(x_2, y_2, z_2)$ into segments with lengths in the ratio $a:b$ has coordinates
$$P\left(\frac{ax_2 + bx_1}{a+b}, \frac{ay_2 + by_1}{a+b}, \frac{az_2 + bz_1}{a+b}\right)$$

\subsubsection{Contours}
Contours allow for the construction of a $N$-dimensional representation/sampling of a $N+1$-dimensional function. For instance, contour surfaces are 3D samples of a 4D space.
We use contour curves\footnote{Although the textbook uses the term level curve interchangably it may be helpful to depict the ``level" as the $z$-value in 3-space.} to represent a 3D surface with points $(x,y,z)$ by projecting the curves produced for a fixed $z=c$ onto the $xy$-plane.
\iffalse
Below is a (low resolution) contour plot of the top half of a sphere:
\begin{center}
\begin{tikzpicture}
\begin{axis}
[
    title={Contour plot of $x^2 + y^2 + z^2 = 1$},
    view={0}{90}
]
\addplot3[
    contour gnuplot={levels={1, 0.8, 0.6, 0.4, 0.2, 0}}
]
{sqrt(x^2+y^2)};
\end{axis}
\end{tikzpicture}
\end{center}
\fi
It is useful to have constant $\Delta z$ intervals so that the contours can visually represent the steepness of the surface.

\section{Notable 2D Figures}

\subsection{Lines}

\subsubsection{Standard Form} The set of points satisfying a linear equation of the form $$Ax + By + C = 0$$
Although there are 3 nominal coefficients, there are only 2 effective coefficients because (assuming at least one of $A,B$ is non-zero), we can divide by a coefficient, thus leaving 2 unknown ratios as coefficients. This explains why 2 points are sufficient to uniquely define a line, and distinct lines can only intersect at 1 point.

\subsubsection{Additional Equations}
\begin{itemize}
    \item Point-Slope: $y-y_1 = m(x-x_1)$
    \item Slope-Intercept: $y=mx+b$
    \item Slope Form: $m=\frac{y-y_1}{x-x_1}$
    \item Intercept Form: $\frac{x}{a} + \frac{y}{b} = 1$, axis intercepts at $(a, 0)$ and $(0, b)$
    \item Polar Form: $A\left(r\cos{\theta}\right)+B\left(r\sin{\theta}\right) + C = 0$
    \item Normal Form: $x\cos{\alpha} + y\cos{\beta} = d$, d is the length of the normal to the origin, $\alpha$ is the angle of the normal to the x-axis, $\beta$ is the angle of the normal to the y-axis
    \item Determinant Form: $\begin{vmatrix}
x & y & 1\\
x_1 & y_1 & 1 \\
x_2 & y_2 & 1
\end{vmatrix}= 0$
\end{itemize}

\subsubsection{Distance from a Point} \label{pt-dist-2-space}
The distance to a line $ax+by+c=0$ from point $(x_1, y_1)$ is
$$d=\frac{\left|ax_1 + by_1 + c\right|}{\sqrt{a^2+b^2}}$$

\subsubsection{Area of a Triangle in 2-space}
Given points $(x_1, y_1), (x_2, y_2), (x_3, y_3)$, the area of the triangle formed by these points is
$$A = \frac{1}{2}\begin{vmatrix}
    x_1 & y_1 & 1 \\
    x_2 & y_2 & 1 \\
    x_3 & y_3 & 1 \\
\end{vmatrix} = \frac{1}{2}\left|(x_1-x_2)(y_2-y_3) - (x_2 - x_3)(y_1 - y_2)\right|$$
The latter form can be derived using the distance from a point formula. See section \ref{determinant} for more information about the former form.
This is also known as the shoelace formula, because it can be represented as taking the points in order (clockwise/counterclockwise) and ``lacing" the coordinates to form equivalent products.

\subsubsection{Dimensional Analysis of the Determinant Method}
In the case of the determinant representing a triangle's area, we can translate and rotate the triangle -- since these are rigid motions, area is preserved -- making a new figure with coordinates that form a matrix with the following determinant
$$\begin{vmatrix}
    0 & 0 & 1 \\
    x_1' & 0 & 1 \\
    x_2' & y_2' & 1 \\
\end{vmatrix} = x_1'y_2'$$
where $x_1'$ represents the base length (rotated onto the x-axis), and $y_2'$ represents the height (rotated to be parallel to the y-axis). This determinant represents area (square distance units).


\subsubsection{Projection}
The projection of a line segment with slope $m$ and length $l$ onto the $x$-axis is
$$\Delta x = l\sqrt{\frac{1}{1+m^2}}$$

\subsubsection{Angle Between Lines}
For two intersecting lines with slopes $m_1$ and $m_2$, $m_1 < m_2$ the angle between the two can be found by $$\arctan{m_2} - \arctan{m_1} = \theta$$

\subsection{Conic Sections}

\paragraph{General Equation} The set of points satisfying a degree 2 equation of the form $$Ax^2+By^2 + Cxy + Dx + Ey + F =0$$
There are 6 nominal coefficients and 5 effective coefficients. This explains why 5 points are sufficient to uniquely define a conic section, and distinct conic sections can intersect at a maximum of 4 points.

\subsection{Cubics}

\paragraph{Standard Form} The set of points satisfying a degree 3 equation of the form $$Ax^3 + By^3 + Cx^2y + Dxy^2 + Ex^2 + Fy^2 + Gxy + Hx + Jy + K = 0$$
There are 10 nominal coefficients and 9 effective coefficients. However, distinct conics can intersect at more than 8 points. This is known as the Euler-Cramer Paradox.

\subsubsection{The Euler-Cramer Paradox}
For polynomials of degree $\ge 3$, Because the system of intersection points may contain rows that are \textbf{not} linearly independent, it is possible that that the determinant of the matrix would be $0$: there is not a unique solution curve for the system. See section \ref{determinant} for more information.

\section{Determinants} \label{determinant}
A \textbf{matrix} is a rectangular array of numbers.\footnote{Tensors can be considered a generalization of matrices to higher dimensions.} The dimensions are denoted $N \times M$, with $N$ being the number of rows, and $M$ being the number of columns. By convention, matrix variables are denoted with capital letters. Matrix elements can be addressed with subscripts $a_{n,m}$, with the comma delimiter being optional.
$$A = \begin{bmatrix}
 a_{11} & a_{12} & a_{13} & \cdots & a_{1m} \\
 a_{21} & a_{22} & a_{23} & \cdots & a_{2m} \\
 \vdots & \vdots & \vdots & \vdots & \vdots \\
 a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nm} \\
\end{bmatrix}$$
A \textbf{determinant} is a function that maps a real number to every \textbf{square} matrix.
The determinant of a $1\times1$ matrix $\left[a\right]$ is the scalar $a$, and the determinant of a $2\times2$ matrix $\left[\begin{smallmatrix} a & b \\ c & d \end{smallmatrix}\right]$ is $ad - bc$.

\subsection{Recursive (Cofactor) Definition}
Given an element $a_{ij}$ of matrix $A$, the corresponding minor $M_{ij}$ is the determinant obtained by deleting row $i$ and column $j$ of matrix A.
The cofactor (co-factor) of $a_{ij}$ is $$C_{ij} = (-1)^{i+j}M_{ij}$$
The determinant of matrix A is $$\det{A} = \sum_{j=1}^n a_{ij}C_{ij} = \sum_{i=1}^m a_{ij}C_{ij}$$ given a fixed value for $i$ (selecting a row) or $j$ (selecting a column) respectively.
To perform this by hand, one can take an arbitrary row/column\footnote{It is usually helpful to select a row/column with values of $0$ or $1$ to minimize needed calculations.} and then take the minors of each element in that row/column, multiplying against the sign of the cofactor. Note that the sign of the cofactor alternates in a pattern like so:
$$\begin{bmatrix}
    + & - & + & - & \cdots \\
    - & + & - & + & \cdots \\
    + & - & + & - & \cdots \\
    \vdots & \vdots & \vdots & \vdots & \ddots \\
\end{bmatrix}$$

\subsection{Explicit Definition}
This definition is listed as a fun aside; it is useful for proving determinant properties for the general case.
For the set of all permutations $S_n$, with the signature function $\sgn(\sigma)$ denoting $(-1)^n$, with $n$ being the number of transpositions (element swaps) from the original ordering of elements to get the permutation $\sigma(i)$:
$$\det{A} = \sum_{\sigma \in S_n} \sgn{\sigma} \prod_{i=1}^n a_{i,\sigma(i)}$$

\subsection{Determinant Properties}
Note that many of these properties do not have standard names.
The sum property (\ref{det-sum}), combined with the switching property (\ref{det-switch}), are sometimes referred to as the \textbf{multilinearity} of determinants in columns/rows.

\subsubsection{Scalar Multiple Property}
If any row or column is multiplied by a constant $k$, the determinant value gets multiplied by $k$.

\paragraph{Proof} Consider the recursive determinant sum for the new matrix, using the row/column that was multiplied. $k$ can be factored out of the entire sum, leaving the original determinant sum multiplied by the scalar $k$.

\subsubsection{Sum Property} \label{det-sum}
If all the elements of a row or column are expressed as a summation of two or more addends, then the determinant can be broken down as a sum of corresponding smaller elements.
$$
    \begin{vmatrix}
        a_{11} + b_{11} & a_{12} & a_{13} \\
        a_{21} + b_{21} & a_{22} & a_{23} \\
        a_{31} + b_{31} & a_{32} & a_{33} \\
    \end{vmatrix}
    =
    \begin{vmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33} \\
    \end{vmatrix}
    +
    \begin{vmatrix}
        b_{11} & a_{12} & a_{13} \\
        b_{21} & a_{22} & a_{23} \\
        b_{31} & a_{32} & a_{33} \\
    \end{vmatrix}
$$
This property can be applied multiple times to separate rows.

\paragraph{Proof} Consider the explicit definition of the determinant. Choosing $i$ or $j$ to target the addends' row/column, we can use the distributive property to split the product into two, which represent the determinant of the separate matrices.

\subsubsection{Switching Property} \label{det-switch}
If we interchange any two rows/columns of the determinant, the determinant is multiplied by $-1$.

\subsubsection{Sum of Rows Property} \label{det-sum-rows}
Let $A$ be an $n\times n$ matrix and let B be a matrix which results from adding a multiple of a row/column to another row/column. Then $\det{A} = \det{B}$.

\subsubsection{Zero Property}\label{det-zero}
The determinant of a matrix with a row/column of zeroes is $0$.

\paragraph{Proof} Consider the recursive definition of the determinant. Choosing the row with $0$s will lead to $a_{ij}$ always being $0$, so the final sum is 0.
This property allows us to show that since the determinant form of a line results in an expression of the form $Ax+By+C=0$, and that the two points $(x_1,y_1)$ and $(x_2,y_2)$ satisfy this equation, the two points definitively show that the solutions to the equation form the line.

\subsubsection{Identical Column Zero Property}
If a (square) matrix has two identical rows/columns (or rows that are a linear combination of others), then its determinant is zero.

\paragraph{Proof} Using the sum of rows property (\ref{det-sum-rows}), we can manipulate the matrix such that the row is only $0$s, thus allowing us to apply the zero property (\ref{det-zero}).\footnote{Although not mentioned in class, the determinant of a transposed matrix is equal to the original matrix, making this applicable to columns too.}

\section{3D Figures}
To draw a 3D figure, it is helpful to consider its level curves and traces (cross-sections in planes such as the $xz$ or $yz$ plane) by setting one of the variables $=0$ or $=c$.

\subsection{Planes}
\subsubsection{Drawing a Plane}
Because planes extend infinitely, we draw a quadrilateral or triangular slice of a plane in 3-space to represent it.

\subsubsection{Special Cases}
A single variable equal to a constant represents a plane parallel to the other axes' plane. For instance, the $xz$-plane can be represented with $y=0$, and $y=c$ produces planes parallel to the $xz$-plane.

A linear equation in 2 variables (e.g. $y=mx+b$) produces a plane in 3-space that intersects the equivalent line on its corresponding plane (e.g. the $xy$-plane where $z=0$) and serves as a cylinder with rulings parallel to the third axis.

\subsubsection{General Linear Functions}
$$ax+by+cz=d$$
The contours of a general linear function are evenly spaced, parallel lines. (Proof: Consider $z=c$. A series of lines linearly dependent on $c$ is produced.)
The constant term $d$ corresponds with the z-intercept ($0$, $0$, $\frac{d}{c}$).

\subsubsection{Point-Slope-Slope}
Given the slopes $m_x=\frac{\Delta{z}}{\Delta{x}}$ and $m_x=\frac{\Delta{z}}{\Delta{y}}$ and a point ($x_1$, $y_1$, $z_1$) on the plane:
$$z-z_1 = m_x(x-x_1) + m_y(y-y_1)$$
$m_x$ and $m_y$ are most easily calculated when you have lines on the $xz$ and $yz$ planes respectively, or pairs of points on parallel points (i.e. ($x_1$, $c$, $z_1$) and ($x_2$, $c$, $z_2$) for $m_x$). Note that point-slope-slope form can be rearranged into standard form. $m_x$ and $m_y$ can also be solved for from standard form by isolating $z$ and dividing by $-c$.

\subsubsection{Intercept Form}
For a plane intersecting the x-axis at $(a,0,0)$, the y-axis at $(0,b,0)$, and the z-axis at $(0,0,c)$, the equation for the plane can be defined as:
$$\frac{x}{a}+\frac{y}{b}+\frac{z}{c}=1$$
Intercept form is useful for providing the three non-collinear points that can be used to draw a plane.

\subsubsection{``Slope" on a Plane}
For an arbitrary plane defined by a line with angle $\theta$ to the $x$-axis in the $xy$-plane, the slope $m_\theta$ of the intersection line between this vertical plane and a plane with slopes $m_x$ and $m_y$ is
$$m_\theta = m_x\cos{\theta} + m_y\sin{\theta}= m_x\frac{\Delta x}{\Delta t} + m_y\frac{\Delta y}{\Delta t}$$ as shown by the right triangle (with angle $\theta$) formed by the line, the $x$-axis, and the $y$-axis.

An alternative form for $m_\theta=\frac{\Delta z}{\Delta l}$, with $l$ being the distance along the line defined by $\theta$, is $$m_\theta = \frac{m_x+m_y\tan{\theta}}{\sqrt{1+\tan^2{\theta}}}$$

\subsubsection{Distance from a Point}
The distance to a plane $ax+by+cz+d=0$ from point $(x_1, y_1, z_1)$ in 3-space is similar to that of a point to a line in 2-space (\ref{pt-dist-2-space}).
\begin{align*}
d=\frac{\abs{ax_1+by_1+cz_1+d}}{\sqrt{a^2+b^2+c^2}} =
\comp_{\langle a, b, c \rangle} \vect{D} = \frac{\langle a, b, c \rangle \cdot \vect{D}}{\magnitude{\langle a, b, c \rangle}}
\end{align*}

This can be derived by taking the parallel component of the difference/distance vector $\vect{D} = \langle x-x_1, y-y_1, z-z_1\rangle$ between a point $(x,y,z)$ on the plane and the point $(x_1, y_1, z_1)$ and the perpendicular to the plane $\langle a, b, c \rangle$.

\paragraph{Distance between Planes}
The distance between planes $ax + by + cz + d = 0$ and $ax + by + cz + e = 0$ can be derived from the distance between the first plane and a point $(x_0, y_0, z_0)$ on the second plane.
\[
D=\frac{\abs{d-e}}{\sqrt{a^2+b^2+c^2}}
\]

\subsection{Cylinders}
Cylinders with rulings (straight lines) parallel to an axis are represented with equations in 2 variables for 3-space.
For instance, $1 = x^2 + y^2$ represents a circular cylinder, with rulings parallel to the omitted $z$-axis.
Other equations can produce non-circular cylinders.

\subsection{Quadric Surfaces}
Quadric surfaces are second-degree polynomials in 3-space. Equations are given for the surfaces on the $z$ axis, but these variables can be interchanged to produce surfaces along the $x$ or $y$ axes.

\subsubsection{Ellipsoids}
An ellipsoid has traces that are all ellipses. In analogy to 2D ellipses, the variables $a$, $b$, $c$ represents half the width of the ellipse along that axis.
$$\frac{x^2}{a^2} + \frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$

\paragraph{Spheres} are special cases of the ellipsoid (in which $a=b=c=r$), and they can also be defined as the set of points equidistant from a center, based on the 3D distance formula. A sphere of radius $r$ centered at point $(h, k, l)$ has equation
$$(x-h)^2+(y-k)^2+(z-l)^2=r^2$$

\subsubsection{Cones}
Cones have elliptical level curves, and hyperbolic vertical traces (or in the degenerate case at the origin, two crossing lines similar to the absolute value function).
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} = \frac{z^2}{c^2}$$
Notice that if instead of $z^2$, one uses $z=\sqrt{\ldots}$, a ``single" cone appears.

\subsubsection{Elliptic Paraboloid}
Elliptic paraboloids have parabolic vertical cross sections, and elliptical horizontal contours. Wow!
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} = \frac{z}{c}$$

\subsubsection{Hyperbolic Paraboloid}
Hyperbolic paraboloids have parabolic vertical cross sections, and elliptical horizontal contours. Also wow!
$$\frac{x^2}{a^2} - \frac{y^2}{b^2} = \frac{z}{c}$$
The hyperbolic paraboloid is also a ruled surface -- it is possible to run a series of straight lines across it.

\subsubsection{Hyperboloid of One Sheet}
Horizontal traces are ellipses, and vertical traces are hyperbolas (degenerate at the plane $y=b$ or $x=a$).
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2}=1$$

\subsubsection{Hyperboloid of Two Sheets}
Horizontal traces are ellipses, and vertical traces are hyperbolas. The textbook likes to say that 2 minus signs correlate with 2 sheets.
$$\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2}= -1$$
$$-\frac{x^2}{a^2} - \frac{y^2}{b^2} + \frac{z^2}{c^2}= 1$$

\section{Vectors}

Vectors possess both magnitude and direction. They can be represented as arrows (directed line segments). They possess an initial point (tail) and a terminal point (arrow-head).
There is a one-to-one correspondence between points and (position) vectors.

\subsection{Notation and Terminology}
Vectors are usually represented with bolded or lowercase variables $\Vec{a}$, with the arrow head pointing towards the terminal point (e.g. $\overrightarrow{PQ}$). Examples in this section will be in 2-space, but can be generalized to 3-space.

Free vectors are vectors with initial points "freely roaming" about space. Position vectors are vectors with initial points on the original of space. These vectors are equivalent (see below).

The magnitude of a vector is denoted $\left|\Vec{a}\right|$ or $\left|\left|\Vec{a}\right|\right|$. This symbol is not the absolute value operator when vectors are involved.

A unit vector is a vector with a magnitude of $1$. Unit vectors are represented with a hat, such as the ortho-normal basis vectors\footnote{ortho(gonal -- perpendicular; normal -- magnitude 1; basis vectors -- vectors that can be linearly combined (i.e. added and scaled) to represent all of space} $\hat{i} = \langle1, 0$, $0\rangle$, $\hat{j} = \langle 0, 1, 0 \rangle$, $\hat{k} = \langle 0, 0, 1 \rangle$ pointing in the $x$, $y$, and $z$ directions.

The coordinates of the terminal point of a position vector are called the components of the vector $\left\langle a_1, a_2 \right\rangle$.

\subsection{Properties of Vectors}
\begin{itemize}
    \item $\Vec{a}=\Vec{b}$ if they have the same magnitude and direction. Thus, a vector is the same regardless of what specific points it originates/ends at.
    \item $-\overrightarrow{PQ} = \overrightarrow{QP}$, the negation of a vector reverses its direction.
    \item $\overrightarrow{0}$ is defined to have no direction and $0$ magnitude.
\end{itemize}

\subsection{Operations on Vectors}
\subsubsection{Addition and Subtraction}
Addition of vectors $\overrightarrow{a}$ and $\overrightarrow{b}$ can be defined as adding their components.
$$\langle a_1, a_2 \rangle + \langle b_1, b_2 \rangle = \langle a_1 + b_1, a_2 + b_2\rangle$$

Graphically, addition of two vectors can be represented as attaching the head of on vector to the tail of another, with their sum being the vector represented by the tail of the first vector and the head of the final vector. Alternatively, the parallelogram bounding the two vectors (with tails at the same point) contains the sum vector as its diagonal.

Subtraction of vectors is the addition of the negative vector.

\subsubsection{Magnitude}
The magnitude can be represented as the distance from the tail to the head. For a vector $\overrightarrow{a} = \langle a_1, a_2 \rangle$:
$$\left|\overrightarrow{a}\right| = \sqrt{a_1^2+a_2^2}$$

\paragraph{Unit Vector in the Direction of a Given Vector}
For an arbitrary vector $\overrightarrow{a}$:
$$\hat{a} = \frac{\overrightarrow{a}}{\left|\left|\overrightarrow{a}\right|\right|} = \left\langle \frac{a_1}{\left|\left|\overrightarrow{a}\right|\right|}, \frac{a_2}{\left|\left|\overrightarrow{a}\right|\right|}, \frac{a_3}{\left|\left|\overrightarrow{a}\right|\right|} \right\rangle$$
For a 2-space vector with direction described by angle $\alpha$ to the $x$-axis (and $\beta$ as its complement to the $y$-axis):
$$\hat{a} = \left\langle \cos{\alpha}, \sin{\alpha} \right\rangle = \left\langle \cos{\alpha}, \cos{\beta} \right\rangle$$
This can be generalized to 3-space by drawing perpendiculars from the terminal point of the vector to each of the axes in space, and taking the cosines of each obtained angle.
$$\hat{a} = \left\langle \cos{\alpha}, \cos{\beta}, \cos{\gamma} \right\rangle$$

\subsubsection{Scalar Multiplication}
Multiplying a vector by a scalar $k$ results in a vector in the same direction, but with the components scaled by $k$:
$$k\overrightarrow{a} = \langle ka_1, ka_2 \rangle$$
The magnitude of the new vector is $k\left|\left|\overrightarrow{a}\right|\right|$.

\section{Vector Products}
Vector "multiplication" encompasses multiple defined products between vectors.

\subsection{Dot Product}
The dot product ("inner product", "scalar product") of two vectors $\vect{a} = \langle a_1, a_2, a_3 \rangle$, $\vect{b} = \langle b_1, b_2, b_3 \rangle$, with an angle $\theta$ between them, is the scalar
$$\vect{a} \cdot \vect{b} = \magvect{a}\magvect{b}\cos{\theta} = a_1b_1 + a_2b_2 + a_3b_3$$
Using a geometric interpretation of the two vectors and their difference, one can use the law of cosines to prove the equivalence of the two definitions.

As will be shown in the next sections, it is useful to conceptualize sums of products as dot products.

\subsubsection{Projection}
The scalar projection of a vector \vect{a} onto \vect{b} is the "component" of \vect{a} on \vect{b}, delimited by a perpendicular to \vect{b} drawn from the tip of \vect{a}. With the angle between \vect{a} and \vect{b} being $\theta$, this is
$$\comp_{\vect{b}} \vect{a} = \magvect{a}\cos{\theta} = \frac{\vect{a} \cdot \vect{b}}{\magvect{b}}$$

The vector projection of a vector \vect{a} onto \vect{b} is the "projection" of \vect{a} on \vect{b} pointing in the same direction at \vect{b}.
$$\proj_{\vect{b}}\vect{a} = \left(\comp_{\vect{b}}\vect{a}\right)\hat{b} = \frac{\vect{a} \cdot \vect{b}}{\magvect{b}^2} \vect{b}$$

\subsubsection{Properties of the Dot Product}
\begin{enumerate}
    \item Commutative Property: $\vect{a} \cdot \vect{b} = \vect{b} \cdot \vect{a}$
    \item Distributive Property\footnote{Over addition/subtraction}: $c\left(\vect{a} \cdot \vect{b}\right) = c\vect{a} \cdot \vect{b} = \vect{a} \cdot c\vect{b}$
    \item Scalar Multiplication Property: $\vect{a} \cdot \left(\vect{b} + \vect{c}\right) = \vect{a} \cdot \vect{b} + \vect{a} \cdot \vect{c}$
    \item Zero Property: $\vect{a} \cdot \vect{0} = 0$
    \item ``Squaring" Theorem: $\vect{a} \cdot \vect{a} = \magvect{a}^2$
\end{enumerate}
Note that $\vect{0}$ is the zero vector, with $0$ magnitude and undefined direction.

\subsubsection{Theorem 1: The Cauchy-Schwartz Inequality}
$$\vect{a} \cdot \vect{b} \le \magvect{a}\magvect{b}$$

The Arithmetic Mean/Quadratic Mean (AM-QM) Inequality can be proven using a vector $\langle1, 1, 1, ...\rangle$:
$$\frac{a_1 + a_2 + \cdots + a_n}{n} \le \sqrt{\frac{a_1^2 + a_2^2 + \cdots + a_n^2}{n}}$$

\subsubsection{Theorem 2: Perpendicular Vectors}
$$\vect{a} \perp \vect{b} \iff \vect{a} \cdot \vect{b} = 0$$ for $\vect{a}, \vect{b} \ne 0$, such that $\magvect{a}, \magvect{b} \ne 0$, implying that for $\magvect{a}\magvect{b}\cos{\theta} = 0$, $\cos{\theta} = 0$. More generally, the solutions for $\theta$ indicate the angle between two vectors.

\subsubsection{Perpendiculars and Angles to Lines}
A line $Ax + By + C = 0$ is parallel to $Ax + By = 0$. This can be rewritten as the dot product $\langle A, B \rangle \cdot \langle x, y \rangle$, with $\langle x, y \rangle$ representing the set of vectors parallel to the line. From Theorem 2,
$$\langle A, B \rangle \perp Ax + By + C = 0$$
the vector $\langle A, B \rangle$, along with its family of scaled vectors, is perpendicular to the line.

One can also determine the angle between lines $A_1x + B_1y = C1$ and $A_2x + B_2y = C_2$ by taking the dot product between vectors parallel to the lines (or perpendicular vectors, i.e. $\langle A_1, B_1 \rangle \cdot \langle A_2, B_2 \rangle$).

\subsubsection{Perpendiculars to Planes}
The set of all vectors $\langle x, y, z \rangle$ perpendicular to a vector $\langle A, B, C \rangle$ in 3-space is a plane.
\begin{align*}
    \langle A, B, C \rangle \cdot \langle x, y, z \rangle &= 0 \\
    Ax + By + Cz &= 0
\end{align*}

To obtain the specific plane perpendicular to a vector passing through point $(x_0, y_0, z_0)$, one evaluates the dot product
\begin{align*}
    \langle A, B, C \rangle \cdot \langle x - x_0, y - y_0, z - z_0 \rangle &= 0 \\
    A\left(x - x_0\right) + B\left(y - y_0\right) + C\left(z - z_0\right) &= 0
\end{align*}

Note that a plane of the form $z = m_xx + m_yy$ can be rewritten as
\begin{align*}
    0 &= m_xx + m_yy - z \\
    0 &= \langle m_x, m_y, -1 \rangle \cdot \langle x, y, z \rangle
\end{align*}

\subsection{Cross Product}
The cross product ("vector product", "outer product") of $\vect{a} = \langle a_1, a_2, a_3 \rangle$ and $\vect{b} = \langle a_1, a_2, a_3 \rangle$ is the vector
$$\vect{a} \times \vect{b} = \left\langle a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1\right\rangle =
    \begin{vmatrix}
        \hat{i} & \hat{j} & \hat{k} \\
        a_1     & a_2     & a_3     \\
        b_1     & b_2     & b_3     \\
    \end{vmatrix}
$$
This vector is constructed such that $\vect{a} \times \vect{b} \perp \vect{a}$ and $\vect{a} \times \vect{b} \perp \vect{b}$, representing the family of vectors perpendicular to the perpendicular vectors of two planes, or the vector of the line of intersection between those planes. This third vector satisfies the right-hand rule.

\subsubsection{Properties of the Dot Product}
\begin{enumerate}
    \item Zero Property: $\vect{a} \times \vect{0} = \vect{0} = \vect{0} \times \vect{a}$
    \item $\vect{a} \times \vect{b} \perp \vect{a}$ and $\vect{a} \times \vect{b} \perp \vect{b}$
    \item Anti-Commutative Property: $\vect{a} \times \vect{b} = -\left(\vect{a} \times \vect{b}\right)$
    \item Distributive Property: $\vect{a} \times \left(\vect{b} + \vect{c}\right) = \vect{a} \times \vect{b} + \vect{a} \times \vect{c}$
\end{enumerate}
\subsubsection{Theorem 1: Magnitude of the Cross Product}\label{mag-cross-product}
The magnitude of the cross product is
$$\magnitude{\vect{a} \times \vect{b}} = \magvect{a}\magvect{b}\sin{\theta}$$

\paragraph{Lemma} Through algebraic expansion, it can be shown that $$\magnitude{\vect{a} \times \vect{b}}^2 = \magvect{a}^2\magvect{b}^2 - \left(\vect{a} \cdot \vect{b}\right)^2$$
Using the non-coordinate definition of the dot product, we can use the identity $1-\cos^2{x} = \sin^2{x}$ to prove the expression for the magnitude of a cross product where $0 \le \theta \le \pi$.

\paragraph{Corollaries}
The orthogonal component of $\vect{a}$ onto $\vect{b}$ is $$\frac{\magnitude{\vect{a}\times\vect{b}}}{\magvect{b}}$$
Using the orthogonal component as a height, it can be shown that cross product of two vectors also represents the area of the parallelogram bounded by the two vectors. (Half of this area is the triangle bounded by the two vectors.)

\subsubsection{Theorem 2: Parallel Vectors}
From \ref{mag-cross-product}, if $\vect{a}, \vect{b} \ne \vect{0}$, then $$\vect{a} \times \vect{b} = 0 \iff \vect{a} \parallel \vect{b}$$

\subsection{Lines in 3-space}
A line in 3-space can be defined as a vector-valued function ($\mathbb{R} \to \mathbb{R}^n$) or an equivalent parametric curve.
Lines in 3-space can be described as
\begin{itemize}
    \item Intersecting
    \item Parallel, if
    \begin{itemize}
        \item there exists a plane containing the two non-intersecting lines, and/or
        \item the two lines have direction vectors that are scalar multiples of one another, and/or
        \item the distance between the lines is constant.
    \end{itemize}
    \item Skew, if
    \begin{itemize}
        \item the lines are neither parallel or skew, and/or
        \item the lines cannot be contained by a single plane, and/or
        \item the lines can be contained in two parallel planes.
    \end{itemize}
\end{itemize}
intersect, be parallel, or skew (the lines live in different parallel planes).

They can be either intersecting or parallel with planes.

\subsubsection{Vector Form}
Treating the set of points $(x,y,z)$ as analogous vectors:
$$\langle x, y, z \rangle = \langle x_o, y_o, z_o \rangle + \langle a, b, c \rangle t = \vect{r}(t)$$
The initial point is $\langle x_o, y_o, z_o \rangle$, and $\langle a,b,c \rangle$ is the direction vector. $t \in \mathbb{R}$, it is a scalar that allows the direction vector to trace out the whole line.\\
There are two methods for finding the line of intersection between two points.
\paragraph{Method I} By setting the equations of the two planes equal to one another, we can obtain linear expressions for all three variables in terms of a fourth parameter $t$ (i.e. there is a single degree of freedom in this system of 2 equations and 3 unknowns). Any constants in these expressions are used in the initial point, any variable terms can have $t$ factored out to produce the direction vector term.

\paragraph{Method II} The cross product of the perpendiculars to two planes is parallel to the line of intersection, $\vect{n_1} \times \vect{n_2}$ can be used as a direction vector.

The offset can be determined by setting one of the variables $x, y, z = 0$, and solving for the other two variables through the system of 2 planes, leading to a point that can be used for the initial point.

\subsubsection{Parametric Form}
The linear expressions for $x,y,z$ mentioned in Method I can be used as functions in a parametric equation:
\begin{align*}
    x(t) &= x_o + at \\
    y(t) &= y_o + bt \\
    z(t) &= z_o + ct
\end{align*}

\paragraph{Symmetric Form}
We can also express the line as the set of points satisfying this equation:
$$
\frac{x-x_o}{a} = \frac{y-y_o}{b} = \frac{z-z_o}{c}
$$
The value of this ratio can be considered the ``value of $t$" in the functional forms of the line.

\subsubsection{Distance of Point to Line}
Using a difference/distance vector from the point to a point on the line, we can take the cross product to get the distance to the line.

\subsubsection{Distance of Skew Lines}
Using the cross product of the two lines' direction vectors, we can find the parallel planes that contain the lines. This can then be used to find the distance between the lines.

\subsection{Scalar Triple Product}
The scalar triple product is the product of three vectors
\[
    \vect{a}\cdot\vect{b}\times\vect{c}=\vect{a}\times\vect{b}\cdot\vect{c} =
    \begin{vmatrix}
        a_1 & a_2 & a_3\\
        b_1 & b_2 & b_3\\
        c_1 & c_2 & c_3\\
    \end{vmatrix}
\]
The value of the scalar triple product represents the volume of the parallelepiped (tilted box/prism with parallelogram faces) with sides represented by the three co-terminal vectors.

If the three vectors are co-planar, then the scalar triple product will be equal to $0$.

\section{3D Limits}
The definition for limits of functions $f: \mathbb{R}^2 \to \mathbb{R}$ is:
\[
    \lim_{(x,y)\to(a,b)}f(x,y) = L
\] if and only if $\forall\epsilon>0, \exists\delta>0 \ni 0<\sqrt{(x-a)^2+(y-b)^2}<\delta \Rightarrow \abs{f(x,y) - L}<\epsilon$.\footnote{The limit of a function $f(x,y)$ is equal to $L$ if and only if for all positive $\epsilon$, there exists a positive $\delta$ such that if the point $(x,y)$ is within a (circular, but could also be rectangular) neighborhood constrained by distance $\delta$, it is guaranteed that the difference between $f(x,y)$ and the value $L$ will be less than $\epsilon$.}

\subsection{Showing Limits DNE}
It is easier to prove that a limit does not exist than it is to show that one exists. In both cases, we choosing relations between $x$ and $y$ (e.g. setting $x=a$/$y=b$, or $y=f(x)$/$x=f(y)$) that constrain the limit to a single variable and allow us to use familiar methods of analysis. Note that L'HÃ´pital's rule only applies for single-variable limits.

\subsubsection{The One-Path Approach}
If \(\lim_{(x,y)\to(a,b)}f(x,y) = DNE\) along some path (a continuous curve) $c_1$, then \(\lim_{(x,y)\to(a,b)}f(x,y)\) does not exist.

As a reminder, a limit going to $\pm\infty$ does not exist.

\subsubsection{The Two-Path Approach}
If \(\lim_{(x,y)\to(a,b)}f(x,y) = L_1\) along some path $c_1$, and if \(\lim_{(x,y)\to(a,b)}f(x,y) = L_2\) along some path $c_2$, and if $L_1\ne L_2$, then \(\lim_{(x,y)\to(a,b)}f(x,y)\) does not exist.

\subsection{Limit Properties}
Given that $f(x,y)=L$, $g(x,y)=M$; $L,M,k\in\mathbb{R}$ and $n\in\mathbb{N}$:
\begin{align*}
    &\lim_{(x,y)\to(a,b)}f(x,y)\pm g(x,y) = L \pm M \\
    &\lim_{(x,y)\to(a,b)}kf(x,y) = kL \\
    &\lim_{(x,y)\to(a,b)}f(x,y) \cdot g(x,y) = L \cdot M \\
    &\lim_{(x,y)\to(a,b)}\frac{f(x,y)}{g(x,y)} = \frac{L}{M} & \text{for } M\ne0 \\
    &\lim_{(x,y)\to(a,b)}(f(x,y))^n = L^n \\
\end{align*}

\subsection{Evaluating Limits}
As suggested by the two-path approach to proving limits do not exist, limits should have consistent values for \emph{all} "paths" taken to approach the limiting point.

\subsubsection{Continuity}
$f(x,y)$ is continuous at $(a,b)$ iff $f(a,b)=\lim_{(x,y)\to(a,b)}f(x,y) = f(a,b)$. This property be proven using $\epsilon-\delta$ proofs that accommodate arbitrary points on $f(x,y)$, or by use of limit properties that combine existing continuous functions.

Based on the Limit Properties, we can determine that sums and products of continuous functions are also continuous.\\
Continuous functions include (considering their respective domains):
\begin{itemize}
    \item all polynomials in $x,y$ (sums of terms in the form $cx^ny^m$, with constants $c\in\mathbb{R}$, $n,m\in\mathbb{N}\cup\left\{0\right\}$)
    \item trigonometric functions $\sin{x}$, $\cos{x}$, etc.
    \item $e^x$ and its inverse $\ln{x}$
\end{itemize}

\paragraph{Algebraic Manipulation}
Canceling denominators in a rational function, like in single-variable limits, is a useful means of simplifying the evaluation of a limit to one that can be evaluated using continuity. For denominators with square-roots, it may be advantageous to use a conjugate to rationalize the denominator.

\subsubsection{Substitution}
Letting $u=g(x,y)$, and letting $u\to g(a,b)$ may simplify the form of a function composed of $g(x,y)$ terms. Take note of range restrictions on $g(x,y)$, such as $g(x,y)=x^2+y^2$ implying that $u > 0$, the limiting value will be $0^+$.

\paragraph{Polar Substitution}
Using $x = r\cos{\theta}$, $y=r\sin{\theta}$, with $0 \le r$ and $0\le\theta<2\pi$ (so as to allow for nearly one-to-one mapping of Cartesian coordinates to polar coordinates), one can restate a function in terms of $r$ and $\theta$.

In particularly fortunate cases, $\theta$ can be eliminated using identities such as $\sin^2{\theta}+\cos^2{\theta}=1$. In other cases, it is useful to consider the bounds of trigonometric functions and construct comparisons that can be used with the Squeeze Theorem.

\subsubsection{Squeeze Theorem}
The Squeeze Theorem is rather useful in cases where we know a function with a simpler expression that is bound to be greater/less than that, or we have a limiting value like $0$.

This simpler expression could be derived by replacing a difficult function (e.g. $\sin$ or $\cos$) with bounds (e.g. $-1$ and $1$) or by comparing the function to one with a simpler numerator/denominator (e.g. $\frac{1}{x^2+4x+5} < \frac{1}{x^2}$).

\section{Derivatives}
\subsection{Definition of a Partial Derivative}
If $z=f(x,y)$, then the partial derivative of $f$ with respect to $x$ at point $(a,b)$ is
\begin{align*}
    f_x(a,b) &= \lim_{h\to0} \frac{f(a+h,b) - f(a,b)}{h} \\
    f_x(a,b) &= \lim_{x \to a} \frac{f(x,b)-f(a,b)}{x-a}
\end{align*}
similarly, the partial derivative of $f$ with respect to $y$ at $(a,b)$ is
\begin{align*}
    f_y(a,b) &= \lim_{h\to0} \frac{f(a,b+h) - f(a,b)}{h} \\
    f_y(a,b) &= \lim_{y \to b} \frac{f(a,y)-f(a,b)}{y-b}
\end{align*}

The partial derivative indicates the slope of a tangent line to a surface along a specific direction in the xy-plane. It does not guarantee that a surface is differentiable at a point.

\subsubsection{Application of Partial Derivatives}
We can determine a \emph{potential} tangent plane/normal vector to a surface at point $(a,b, f(a,b))$ using partial derivatives.

The potential tangent plane has equation
\begin{align*}
	z - f(a,b) &= f_x(x - a) + f_y(y - b)\\
	L(x,y) &= f_x(x-a) + f_y(y-b) + f(a,b)
\end{align*}
$L(x,y)$ can also be referred to as the "linearization of $f$," "linear approximation of $f$," or "tangent plane approximation of $f$."

The normal vector is
$$\langle f_x(a,b), f_y(a,b), -1 \rangle$$

Note that the mere existence of partials does not imply continuity nor differentiability of a surface.

\subsection{Evaluating Partial Derivatives}
Instead of explicitly evaluating the limit, we will usually use differentiation rules to obtain partial derivatives. For $f_x$/$f_y$, the other variable ($y$ or $x$, respectively) is considered a \emph{constant}, not a variable, so it is not necessary to use the Chain Rule like for implicit differentiation (as seen in Calc I).

\subsubsection{Higher Order Partial Derivatives}
A higher order partial derivative can be taken in the same direction as the first order derivative (with additional results being analogous to their 2-space equivalents, i.e. "concavity" on the second order derivative).

Calculating a higher order partial derivative uses the same differentiation rules as for the first order partial derivative, keeping in mind that the partial we are taking may be in a different variable (e.g. $\frac{\partial}{\partial x}$ then $\frac{\partial}{\partial y}$). For a function $\mathbb{R}^2\to\mathbb{R}$, there are $2^n$ possible permutation of the axial variables for an $n$-th order derivative.

Under suitable conditions $f_{xy} = f_{yx}$.

\subsubsection{Implicit Differentiation}
Considering $x,y$ as independent variables, with $z$ being dependent on a relation to $x,y$, we can implicitly differentiate for $\frac{\partial z}{\partial x}$ or $\frac{\partial z}{\partial y}$ (with $\frac{\partial y}{\partial x}$ and $\frac{\partial x}{\partial y}$ both being $0$, as independence implies that $x$ and $y$ are constant with respect to one another) using a Chain Rule analogous to functions in 2-space.

\subsection{Notation}
The partial (derivative) of $f$ (or $z=f$) with respect to $x$ can be written as:
\[
f_x(x,y)=f_x=\frac{\partial}{\partial x}\left[f(x,y)\right] = \frac{\partial f}{\partial x} = \frac{\partial z}{\partial x}=D_xf=f_1=D_1f
\]
Note that the $_1$ subscripts represent the direction of the \emph{first} independent variable (i.e. $x$).\\
This can be replaced with a general unit vector $\hat{u}$ when taking partial derivatives in other directions.

\subsubsection{Higher Order Partials}
The order in which partials are taken is reversed between the two notations we use. Using Leibniz Notation, the partials are read from right to left, as if they were function compositions. Using subscript notation, the partials are read from left to right.
\[
    \frac{\partial}{\partial y} \left(\frac{\partial f}{\partial x}\right) = \frac{\partial^2f}{\partial y \partial x} = f_{xy}(x,y) = f_{12}
\]

\subsection{Partials}
For a differentiable function, we define the differentials $dx=\Delta x$ (differential of x), $dy=\Delta y$, and $dz = f_x(a,b)\,dx + f_y(a,b)\,dy$. For small values of $\Delta x$ and $\Delta y$, $dz\approx\Delta z$

\subsection{Differentiability}
$f(x,y)$ is differentiable at $(a,b)$ if and only if
\[
	\lim_{(x,y)\to(a,b)} \frac{f(x,y) - L(x,y)}{\sqrt{(x-a)^2 + (y-b)^2}} = 0
\]
The existence of the partial derivatives $f_x(x,y)$ and $f_y(x,y)$ is necessary, but not sufficient to ensure differentiability.

Differentiability implies continuity of the original function and existence of the partials (which might not be continuous).

\subsubsection{Justification}
In 2-space, there exists an analogous linear approximation definition of the derivative, which generalizes more easily to 3-space.

Expanding the linearization definition, we can find the following results, showing that the slope of the surface fits on the tangent plane. Letting $f_x(a,b) = m_x$ and $f_y(a,b) = m_y$, and $\theta$ being the angle in which an alternative partial is taken:
\begin{align*}
	\lim_{(x,y)\to(a,b)} \frac{f(x,y) - f(a,b) - (m_x(x - a) + m_y(y - b))}{\sqrt{(x-a)^2+(y-b)^2}} &= 0\\
	\lim_{x\to a} \frac{f(x,b) - f(a,b) - m_x(x - a)}{\sqrt{(x-a)^2}} =
	 \lim_{x\to a} \frac{f(x,b) - f(a,b)}{\sqrt{(x-a)^2}} - m_x &= 0\\
	\lim_{y\to a} \frac{f(a,y) - f(a,b) - m_y(y - b)}{\sqrt{(y-b)^2}} =
	 \lim_{y\to a} \frac{f(a,y) - f(a,b)}{\sqrt{(y-b)^2}} - m_y &= 0\\
	\lim_{(x,y)\to(a,b)} \frac{f(x,y) - f(a,b) - (m_x\Delta{x} + m_y\Delta{y})}{\sqrt{(\Delta{x})^2+(\Delta{y})^2}} &=\\
	 \lim_{(x,y)\to(a,b)} \frac{f(x,y) - f(a,b)}{\sqrt{(\Delta{x})^2+(\Delta{y})^2}} - (m_x\cos{\theta} + m_y\sin{\theta}) &=\\
	 \lim_{(x,y)\to(a,b)} \frac{f(x,y) - f(a,b)}{\sqrt{(\Delta{x})^2+(\Delta{y})^2}} - m_\theta &= 0
\end{align*}


\subsubsection{Finding Differentiable Functions}
\paragraph{Theorem I} If $f_x$ and $f_y$ are continuous at/in a neighborhood around/on some disk containing $(a,b)$ then $f$ is differentiable at $(a,b)$.

This theorem guarantees differentiability, but its conditions are stronger than necessary. This theorem excludes some differentiable functions.

\paragraph{Clairaut's Theorem} If all $n^{th}$ order partials of $f$ are continuous at a point, then the $n^{th}$ order mixed partials are equal. The order of differentiation is immaterial ($f_{xy}=f_{yx}$).

\subsection{Chain Rule}
For a differentiable function $f(x,y)=z$ and $x=g(t)$ and $y=h(t)$:
$$\frac{dz}{dt}=\frac{\partial z}{\partial x}\frac{dx}{dt}+\frac{\partial z}{\partial y}\frac{dy}{dt}$$
Similar results can be shown for $f(x,y,z)=w$ and $x=g(t)$, $y=h(t)$, $z=i(t)$, and for $x=g(t,s)$, $y=h(t,s)$ using $\frac{\partial{x}}{\partial{t}}$, $\frac{\partial{y}}{\partial{t}}$, $\frac{\partial{x}}{\partial{s}}$, and $\frac{\partial{y}}{\partial{s}}$.

\subsubsection{Proof}
If $f$ is differentiable at $(a,b)$ then
\begin{align*}
    \lim_{(x,y)\to(a,b)} \frac{f(x,y)-L(x,y)}{\sqrt{(x-a)^2+(y-b)^2}}&=0\\
    \frac{f(x,y)-L(x,y)}{\sqrt{(x-a)^2+(y-b)^2}}&=\epsilon\\
    f(x,y) = L(x,y) + \epsilon\sqrt{(x-a)^2+(y-b)^2}\\
    f(x,y) = L(x,y) + \epsilon_1\Delta{x} + \epsilon_2\Delta{y} \\
    \Delta{z} = \frac{\partial{z}}{\partial{x}}\Delta{x} + \frac{\partial{z}}{\partial{y}}\Delta{y} + \epsilon_1\Delta{x} + \epsilon_2\Delta{y}\\
    \lim_{\Delta{t}\to0} \frac{\Delta{z}}{\Delta{t}} = \frac{\partial{z}}{\partial{x}}\frac{\Delta{x}}{\Delta{t}} + \frac{\partial{z}}{\partial{y}}\frac{\Delta{y}}{\Delta{t}} + \epsilon_1\frac{\Delta{x}}{\Delta{t}} + \epsilon_2\frac{\Delta{y}}{\Delta{t}}\\
    \frac{dz}{dt}=\frac{\partial z}{\partial x}\frac{dx}{dt}+\frac{\partial z}{\partial y}\frac{dy}{dt}+0+0
\end{align*}
Note that $\epsilon_1, \epsilon_2\to0$ as $(\Delta{x},\Delta{y})\to(0,0)$.

\paragraph{Product/Quotient Rule} The product and quotient rule for $\mathbb{R}\to\mathbb{R}$ functions can be proven by defining a function $F(a,b)=ab$, with $a=f(x)$ and $b=g(x)$, applying the Chain Rule to $F$ produces the same results.

\subsection{Directional Derivatives}
To take the directional derivative in 3-space of a function $f$ at point $(a,b,\cdots)$ along some direction vector $\vect{u}=m\langle u_1, u_2\rangle$ $D_{\vect{u}}$ we take $\hat{u}=\langle u_1, u_2\rangle$ ($D_{\vect{u}} = D_{\hat{u}}$). We can then form parametric equations $x=a+hu_1$, $y=b+hu_2$,for the 2-space line going through $(a,b)$ in the direction of \vect{u} and define
$$D_{\hat{u}}f=\lim_{h\to0}\frac{f(a+hu_1,b+hu_2)-f(a,b)}{h}$$
Using the Chain Rule, $D_{\hat{u}}f = f_xu_1+f_yu_2=\langle f_x, f_y \rangle \cdot \langle u_1, u_2 \rangle$.

\subsubsection{Gradients}
We define the gradient of $f(x,y)$ to be
$$\nabla f(x,y) = \langle f_x(x,y), f_y(x,y) \rangle$$
The "nabla" is the gradient operator, which operates on functions to return a vector-valued function.

From the fact that $$D_{\hat{u}}f=\nabla f \cdot \hat{u}=\abs{\nabla f}\abs{\hat{u}}\cos{\theta}$$ we can warrant that $\nabla f$ points in the direction of \textbf{greatest ascent} (increase in $f$), $-\nabla f$ points in the direction of \textbf{greatest descent}, and that $\abs{\nabla f}$ is the \textbf{largest directional derivative}.

For a level curve $f(x,y)=c$, and assuming the existence of differentiable parameterizations $x(t), y(t)$ that can be used to determine a vector $\langle x'(t), y'(t)\rangle$ tangent to the contour (i.e. $\frac{dz}{dt}=0$), $$\nabla{f}\cdot\langle x'(t), y'(t)\rangle=\frac{\partial f}{\partial x}\frac{dx}{dt}+\frac{\partial f}{\partial y}\frac{dy}{dt}=\frac{dz}{dt}=0$$
indicating that the gradient is perpendicular to the tangent curve, and the level curve as a whole.

This result generalizes to higher dimensions as well, allowing us to more easily find normal vectors to surfaces.

Note that due to the linearity of differentiation, differentiation rules apply to gradients too.

\section{Optimization}
Extrema may exist at critical points $(a,b)$ where $\nabla f(a,b) = \vect{0}$ (i.e. $f_x(a,b)=0$, $f_y(a,b)=0$).

For some surfaces, it is possible to determine the nature of an extremum by analyzing the function (e.g. square values must be positive, suggesting the critical point of the paraboloid $f(x,y)=x^2+y^2$ at $(0,0)$ is a minimum; if you take different slices of a surfaces, and find that a point is a local minimum for some paths, but a local maximum along others, that point is a saddle point).

\subsection{Second Derivatives Test}
If $f(x,y)$ has continuous second-order partials and $\nabla f(a,b)=0$, then consider the determinant/discriminant
\[
D = f_{xx}(a,y)f_{yy}(a,b)-\left(f_{xy}(a,b)\right)^2
\]
\begin{enumerate}
    \item If $D>0$ and $f_{xx}(a,b)$ or $f_{yy}(a,b) > 0$ then $f(a,b)$ is a local minimum.
    \item If $D>0$ and $f_{xx}(a,b)$ or $f_{yy}(a,b) < 0$ then $f(a,b)$ is a local maximum.
    \item If $D<0$, then $f(a,b)$ is a saddle point.
    \item If $D=0$, then the test is inconclusive.
\end{enumerate}
$D$ can also be written as the determinant of the Hessian matrix \(D=\begin{vmatrix}
    f_{xx} & f_{xy}\\
    f_{yx} & f_{yy}
\end{vmatrix}\).

\paragraph{Proof}
The gradient must be $\vect{0}$, as the points in the direction of the gradient would be greater/lesser than the point we are investigating.

Assuming that $f(x,y)$ is infinitely differentiable function, $f$ can be defined as a Taylor Polynomial about point $(a,b)$ in two variables.
\begin{align*}
    f(x,y) = &f(a,b) + f_x(a,b)(x-a) + f_y(a,b)(y-b) +\\
             &\frac{f_{xx}(a,b)}{2}(x-a)^2+\frac{f_{yy}(a,b)}{2}(y-b)^2+f_{xy}(x-b)(y-b)+\cdots
\end{align*}
Moving $f(a,b)$ to the other side, and considering that $\nabla{f}(a,b)=\langle f_x, f_y \rangle = \langle 0,0 \rangle$:
\[
    f(x,y)-f(a,b)=\Delta{f} = \frac{f_{xx}(a,b)}{2}(x-a)^2+\frac{f_{yy}(a,b)}{2}(y-b)^2+f_{xy}(x-b)(y-b)+\cdots
\]
At points sufficiently close to $x$ and $y$, the cubic terms and beyond become negligible. Factoring out a $\frac{\left(y-b\right)^2}{2}$ (the choice of $y$ is arbitrary as $x$ works too; note that this value is always positive):
\[
    \Delta{f} = \frac{\left(y-b\right)^2}{2}\left[f_{xx}(a,b)\left(\frac{x-a}{y-b}\right)^2 + 2f_{xy}(a,b)\left(\frac{x-a}{y-b}\right)+f_{yy}(a,b)\right]
\]

Letting $\omega = \frac{x-a}{y-b}$, we can write the interior expression, which determine the sign of $\Delta{f}$ as a quadratic
\[
f_{xx}(a,b)\omega^2+2f_{xy}(a,b)\omega+f_{yy}(a,b)
\] with $A = f_{xx}(a,b)$, $B=2f_{xy}(a,b)$, $C=f_{yy}(a,b)$. From on the discriminant $B^2-4AC=4D$, we can determine if this quadratic has
\begin{enumerate}
    \item $D > 0$: two real roots, i.e. the expression is sometimes negative and sometimes positive
    \item $D < 0$: two complex roots, i.e. the expression is always positive or negative
    \item $D = 0$: one root
\end{enumerate}
From these results about the sign of the quadratic, we can sometimes determine the sign of $\Delta{f}$, allowing us to discern whether a point is a local maximum/minimum or a saddle point. If $D=0$, we cannot draw a comprehensive conclusion about whether the sign of $\Delta{f}$ changes.

\subsubsection{Optimization Example: Regression (Least-Squares) Line} A \textbf{Linear Regression} aims to produce coefficients for a line $y=Ax+B$ that minimizes the squares [to avoid sign issues] of the $\Delta{y}$s between the points and the line. One can write a function for the sum of the squares of the differences as a function of $A,B$ and determine a minimum point.

\subsection{Optimization on a Curve}
If we are optimizing a surface $f(x,y)$ with $x$ and $y$ being constrained to a curve $g(x,y)=c$ (i.e. a level curve/contour of $g$), critical values occur when $\nabla f(x,y) \parallel \nabla g(x,y)$. This can be expressed as
$$\nabla f(x,y) = \lambda\nabla g(x,y)$$ with $\lambda$ being a scalar known as the Lagrange Multiplier.

This is because the maxima will occur when the directional derivative of $f(x,y)$ is $0$ in the direction of (tangent to) $g(x,y)$, which happens when the gradient of $f$ is perpendicular to the tangent (``velocity") vector. The proof is situated using a chain rule expressing $g(x,y)=c$ as a parametric curve with $x(t)$ and $y(t)$.

Note that with certain constraints, it may be possible to reparameterize a function $f(x,y)$ into a single-variable function that can be easily optimized.

\subsection{Optimization in a Region}
\subsubsection{Regions}
A \textbf{region} $D$ in $\mathbb{R}^2$ is said to be bounded if it can be enclosed (or contained) in a circle (or other figure) with finite radius.\footnote{Note that a region in $\mathbb{R}$ is more familiarly known as an interval.}\\
A point $P$ is:
\begin{itemize}
    \item \textbf{inside} (interior to) $D$ if there exists a neighborhood of $P$ that is also contained in $D$.
    \item \textbf{outside} (exterior to) $D$ if there exists a neighborhood of $P$ that consists of only points $\notin D$.
    \item \textbf{on the boundary} of $D$ if every neighborhood of $P$ contains points inside and outside of $D$.
\end{itemize}
A region is closed if it contains all of its boundary points.

\subsubsection{Extreme Value Theorem}
If $f(x,y)$ is continuous on a closed and bounded region $D$, then $f$ must achieve both an absolute minimum and absolute maximum on $D$.\\
These extrema may occur at critical points or on the boundary of the region.

To apply the EVT to find absolute extrema:
\begin{enumerate}
    \item Find critical points of $f(x,y)$ within the region by solving for $\nabla{f} = \vect{0}$ (or undefined).
    \item Find extrema of $f$ subject to your bounding constraint, using Lagrange Multipliers or other techniques.
    \item Evaluate $f(x,y)$ at all of the relevant points to determine which are the absolute extremes.
\end{enumerate}

\section{Space Curves}
In general, a space curve is a function $f: \mathbb{R}\to\mathbb{R}^n$. We can consider the output of these functions to be vectors or points. The single parameter variable (conventionally $t$) may have domain restrictions, or the range of outputs may be constrained.

Although such functions can parameterize a curve, such a parameterization is not unique; there exist infinitely many functions that produce the same curve.

\subsection{Limits}
The limit of a vector function $\vect{r}(t)$, if it exists, is the limit of its components.

$$\lim_{t\to{a}}\vect{r}(t)=\lim_{t\to{a}}\langle x(t),y(t) \rangle = \langle \lim_{t\to{a}}x(t),\lim_{t\to{a}}y(t) \rangle$$

\subsection{Continuity}
$\vect{r}(t)$ is continuous at $\vect{r}(a)$ if and only if $$\lim_{t\to{a}}\vect{r}(t)=\vect{r}(a)$$

\subsection{Derivatives}
The definition of a vector function $\vect{r}(t)$'s derivative is analogous to that of a function in 2-space.
$$\vect{r}'(t)=\frac{d\vect{r}}{t}=\lim_{h\to0}\frac{\vect{r}(t+h)-\vect{r}(t)}{h}=\langle x'(t), y'(t) \rangle$$
The limit of the secant vector is the tangent vector; this vector is tangent to the curve at $t$ so long as it exists and $\ne\vect{0}$. In this case, the tangent line to the curve $\vect{r}(t)$ at $t=a$ is
$$\langle x,y,z \rangle = \vect{r}(a)+t\cdot\vect{r}'(a)$$ Functions that have continuous derivatives with $\vect{r}'(t)\ne0$ are \textbf{smoothly parameterized}.

\subsubsection{Properties of the Derivative}
Letting $\vect{r}(t), \vect{u}(t), \vect{v}(t)$ be differentiable vector-valued functions, and $f(t)$ be a differentiable scalar function:
\begin{enumerate}
    \item $\frac{d}{dt}\left(c\cdot\vect{r}(t)\right)=c\cdot\vect{r}'(t)$
    \item $\frac{d}{dt}\left(f(t)\cdot\vect{r}(t)\right)=f(t)\cdot\vect{r}'(t)+\vect{r}(t)\cdot f'(t)$
    \item $\frac{d}{dt}\left(\vect{u}(t)\cdot\vect{v}(t)\right)=\vect{u}(t)\cdot\vect{v}'(t)+\vect{u}'(t)\cdot \vect{v}(t)$
    \item $\frac{d}{dt}\left(\vect{u}(t)\times\vect{v}(t)\right)=\vect{u}(t)\times\vect{v}'(t)+\vect{u}'(t)\times \vect{v}(t)$ (Note that order matters due to the cross product being anti-commutative, and all functions must be $\mathbb{R}\to\mathbb{R}^3$.)
\end{enumerate}

\subsubsection{Derivative of the Magnitude}
Considering the derivative of the squared magnitude $\magnitude{\vect{r}(t)}^2$ for a non-zero vector function, we find that
\begin{align*}
    \frac{d}{dt}\left(\magnitude{\vect{r}}^2\right) = \frac{d}{dt}\left(\vect{r}\cdot\vect{r}\right) &= 2\vect{r}\cdot\vect{r}' = 2\magvect{r}\frac{d}{dt}\left(\magvect{r}\right)\\
    \frac{d}{dt}\left(\magvect{r}\right) &= \frac{\vect{r}\cdot\vect{r}'}{\magvect{r}}
\end{align*}
As a corollary, if $\magvect{r}=c$, the velocity vector is perpendicular to the position vector.

(note that $\frac{d}{dt}\left(\magvect{\vect{r}}\right) = \vect{r}'(t)\cos{\theta}$).

\subsubsection{The Unit Tangent}
The unit tangent to $\vect{r}(t)$ is $$\vect{T}(t)=\frac{\vect{r}'(t)}{\magnitude{\vect{r}'(t)}}$$
\section{Appendix}
\subsection{Cartesian Products} \label{set-multiplication}
For sets $A, B$, their Cartesian product is $$A \times B = \left\{ (a,b) | a \in A, b \in B\right\}$$
This produces a set of (ordered) 2-tuples/vectors.
The ``$n$-ary Cartesian product" generalizes this process to produce $n$-tuples/vectors, leading to a definition for ``exponentiating" sets, as in $\mathbb{R}^n$.

\end{document}
